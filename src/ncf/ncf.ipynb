{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# Walk up the directory tree until we find 'src'\n",
        "path = current_dir\n",
        "src_path = None\n",
        "\n",
        "while True:\n",
        "    if os.path.basename(path) == \"src\":\n",
        "        src_path = path\n",
        "        break\n",
        "    parent = os.path.dirname(path)\n",
        "    if parent == path:  # reached filesystem root\n",
        "        break\n",
        "    path = parent\n",
        "\n",
        "# Add src to sys.path if found\n",
        "if src_path and src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "# Local imports\n",
        "from helpers import download_ml1m_dataset\n",
        "from utils.ml_to_ncf import preprocess_ml1m_to_ncf_format\n",
        "from utils.ncfdata import NCFData\n",
        "from helpers.ncf_model import NCF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model architecture: NeuMF-end\n",
            "✓ Directories configured\n",
            "  - Data directory: /Users/abbas/Documents/Codes/thesis/recommender/src/../data (will be created/used for downloaded data)\n",
            "  - Model save path: /Users/abbas/Documents/Codes/thesis/recommender/src/../models\n"
          ]
        }
      ],
      "source": [
        "dataset = 'ml-1m'\n",
        "# ============================================================================\n",
        "# MODEL ARCHITECTURE CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Choose which model architecture to use\n",
        "# Options:\n",
        "#   - 'MLP': Multi-Layer Perceptron only (non-linear interactions)\n",
        "#   - 'GMF': Generalized Matrix Factorization only (linear interactions)\n",
        "#   - 'NeuMF-end': Neural Matrix Factorization trained from scratch (end-to-end)\n",
        "#   - 'NeuMF-pre': Neural Matrix Factorization with pre-trained GMF and MLP models\n",
        "model_name = 'NeuMF-end'\n",
        "assert model_name in ['MLP', 'GMF', 'NeuMF-end', 'NeuMF-pre'], \\\n",
        "    f\"Model must be 'MLP', 'GMF', 'NeuMF-end', or 'NeuMF-pre', got '{model_name}'\"\n",
        "\n",
        "print(f\"✓ Model architecture: {model_name}\")\n",
        "\n",
        "# ============================================================================\n",
        "# DATA AND MODEL PATHS CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Data will be downloaded automatically during training\n",
        "# We'll create a local data directory to store downloaded files\n",
        "\n",
        "data_dir = os.path.join(os.path.dirname(os.getcwd()), '..', 'data')\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "# Model saving directory\n",
        "model_path = os.path.join(os.path.dirname(os.getcwd()), '..', 'models')\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "GMF_model_path = os.path.join(model_path, 'GMF.pth')\n",
        "MLP_model_path = os.path.join(model_path, 'MLP.pth')\n",
        "NeuMF_model_path = os.path.join(model_path, 'NeuMF.pth')\n",
        "\n",
        "print(f\"✓ Directories configured\")\n",
        "print(f\"  - Data directory: {data_dir} (will be created/used for downloaded data)\")\n",
        "print(f\"  - Model save path: {model_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2.4 TRAINING HYPERPARAMETERS\n",
        "# ============================================================================\n",
        "\n",
        "# Learning rate: Controls how big steps the optimizer takes during training\n",
        "# Too high: training might be unstable or diverge\n",
        "# Too low: training will be very slow\n",
        "# Typical range: 0.0001 to 0.01\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Dropout rate: Regularization technique to prevent overfitting\n",
        "# Randomly sets some neurons to zero during training\n",
        "# Range: 0.0 (no dropout) to 0.9 (very aggressive dropout)\n",
        "# 0.0 means no dropout (all neurons active)\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Batch size: Number of training examples processed together in one iteration\n",
        "# Larger batch size: more stable gradients, but requires more memory\n",
        "# Smaller batch size: less memory, but noisier gradients\n",
        "# Typical values: 64, 128, 256, 512\n",
        "batch_size = 256\n",
        "\n",
        "# Number of training epochs: How many times we'll iterate through the entire dataset\n",
        "# More epochs: better learning, but risk of overfitting\n",
        "# Too few epochs: model might not learn enough\n",
        "epochs = 20\n",
        "\n",
        "# Top-K for evaluation: When evaluating, we recommend top K items to each user\n",
        "# We measure if the true item is in the top K recommendations\n",
        "# Common values: 5, 10, 20\n",
        "top_k = 10\n",
        "\n",
        "# Factor number: Dimension of the embedding vectors for users and items\n",
        "# Larger: more capacity to learn complex patterns, but more parameters\n",
        "# Smaller: fewer parameters, faster training, but less capacity\n",
        "# Common values: 8, 16, 32, 64\n",
        "factor_num = 32\n",
        "\n",
        "# Number of MLP layers: Depth of the Multi-Layer Perceptron component\n",
        "# More layers: can learn more complex non-linear patterns\n",
        "# Fewer layers: simpler model, faster training\n",
        "# Typical range: 1 to 5 layers\n",
        "num_layers = 3\n",
        "\n",
        "# Number of negative samples for training: For each positive (user, item) pair,\n",
        "# we sample this many negative items (items the user hasn't interacted with)\n",
        "# More negatives: better learning signal, but slower training\n",
        "# Fewer negatives: faster training, but potentially weaker learning\n",
        "# Common values: 1, 4, 8\n",
        "num_ng = 4\n",
        "\n",
        "# Number of negative samples for testing: During evaluation, for each test item,\n",
        "# we also provide this many negative items. The model should rank the true item higher.\n",
        "# Typically 99 negatives + 1 positive = 100 items total per test case\n",
        "test_num_ng = 99\n",
        "\n",
        "# Whether to save the trained model\n",
        "save_model = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 3.1: Downloading MovieLens 1M Dataset\n",
            "======================================================================\n",
            "✓ Dataset already exists at /Users/abbas/Documents/Codes/thesis/recommender/src/../data/ml-1m/ratings.dat\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"STEP 3.1: Downloading MovieLens 1M Dataset\")\n",
        "print(\"=\" * 70)\n",
        "ratings_file = download_ml1m_dataset(data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loaded 1000209 ratings\n",
            "  - Unique users: 6040\n",
            "  - Unique movies: 3706\n",
            "\n",
            "Filtering positive interactions (ratings >= 4)...\n",
            "✓ 575281 positive interactions (out of 1000209 total)\n",
            "\n",
            "Remapping user and item IDs to be contiguous...\n",
            "✓ Remapped to 6038 users and 3533 items\n",
            "\n",
            "Splitting data (train: 80%, test: 20%)...\n",
            "✓ Training pairs: 460225\n",
            "✓ Test pairs: 115056\n",
            "\n",
            "Saving training data to /Users/abbas/Documents/Codes/thesis/recommender/src/../data/ml-1m.train.rating...\n",
            "✓ Saved 460225 training pairs\n",
            "\n",
            "Creating training interaction matrix...\n",
            "✓ Training matrix created: 460225 interactions\n",
            "\n",
            "Generating test negative samples (99 negatives per test case)...\n",
            "✓ Generated test negative samples: 115056 test cases\n"
          ]
        }
      ],
      "source": [
        "train_rating_path, test_rating_path, test_negative_path, user_num, item_num, train_mat = \\\n",
        "    preprocess_ml1m_to_ncf_format(ratings_file, data_dir, test_ratio=0.2, test_negatives=99)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading training data from /Users/abbas/Documents/Codes/thesis/recommender/src/../data/ml-1m.train.rating...\n",
            "✓ Loaded 460225 training pairs\n",
            "  - Users: 6038\n",
            "  - Items: 3533\n",
            "\n",
            "Creating training interaction matrix...\n",
            "✓ Training matrix created: 460225 interactions\n",
            "\n",
            "Loading test data from /Users/abbas/Documents/Codes/thesis/recommender/src/../data/ml-1m.test.negative...\n",
            "✓ Loaded 11505600 test pairs (including negatives)\n",
            "\n",
            "======================================================================\n",
            "✓ Data loading complete!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def load_all_data(train_rating_path, test_negative_path):    \n",
        "    # Load training data\n",
        "    print(f\"Loading training data from {train_rating_path}...\")\n",
        "    train_data = pd.read_csv(\n",
        "        train_rating_path,\n",
        "        sep='\\t',\n",
        "        header=None,\n",
        "        names=['user', 'item'],\n",
        "        usecols=[0, 1],\n",
        "        dtype={0: np.int32, 1: np.int32}\n",
        "    )\n",
        "    \n",
        "    # Calculate number of users and items\n",
        "    user_num = train_data['user'].max() + 1\n",
        "    item_num = train_data['item'].max() + 1\n",
        "    \n",
        "    print(f\"✓ Loaded {len(train_data)} training pairs\")\n",
        "    print(f\"  - Users: {user_num}\")\n",
        "    print(f\"  - Items: {item_num}\")\n",
        "    \n",
        "    # Convert to list of lists for easier processing\n",
        "    train_data = train_data.values.tolist()\n",
        "    \n",
        "    # Create sparse training matrix (Dictionary of Keys format)\n",
        "    # This is used to quickly check if a user-item pair exists in training data\n",
        "    print(\"\\nCreating training interaction matrix...\")\n",
        "    train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
        "    for u, i in train_data:\n",
        "        train_mat[u, i] = 1.0\n",
        "    print(f\"✓ Training matrix created: {train_mat.nnz} interactions\")\n",
        "    \n",
        "    # Load test data with negative samples\n",
        "    print(f\"\\nLoading test data from {test_negative_path}...\")\n",
        "    test_data = []\n",
        "    with open(test_negative_path, 'r') as fd:\n",
        "        line = fd.readline()\n",
        "        while line is not None and line != '':\n",
        "            # Format: (user, item)\\tneg1\\tneg2\\t...\\tneg99\n",
        "            arr = line.strip().split('\\t')\n",
        "            \n",
        "            # Parse the positive pair: (user, item)\n",
        "            # eval() converts string \"(123, 456)\" to tuple (123, 456)\n",
        "            positive_pair = eval(arr[0])\n",
        "            u = positive_pair[0]\n",
        "            i = positive_pair[1]\n",
        "            \n",
        "            # Add the positive pair\n",
        "            test_data.append([u, i])\n",
        "            \n",
        "            # Add all negative items for this user\n",
        "            for neg_item in arr[1:]:\n",
        "                if neg_item:  # Skip empty strings\n",
        "                    test_data.append([u, int(neg_item)])\n",
        "            \n",
        "            line = fd.readline()\n",
        "    \n",
        "    print(f\"✓ Loaded {len(test_data)} test pairs (including negatives)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"✓ Data loading complete!\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    return train_data, test_data, user_num, item_num, train_mat\n",
        "\n",
        "# Load all data\n",
        "train_data, test_data, user_num, item_num, train_mat = load_all_data(\n",
        "    train_rating_path, test_negative_path\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = NCFData(\n",
        "    train_data,\n",
        "    item_num,\n",
        "    train_mat,\n",
        "    num_ng=num_ng,  # From Step 2 configuration\n",
        "    is_training=True\n",
        ")\n",
        "\n",
        "test_dataset = NCFData(\n",
        "    test_data,\n",
        "    item_num,\n",
        "    train_mat,\n",
        "    num_ng=0,  # No negative sampling for testing\n",
        "    is_training=False\n",
        ")\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,  # From Step 2 configuration\n",
        "    shuffle=True,  # Shuffle training data each epoch\n",
        "    num_workers=0,  # MUST be 0 for Jupyter notebooks (avoids pickling errors)\n",
        "    pin_memory=True if torch.cuda.is_available() else False  # Faster GPU transfer\n",
        ")\n",
        "\n",
        "test_loader = data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=test_num_ng + 1,  # 1 positive + test_num_ng negatives\n",
        "    shuffle=False,  # Don't shuffle test data\n",
        "    num_workers=0,  # MUST be 0 for Jupyter notebooks\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "# Verify num_workers is 0 (safety check)\n",
        "assert train_loader.num_workers == 0, f\"ERROR: train_loader.num_workers is {train_loader.num_workers}, must be 0!\"\n",
        "assert test_loader.num_workers == 0, f\"ERROR: test_loader.num_workers is {test_loader.num_workers}, must be 0!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 5.2: Creating and Initializing Model\n",
            "======================================================================\n",
            "✓ Model on CPU\n",
            "\n",
            "✓ Model created successfully!\n",
            "  - Total parameters: 1,574,657\n",
            "  - Trainable parameters: 1,574,657\n",
            "\n",
            "Model Architecture:\n",
            "  - Users: 6,038\n",
            "  - Items: 3,533\n",
            "  - GMF embeddings: 32 dimensions\n",
            "  - MLP embeddings: 128 dimensions\n",
            "  - MLP layers: 3 (with dropout=0.1)\n",
            "  - Prediction layer: 64 → 1\n",
            "\n",
            "======================================================================\n",
            "COMPLETE MODEL STRUCTURE:\n",
            "======================================================================\n",
            "NCF(\n",
            "  (embed_user_GMF): Embedding(6038, 32)\n",
            "  (embed_item_GMF): Embedding(3533, 32)\n",
            "  (embed_user_MLP): Embedding(6038, 128)\n",
            "  (embed_item_MLP): Embedding(3533, 128)\n",
            "  (MLP_layers): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Dropout(p=0.1, inplace=False)\n",
            "    (7): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (8): ReLU()\n",
            "  )\n",
            "  (predict_layer): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "LAYER-BY-LAYER BREAKDOWN:\n",
            "======================================================================\n",
            "\n",
            "[GMF Path - Generalized Matrix Factorization]\n",
            "  embed_user_GMF: Embedding(6038, 32)\n",
            "    → Converts user IDs to 32-dimensional vectors\n",
            "  embed_item_GMF: Embedding(3533, 32)\n",
            "    → Converts item IDs to 32-dimensional vectors\n",
            "  Element-wise product: user_emb * item_emb\n",
            "    → Output shape: [batch_size, 32]\n",
            "\n",
            "[MLP Path - Multi-Layer Perceptron]\n",
            "  embed_user_MLP: Embedding(6038, 128)\n",
            "    → Converts user IDs to 128-dimensional vectors\n",
            "  embed_item_MLP: Embedding(3533, 128)\n",
            "    → Converts item IDs to 128-dimensional vectors\n",
            "  Concatenation: [user_emb, item_emb]\n",
            "    → Output shape: [batch_size, 256]\n",
            "\n",
            "  MLP Layers (3 layers):\n",
            "    Layer 1:\n",
            "      Dropout(p=0.1)\n",
            "      Linear(256, 128)\n",
            "      ReLU()\n",
            "      → Output shape: [batch_size, 128]\n",
            "    Layer 2:\n",
            "      Dropout(p=0.1)\n",
            "      Linear(128, 64)\n",
            "      ReLU()\n",
            "      → Output shape: [batch_size, 64]\n",
            "    Layer 3:\n",
            "      Dropout(p=0.1)\n",
            "      Linear(64, 32)\n",
            "      ReLU()\n",
            "      → Output shape: [batch_size, 32]\n",
            "\n",
            "[Prediction Layer]\n",
            "  Input: Concatenated GMF + MLP [64 dimensions]\n",
            "    → GMF: [32 dims] + MLP: [32 dims]\n",
            "  Linear(64, 1)\n",
            "    → Output: [batch_size, 1] (interaction score)\n",
            "    → Higher score = more likely user will like item\n",
            "\n",
            "======================================================================\n",
            "PARAMETER BREAKDOWN:\n",
            "======================================================================\n",
            "\n",
            "GMF Embeddings:\n",
            "  User embeddings: 6,038 × 32 = 193,216 parameters\n",
            "  Item embeddings: 3,533 × 32 = 113,056 parameters\n",
            "  GMF Total: 306,272 parameters\n",
            "\n",
            "MLP Embeddings:\n",
            "  User embeddings: 6,038 × 128 = 772,864 parameters\n",
            "  Item embeddings: 3,533 × 128 = 452,224 parameters\n",
            "  MLP Embeddings Total: 1,225,088 parameters\n",
            "\n",
            "MLP Layers:\n",
            "  Layer 1 (Linear(256, 128)): 32,896 parameters\n",
            "  Layer 2 (Linear(128, 64)): 8,256 parameters\n",
            "  Layer 3 (Linear(64, 32)): 2,080 parameters\n",
            "  MLP Layers Total: 43,232 parameters\n",
            "\n",
            "Prediction Layer:\n",
            "  Linear(64, 1): 65 parameters\n",
            "\n",
            "======================================================================\n",
            "TOTAL MODEL PARAMETERS: 1,574,657\n",
            "Model Size (float32): ~6.01 MB\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 5.2 CREATE AND INITIALIZE THE MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 5.2: Creating and Initializing Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check if we need pre-trained models (for NeuMF-pre)\n",
        "if model_name == 'NeuMF-pre':\n",
        "    # For NeuMF-pre, we would load pre-trained GMF and MLP models\n",
        "    # For now, we'll use NeuMF-end (training from scratch)\n",
        "    print(\"⚠ NeuMF-pre requires pre-trained models.\")\n",
        "    print(\"  Switching to NeuMF-end (training from scratch)...\")\n",
        "    model_name = 'NeuMF-end'\n",
        "\n",
        "# Create the model\n",
        "ncf_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name=model_name,\n",
        "    GMF_model=None,\n",
        "    MLP_model=None\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    ncf_model = ncf_model.cuda()\n",
        "    print(\"✓ Model moved to GPU\")\n",
        "else:\n",
        "    print(\"✓ Model on CPU\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in ncf_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in ncf_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n✓ Model created successfully!\")\n",
        "print(f\"  - Total parameters: {total_params:,}\")\n",
        "print(f\"  - Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Print model architecture\n",
        "print(f\"\\nModel Architecture:\")\n",
        "print(f\"  - Users: {user_num:,}\")\n",
        "print(f\"  - Items: {item_num:,}\")\n",
        "if model_name != 'MLP':\n",
        "    print(f\"  - GMF embeddings: {factor_num} dimensions\")\n",
        "if model_name != 'GMF':\n",
        "    mlp_embed_dim = factor_num * (2 ** (num_layers - 1))\n",
        "    print(f\"  - MLP embeddings: {mlp_embed_dim} dimensions\")\n",
        "    print(f\"  - MLP layers: {num_layers} (with dropout={dropout_rate})\")\n",
        "print(f\"  - Prediction layer: {factor_num if model_name in ['MLP', 'GMF'] else factor_num * 2} → 1\")\n",
        "\n",
        "\n",
        "\n",
        "# Print the full model structure\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"COMPLETE MODEL STRUCTURE:\")\n",
        "print(\"=\" * 70)\n",
        "print(ncf_model)\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Print detailed layer information\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LAYER-BY-LAYER BREAKDOWN:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if model_name != 'MLP':\n",
        "    print(\"\\n[GMF Path - Generalized Matrix Factorization]\")\n",
        "    print(f\"  embed_user_GMF: Embedding({user_num}, {factor_num})\")\n",
        "    print(f\"    → Converts user IDs to {factor_num}-dimensional vectors\")\n",
        "    print(f\"  embed_item_GMF: Embedding({item_num}, {factor_num})\")\n",
        "    print(f\"    → Converts item IDs to {factor_num}-dimensional vectors\")\n",
        "    print(f\"  Element-wise product: user_emb * item_emb\")\n",
        "    print(f\"    → Output shape: [batch_size, {factor_num}]\")\n",
        "\n",
        "if model_name != 'GMF':\n",
        "    mlp_embed_dim = factor_num * (2 ** (num_layers - 1))\n",
        "    print(\"\\n[MLP Path - Multi-Layer Perceptron]\")\n",
        "    print(f\"  embed_user_MLP: Embedding({user_num}, {mlp_embed_dim})\")\n",
        "    print(f\"    → Converts user IDs to {mlp_embed_dim}-dimensional vectors\")\n",
        "    print(f\"  embed_item_MLP: Embedding({item_num}, {mlp_embed_dim})\")\n",
        "    print(f\"    → Converts item IDs to {mlp_embed_dim}-dimensional vectors\")\n",
        "    print(f\"  Concatenation: [user_emb, item_emb]\")\n",
        "    print(f\"    → Output shape: [batch_size, {mlp_embed_dim * 2}]\")\n",
        "    \n",
        "    print(f\"\\n  MLP Layers ({num_layers} layers):\")\n",
        "    for i in range(num_layers):\n",
        "        input_size = factor_num * (2 ** (num_layers - i))\n",
        "        output_size = input_size // 2\n",
        "        print(f\"    Layer {i+1}:\")\n",
        "        print(f\"      Dropout(p={dropout_rate})\")\n",
        "        print(f\"      Linear({input_size}, {output_size})\")\n",
        "        print(f\"      ReLU()\")\n",
        "        print(f\"      → Output shape: [batch_size, {output_size}]\")\n",
        "\n",
        "print(\"\\n[Prediction Layer]\")\n",
        "if model_name == 'GMF':\n",
        "    predict_input = factor_num\n",
        "    print(f\"  Input: GMF output [{factor_num} dimensions]\")\n",
        "elif model_name == 'MLP':\n",
        "    predict_input = factor_num\n",
        "    print(f\"  Input: MLP output [{factor_num} dimensions]\")\n",
        "else:  # NeuMF\n",
        "    predict_input = factor_num * 2\n",
        "    print(f\"  Input: Concatenated GMF + MLP [{factor_num * 2} dimensions]\")\n",
        "    print(f\"    → GMF: [{factor_num} dims] + MLP: [{factor_num} dims]\")\n",
        "\n",
        "print(f\"  Linear({predict_input}, 1)\")\n",
        "print(f\"    → Output: [batch_size, 1] (interaction score)\")\n",
        "print(f\"    → Higher score = more likely user will like item\")\n",
        "\n",
        "# Calculate and print parameter breakdown\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PARAMETER BREAKDOWN:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "total_params = 0\n",
        "if model_name != 'MLP':\n",
        "    gmf_user_params = user_num * factor_num\n",
        "    gmf_item_params = item_num * factor_num\n",
        "    gmf_total = gmf_user_params + gmf_item_params\n",
        "    total_params += gmf_total\n",
        "    print(f\"\\nGMF Embeddings:\")\n",
        "    print(f\"  User embeddings: {user_num:,} × {factor_num} = {gmf_user_params:,} parameters\")\n",
        "    print(f\"  Item embeddings: {item_num:,} × {factor_num} = {gmf_item_params:,} parameters\")\n",
        "    print(f\"  GMF Total: {gmf_total:,} parameters\")\n",
        "\n",
        "if model_name != 'GMF':\n",
        "    mlp_embed_dim = factor_num * (2 ** (num_layers - 1))\n",
        "    mlp_user_params = user_num * mlp_embed_dim\n",
        "    mlp_item_params = item_num * mlp_embed_dim\n",
        "    mlp_embed_total = mlp_user_params + mlp_item_params\n",
        "    total_params += mlp_embed_total\n",
        "    print(f\"\\nMLP Embeddings:\")\n",
        "    print(f\"  User embeddings: {user_num:,} × {mlp_embed_dim} = {mlp_user_params:,} parameters\")\n",
        "    print(f\"  Item embeddings: {item_num:,} × {mlp_embed_dim} = {mlp_item_params:,} parameters\")\n",
        "    print(f\"  MLP Embeddings Total: {mlp_embed_total:,} parameters\")\n",
        "    \n",
        "    # MLP layers parameters\n",
        "    mlp_layer_params = 0\n",
        "    print(f\"\\nMLP Layers:\")\n",
        "    for i in range(num_layers):\n",
        "        input_size = factor_num * (2 ** (num_layers - i))\n",
        "        output_size = input_size // 2\n",
        "        layer_params = (input_size * output_size) + output_size  # weights + bias\n",
        "        mlp_layer_params += layer_params\n",
        "        print(f\"  Layer {i+1} (Linear({input_size}, {output_size})): {layer_params:,} parameters\")\n",
        "    total_params += mlp_layer_params\n",
        "    print(f\"  MLP Layers Total: {mlp_layer_params:,} parameters\")\n",
        "\n",
        "# Prediction layer\n",
        "if model_name in ['MLP', 'GMF']:\n",
        "    predict_input = factor_num\n",
        "else:\n",
        "    predict_input = factor_num * 2\n",
        "predict_params = (predict_input * 1) + 1  # weights + bias\n",
        "total_params += predict_params\n",
        "print(f\"\\nPrediction Layer:\")\n",
        "print(f\"  Linear({predict_input}, 1): {predict_params:,} parameters\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"TOTAL MODEL PARAMETERS: {total_params:,}\")\n",
        "print(f\"Model Size (float32): ~{total_params * 4 / 1024 / 1024:.2f} MB\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 6.1: Hit Rate Metric\n",
            "======================================================================\n",
            "✓ Hit Rate function defined\n",
            "  - Returns 1 if true item is in top-K recommendations\n",
            "  - Returns 0 otherwise\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 6: EVALUATION METRICS\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This step implements evaluation metrics for recommendation systems:\n",
        "- Hit Rate (HR@K): Binary metric - is the true item in top K?\n",
        "- NDCG (Normalized Discounted Cumulative Gain@K): Ranking quality metric\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 6.1 HIT RATE METRIC\n",
        "# ============================================================================\n",
        "\n",
        "def hit(gt_item, pred_items):\n",
        "    \"\"\"\n",
        "    Calculate Hit Rate for a single test case.\n",
        "    \n",
        "    Hit Rate is 1 if the ground truth item is in the predicted top-K items,\n",
        "    otherwise 0.\n",
        "    \n",
        "    Parameters:\n",
        "    - gt_item: Ground truth item ID (the item user actually interacted with)\n",
        "    - pred_items: List of top-K predicted item IDs (recommended items)\n",
        "    \n",
        "    Returns:\n",
        "    - 1 if gt_item is in pred_items, 0 otherwise\n",
        "    \"\"\"\n",
        "    if gt_item in pred_items:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 6.1: Hit Rate Metric\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ Hit Rate function defined\")\n",
        "print(\"  - Returns 1 if true item is in top-K recommendations\")\n",
        "print(\"  - Returns 0 otherwise\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 6.2: NDCG Metric\n",
            "======================================================================\n",
            "✓ NDCG function defined\n",
            "  - Measures ranking quality\n",
            "  - Higher score for items ranked higher\n",
            "  - Returns 0 if true item not in recommendations\n",
            "\n",
            "NDCG Examples:\n",
            "  Top-5 recommendations: [10, 20, 30, 40, 50]\n",
            "  If true item is at position 0: NDCG = 1.000\n",
            "  If true item is at position 2: NDCG = 0.500\n",
            "  If true item is at position 4: NDCG = 0.387\n",
            "  If true item not in list: NDCG = 0.000\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 6.2 NDCG METRIC\n",
        "# ============================================================================\n",
        "\n",
        "def ndcg(gt_item, pred_items):\n",
        "    \"\"\"\n",
        "    Calculate Normalized Discounted Cumulative Gain (NDCG) for a single test case.\n",
        "    \n",
        "    NDCG measures ranking quality by:\n",
        "    1. Giving more weight to items ranked higher (position matters)\n",
        "    2. Using logarithmic discounting (relevance decreases with position)\n",
        "    \n",
        "    Formula: NDCG = 1 / log2(position + 2)\n",
        "    - Position 0 (top): 1 / log2(2) = 1.0\n",
        "    - Position 1: 1 / log2(3) ≈ 0.63\n",
        "    - Position 2: 1 / log2(4) = 0.5\n",
        "    - Position 9: 1 / log2(11) ≈ 0.29\n",
        "    \n",
        "    Parameters:\n",
        "    - gt_item: Ground truth item ID (the item user actually interacted with)\n",
        "    - pred_items: List of top-K predicted item IDs (recommended items)\n",
        "    \n",
        "    Returns:\n",
        "    - NDCG score (0.0 to 1.0) if gt_item is in pred_items\n",
        "    - 0.0 if gt_item is not in pred_items\n",
        "    \"\"\"\n",
        "    if gt_item in pred_items:\n",
        "        # Find the position (index) of the ground truth item\n",
        "        index = pred_items.index(gt_item)\n",
        "        # Calculate NDCG: 1 / log2(position + 2)\n",
        "        # +2 because: position 0 should give 1/log2(2) = 1.0\n",
        "        return np.reciprocal(np.log2(index + 2))\n",
        "    return 0.0\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 6.2: NDCG Metric\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ NDCG function defined\")\n",
        "print(\"  - Measures ranking quality\")\n",
        "print(\"  - Higher score for items ranked higher\")\n",
        "print(\"  - Returns 0 if true item not in recommendations\")\n",
        "\n",
        "# Example to demonstrate NDCG\n",
        "print(\"\\nNDCG Examples:\")\n",
        "example_items = [10, 20, 30, 40, 50]\n",
        "print(f\"  Top-5 recommendations: {example_items}\")\n",
        "print(f\"  If true item is at position 0: NDCG = {ndcg(10, example_items):.3f}\")\n",
        "print(f\"  If true item is at position 2: NDCG = {ndcg(30, example_items):.3f}\")\n",
        "print(f\"  If true item is at position 4: NDCG = {ndcg(50, example_items):.3f}\")\n",
        "print(f\"  If true item not in list: NDCG = {ndcg(99, example_items):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 6.3: Evaluation Function\n",
            "======================================================================\n",
            "✓ Evaluation function defined\n",
            "  - Evaluates model on test data\n",
            "  - Calculates average Hit Rate and NDCG\n",
            "  - Works with GPU or CPU\n",
            "\n",
            "✓ Evaluation ready (device: cpu)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 6.3 EVALUATION FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_metrics(model, test_loader, top_k, device='cuda'):\n",
        "    \"\"\"\n",
        "    Evaluate model performance on test data.\n",
        "    \n",
        "    This function:\n",
        "    1. For each test case (1 positive + 99 negatives):\n",
        "       - Gets model predictions for all 100 items\n",
        "       - Selects top-K items with highest scores\n",
        "       - Checks if the true item is in top-K (Hit Rate)\n",
        "       - Calculates NDCG based on true item's position\n",
        "    2. Averages metrics across all test cases\n",
        "    \n",
        "    Parameters:\n",
        "    - model: Trained NCF model\n",
        "    - test_loader: DataLoader with test data\n",
        "    - top_k: Number of top items to consider (e.g., 10)\n",
        "    - device: 'cuda' or 'cpu'\n",
        "    \n",
        "    Returns:\n",
        "    - mean_HR: Average Hit Rate across all test cases\n",
        "    - mean_NDCG: Average NDCG across all test cases\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode (disables dropout)\n",
        "    \n",
        "    HR_list = []  # List to store Hit Rate for each test case\n",
        "    NDCG_list = []  # List to store NDCG for each test case\n",
        "    \n",
        "    with torch.no_grad():  # Disable gradient computation (faster, saves memory)\n",
        "        for user, item, label in test_loader:\n",
        "            # Move data to device (GPU or CPU)\n",
        "            if device == 'cuda' and torch.cuda.is_available():\n",
        "                user = user.cuda()\n",
        "                item = item.cuda()\n",
        "            else:\n",
        "                device = 'cpu'\n",
        "            \n",
        "            # Get model predictions for all items in this batch\n",
        "            # Batch size = test_num_ng + 1 = 100 (1 positive + 99 negatives)\n",
        "            predictions = model(user, item)  # [100] tensor of scores\n",
        "            \n",
        "            # Get top-K items with highest prediction scores\n",
        "            # torch.topk returns (values, indices)\n",
        "            _, indices = torch.topk(predictions, top_k)\n",
        "            \n",
        "            # Get the actual item IDs for top-K recommendations\n",
        "            # torch.take extracts items at given indices\n",
        "            recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
        "            \n",
        "            # The first item in the batch is always the positive (true) item\n",
        "            gt_item = item[0].item()  # Ground truth item ID\n",
        "            \n",
        "            # Calculate metrics for this test case\n",
        "            HR_list.append(hit(gt_item, recommends))\n",
        "            NDCG_list.append(ndcg(gt_item, recommends))\n",
        "    \n",
        "    # Calculate average metrics across all test cases\n",
        "    # print(f\"HR_list: {HR_list}\")\n",
        "    # print(f\"NDCG_list: {NDCG_list}\")\n",
        "    try:\n",
        "        mean_HR = np.mean(HR_list)\n",
        "        mean_NDCG = np.mean(NDCG_list)\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating mean: {e}\")\n",
        "        mean_HR = 0\n",
        "        mean_NDCG = 0\n",
        "    \n",
        "    return mean_HR, mean_NDCG\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 6.3: Evaluation Function\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ Evaluation function defined\")\n",
        "print(\"  - Evaluates model on test data\")\n",
        "print(\"  - Calculates average Hit Rate and NDCG\")\n",
        "print(\"  - Works with GPU or CPU\")\n",
        "\n",
        "# Determine device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"\\n✓ Evaluation ready (device: {device})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 7.1: Setting Up Loss Function and Optimizer\n",
            "======================================================================\n",
            "✓ Loss function: BCEWithLogitsLoss\n",
            "  - For binary classification (like/dislike)\n",
            "  - Combines sigmoid + cross-entropy for stability\n",
            "\n",
            "✓ Optimizer: Adam\n",
            "  - Learning rate: 0.001\n",
            "  - Adaptive: adjusts learning rate automatically\n",
            "\n",
            "✓ Model ready for training\n",
            "  - Trainable parameters: 1,574,657\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 7: TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This step implements the complete training process for the NCF model.\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 7.1 SETUP LOSS FUNCTION AND OPTIMIZER\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 7.1: Setting Up Loss Function and Optimizer\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Loss Function: Binary Cross-Entropy with Logits\n",
        "# This combines sigmoid activation + binary cross-entropy loss\n",
        "# More numerically stable than applying sigmoid separately\n",
        "# \n",
        "# Why BCEWithLogitsLoss?\n",
        "# - Our task: Predict if user will like item (binary: 1 or 0)\n",
        "# - Model outputs raw scores (logits), not probabilities\n",
        "# - BCEWithLogitsLoss applies sigmoid internally and computes loss\n",
        "# - More stable than: sigmoid(output) then BCE(sigmoid_output, label)\n",
        "\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "print(\"✓ Loss function: BCEWithLogitsLoss\")\n",
        "print(\"  - For binary classification (like/dislike)\")\n",
        "print(\"  - Combines sigmoid + cross-entropy for stability\")\n",
        "\n",
        "# Optimizer: Adam (Adaptive Moment Estimation)\n",
        "# Adam is an adaptive learning rate optimizer that:\n",
        "# - Adjusts learning rate per parameter\n",
        "# - Uses momentum (moving average of gradients)\n",
        "# - Works well for most deep learning tasks\n",
        "# - Better than SGD for this problem\n",
        "\n",
        "optimizer = optim.Adam(ncf_model.parameters(), lr=learning_rate)\n",
        "print(f\"\\n✓ Optimizer: Adam\")\n",
        "print(f\"  - Learning rate: {learning_rate}\")\n",
        "print(f\"  - Adaptive: adjusts learning rate automatically\")\n",
        "\n",
        "# Count trainable parameters\n",
        "total_params = sum(p.numel() for p in ncf_model.parameters() if p.requires_grad)\n",
        "print(f\"\\n✓ Model ready for training\")\n",
        "print(f\"  - Trainable parameters: {total_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 7.2: Starting Training\n",
            "======================================================================\n",
            "Training for 20 epochs...\n",
            "Model: NeuMF-end\n",
            "Device: cpu\n",
            "======================================================================\n",
            "\n",
            "Epoch 1/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.4929\n",
            "  Batch 200/8989 - Loss: 0.4423\n",
            "  Batch 300/8989 - Loss: 0.4201\n",
            "  Batch 400/8989 - Loss: 0.4061\n",
            "  Batch 500/8989 - Loss: 0.3976\n",
            "  Batch 600/8989 - Loss: 0.3920\n",
            "  Batch 700/8989 - Loss: 0.3874\n",
            "  Batch 800/8989 - Loss: 0.3839\n",
            "  Batch 900/8989 - Loss: 0.3813\n",
            "  Batch 1000/8989 - Loss: 0.3789\n",
            "  Batch 1100/8989 - Loss: 0.3766\n",
            "  Batch 1200/8989 - Loss: 0.3749\n",
            "  Batch 1300/8989 - Loss: 0.3728\n",
            "  Batch 1400/8989 - Loss: 0.3718\n",
            "  Batch 1500/8989 - Loss: 0.3707\n",
            "  Batch 1600/8989 - Loss: 0.3695\n",
            "  Batch 1700/8989 - Loss: 0.3684\n",
            "  Batch 1800/8989 - Loss: 0.3676\n",
            "  Batch 1900/8989 - Loss: 0.3666\n",
            "  Batch 2000/8989 - Loss: 0.3658\n",
            "  Batch 2100/8989 - Loss: 0.3647\n",
            "  Batch 2200/8989 - Loss: 0.3639\n",
            "  Batch 2300/8989 - Loss: 0.3633\n",
            "  Batch 2400/8989 - Loss: 0.3627\n",
            "  Batch 2500/8989 - Loss: 0.3620\n",
            "  Batch 2600/8989 - Loss: 0.3613\n",
            "  Batch 2700/8989 - Loss: 0.3605\n",
            "  Batch 2800/8989 - Loss: 0.3598\n",
            "  Batch 2900/8989 - Loss: 0.3589\n",
            "  Batch 3000/8989 - Loss: 0.3582\n",
            "  Batch 3100/8989 - Loss: 0.3572\n",
            "  Batch 3200/8989 - Loss: 0.3565\n",
            "  Batch 3300/8989 - Loss: 0.3559\n",
            "  Batch 3400/8989 - Loss: 0.3551\n",
            "  Batch 3500/8989 - Loss: 0.3541\n",
            "  Batch 3600/8989 - Loss: 0.3534\n",
            "  Batch 3700/8989 - Loss: 0.3527\n",
            "  Batch 3800/8989 - Loss: 0.3520\n",
            "  Batch 3900/8989 - Loss: 0.3512\n",
            "  Batch 4000/8989 - Loss: 0.3505\n",
            "  Batch 4100/8989 - Loss: 0.3498\n",
            "  Batch 4200/8989 - Loss: 0.3491\n",
            "  Batch 4300/8989 - Loss: 0.3485\n",
            "  Batch 4400/8989 - Loss: 0.3479\n",
            "  Batch 4500/8989 - Loss: 0.3473\n",
            "  Batch 4600/8989 - Loss: 0.3467\n",
            "  Batch 4700/8989 - Loss: 0.3461\n",
            "  Batch 4800/8989 - Loss: 0.3454\n",
            "  Batch 4900/8989 - Loss: 0.3448\n",
            "  Batch 5000/8989 - Loss: 0.3441\n",
            "  Batch 5100/8989 - Loss: 0.3435\n",
            "  Batch 5200/8989 - Loss: 0.3428\n",
            "  Batch 5300/8989 - Loss: 0.3423\n",
            "  Batch 5400/8989 - Loss: 0.3417\n",
            "  Batch 5500/8989 - Loss: 0.3411\n",
            "  Batch 5600/8989 - Loss: 0.3405\n",
            "  Batch 5700/8989 - Loss: 0.3399\n",
            "  Batch 5800/8989 - Loss: 0.3394\n",
            "  Batch 5900/8989 - Loss: 0.3388\n",
            "  Batch 6000/8989 - Loss: 0.3384\n",
            "  Batch 6100/8989 - Loss: 0.3380\n",
            "  Batch 6200/8989 - Loss: 0.3375\n",
            "  Batch 6300/8989 - Loss: 0.3370\n",
            "  Batch 6400/8989 - Loss: 0.3365\n",
            "  Batch 6500/8989 - Loss: 0.3361\n",
            "  Batch 6600/8989 - Loss: 0.3357\n",
            "  Batch 6700/8989 - Loss: 0.3352\n",
            "  Batch 6800/8989 - Loss: 0.3348\n",
            "  Batch 6900/8989 - Loss: 0.3343\n",
            "  Batch 7000/8989 - Loss: 0.3339\n",
            "  Batch 7100/8989 - Loss: 0.3334\n",
            "  Batch 7200/8989 - Loss: 0.3331\n",
            "  Batch 7300/8989 - Loss: 0.3327\n",
            "  Batch 7400/8989 - Loss: 0.3323\n",
            "  Batch 7500/8989 - Loss: 0.3318\n",
            "  Batch 7600/8989 - Loss: 0.3315\n",
            "  Batch 7700/8989 - Loss: 0.3310\n",
            "  Batch 7800/8989 - Loss: 0.3305\n",
            "  Batch 7900/8989 - Loss: 0.3301\n",
            "  Batch 8000/8989 - Loss: 0.3297\n",
            "  Batch 8100/8989 - Loss: 0.3293\n",
            "  Batch 8200/8989 - Loss: 0.3290\n",
            "  Batch 8300/8989 - Loss: 0.3286\n",
            "  Batch 8400/8989 - Loss: 0.3282\n",
            "  Batch 8500/8989 - Loss: 0.3278\n",
            "  Batch 8600/8989 - Loss: 0.3273\n",
            "  Batch 8700/8989 - Loss: 0.3270\n",
            "  Batch 8800/8989 - Loss: 0.3265\n",
            "  Batch 8900/8989 - Loss: 0.3262\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:43\n",
            "  Loss: 0.3259\n",
            "  HR@10: 0.7027\n",
            "  NDCG@10: 0.4244\n",
            "  ✓ New best model! (HR@10: 0.7027)\n",
            "  ✓ Model saved to /Users/abbas/Documents/Codes/thesis/recommender/src/../models/NeuMF-end.pth\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 2/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2826\n",
            "  Batch 200/8989 - Loss: 0.2810\n",
            "  Batch 300/8989 - Loss: 0.2786\n",
            "  Batch 400/8989 - Loss: 0.2774\n",
            "  Batch 500/8989 - Loss: 0.2770\n",
            "  Batch 600/8989 - Loss: 0.2769\n",
            "  Batch 700/8989 - Loss: 0.2767\n",
            "  Batch 800/8989 - Loss: 0.2774\n",
            "  Batch 900/8989 - Loss: 0.2777\n",
            "  Batch 1000/8989 - Loss: 0.2772\n",
            "  Batch 1100/8989 - Loss: 0.2770\n",
            "  Batch 1200/8989 - Loss: 0.2769\n",
            "  Batch 1300/8989 - Loss: 0.2771\n",
            "  Batch 1400/8989 - Loss: 0.2774\n",
            "  Batch 1500/8989 - Loss: 0.2770\n",
            "  Batch 1600/8989 - Loss: 0.2771\n",
            "  Batch 1700/8989 - Loss: 0.2774\n",
            "  Batch 1800/8989 - Loss: 0.2773\n",
            "  Batch 1900/8989 - Loss: 0.2770\n",
            "  Batch 2000/8989 - Loss: 0.2771\n",
            "  Batch 2100/8989 - Loss: 0.2769\n",
            "  Batch 2200/8989 - Loss: 0.2771\n",
            "  Batch 2300/8989 - Loss: 0.2768\n",
            "  Batch 2400/8989 - Loss: 0.2767\n",
            "  Batch 2500/8989 - Loss: 0.2767\n",
            "  Batch 2600/8989 - Loss: 0.2768\n",
            "  Batch 2700/8989 - Loss: 0.2767\n",
            "  Batch 2800/8989 - Loss: 0.2765\n",
            "  Batch 2900/8989 - Loss: 0.2765\n",
            "  Batch 3000/8989 - Loss: 0.2762\n",
            "  Batch 3100/8989 - Loss: 0.2761\n",
            "  Batch 3200/8989 - Loss: 0.2761\n",
            "  Batch 3300/8989 - Loss: 0.2762\n",
            "  Batch 3400/8989 - Loss: 0.2763\n",
            "  Batch 3500/8989 - Loss: 0.2761\n",
            "  Batch 3600/8989 - Loss: 0.2760\n",
            "  Batch 3700/8989 - Loss: 0.2760\n",
            "  Batch 3800/8989 - Loss: 0.2759\n",
            "  Batch 3900/8989 - Loss: 0.2759\n",
            "  Batch 4000/8989 - Loss: 0.2759\n",
            "  Batch 4100/8989 - Loss: 0.2758\n",
            "  Batch 4200/8989 - Loss: 0.2758\n",
            "  Batch 4300/8989 - Loss: 0.2757\n",
            "  Batch 4400/8989 - Loss: 0.2757\n",
            "  Batch 4500/8989 - Loss: 0.2757\n",
            "  Batch 4600/8989 - Loss: 0.2756\n",
            "  Batch 4700/8989 - Loss: 0.2755\n",
            "  Batch 4800/8989 - Loss: 0.2753\n",
            "  Batch 4900/8989 - Loss: 0.2752\n",
            "  Batch 5000/8989 - Loss: 0.2751\n",
            "  Batch 5100/8989 - Loss: 0.2750\n",
            "  Batch 5200/8989 - Loss: 0.2750\n",
            "  Batch 5300/8989 - Loss: 0.2748\n",
            "  Batch 5400/8989 - Loss: 0.2747\n",
            "  Batch 5500/8989 - Loss: 0.2747\n",
            "  Batch 5600/8989 - Loss: 0.2745\n",
            "  Batch 5700/8989 - Loss: 0.2745\n",
            "  Batch 5800/8989 - Loss: 0.2744\n",
            "  Batch 5900/8989 - Loss: 0.2744\n",
            "  Batch 6000/8989 - Loss: 0.2743\n",
            "  Batch 6100/8989 - Loss: 0.2743\n",
            "  Batch 6200/8989 - Loss: 0.2742\n",
            "  Batch 6300/8989 - Loss: 0.2742\n",
            "  Batch 6400/8989 - Loss: 0.2741\n",
            "  Batch 6500/8989 - Loss: 0.2741\n",
            "  Batch 6600/8989 - Loss: 0.2740\n",
            "  Batch 6700/8989 - Loss: 0.2739\n",
            "  Batch 6800/8989 - Loss: 0.2738\n",
            "  Batch 6900/8989 - Loss: 0.2738\n",
            "  Batch 7000/8989 - Loss: 0.2737\n",
            "  Batch 7100/8989 - Loss: 0.2736\n",
            "  Batch 7200/8989 - Loss: 0.2736\n",
            "  Batch 7300/8989 - Loss: 0.2735\n",
            "  Batch 7400/8989 - Loss: 0.2734\n",
            "  Batch 7500/8989 - Loss: 0.2734\n",
            "  Batch 7600/8989 - Loss: 0.2733\n",
            "  Batch 7700/8989 - Loss: 0.2732\n",
            "  Batch 7800/8989 - Loss: 0.2731\n",
            "  Batch 7900/8989 - Loss: 0.2731\n",
            "  Batch 8000/8989 - Loss: 0.2730\n",
            "  Batch 8100/8989 - Loss: 0.2729\n",
            "  Batch 8200/8989 - Loss: 0.2729\n",
            "  Batch 8300/8989 - Loss: 0.2728\n",
            "  Batch 8400/8989 - Loss: 0.2727\n",
            "  Batch 8500/8989 - Loss: 0.2727\n",
            "  Batch 8600/8989 - Loss: 0.2726\n",
            "  Batch 8700/8989 - Loss: 0.2725\n",
            "  Batch 8800/8989 - Loss: 0.2725\n",
            "  Batch 8900/8989 - Loss: 0.2724\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:43\n",
            "  Loss: 0.2724\n",
            "  HR@10: 0.7349\n",
            "  NDCG@10: 0.4512\n",
            "  ✓ New best model! (HR@10: 0.7349)\n",
            "  ✓ Model saved to /Users/abbas/Documents/Codes/thesis/recommender/src/../models/NeuMF-end.pth\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 3/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2463\n",
            "  Batch 200/8989 - Loss: 0.2475\n",
            "  Batch 300/8989 - Loss: 0.2492\n",
            "  Batch 400/8989 - Loss: 0.2483\n",
            "  Batch 500/8989 - Loss: 0.2480\n",
            "  Batch 600/8989 - Loss: 0.2479\n",
            "  Batch 700/8989 - Loss: 0.2479\n",
            "  Batch 800/8989 - Loss: 0.2486\n",
            "  Batch 900/8989 - Loss: 0.2487\n",
            "  Batch 1000/8989 - Loss: 0.2486\n",
            "  Batch 1100/8989 - Loss: 0.2487\n",
            "  Batch 1200/8989 - Loss: 0.2487\n",
            "  Batch 1300/8989 - Loss: 0.2489\n",
            "  Batch 1400/8989 - Loss: 0.2492\n",
            "  Batch 1500/8989 - Loss: 0.2493\n",
            "  Batch 1600/8989 - Loss: 0.2493\n",
            "  Batch 1700/8989 - Loss: 0.2493\n",
            "  Batch 1800/8989 - Loss: 0.2491\n",
            "  Batch 1900/8989 - Loss: 0.2491\n",
            "  Batch 2000/8989 - Loss: 0.2489\n",
            "  Batch 2100/8989 - Loss: 0.2489\n",
            "  Batch 2200/8989 - Loss: 0.2490\n",
            "  Batch 2300/8989 - Loss: 0.2488\n",
            "  Batch 2400/8989 - Loss: 0.2489\n",
            "  Batch 2500/8989 - Loss: 0.2490\n",
            "  Batch 2600/8989 - Loss: 0.2490\n",
            "  Batch 2700/8989 - Loss: 0.2491\n",
            "  Batch 2800/8989 - Loss: 0.2492\n",
            "  Batch 2900/8989 - Loss: 0.2492\n",
            "  Batch 3000/8989 - Loss: 0.2491\n",
            "  Batch 3100/8989 - Loss: 0.2491\n",
            "  Batch 3200/8989 - Loss: 0.2492\n",
            "  Batch 3300/8989 - Loss: 0.2493\n",
            "  Batch 3400/8989 - Loss: 0.2493\n",
            "  Batch 3500/8989 - Loss: 0.2492\n",
            "  Batch 3600/8989 - Loss: 0.2493\n",
            "  Batch 3700/8989 - Loss: 0.2493\n",
            "  Batch 3800/8989 - Loss: 0.2493\n",
            "  Batch 3900/8989 - Loss: 0.2494\n",
            "  Batch 4000/8989 - Loss: 0.2494\n",
            "  Batch 4100/8989 - Loss: 0.2494\n",
            "  Batch 4200/8989 - Loss: 0.2495\n",
            "  Batch 4300/8989 - Loss: 0.2496\n",
            "  Batch 4400/8989 - Loss: 0.2495\n",
            "  Batch 4500/8989 - Loss: 0.2496\n",
            "  Batch 4600/8989 - Loss: 0.2497\n",
            "  Batch 4700/8989 - Loss: 0.2498\n",
            "  Batch 4800/8989 - Loss: 0.2498\n",
            "  Batch 4900/8989 - Loss: 0.2498\n",
            "  Batch 5000/8989 - Loss: 0.2498\n",
            "  Batch 5100/8989 - Loss: 0.2499\n",
            "  Batch 5200/8989 - Loss: 0.2499\n",
            "  Batch 5300/8989 - Loss: 0.2499\n",
            "  Batch 5400/8989 - Loss: 0.2499\n",
            "  Batch 5500/8989 - Loss: 0.2499\n",
            "  Batch 5600/8989 - Loss: 0.2499\n",
            "  Batch 5700/8989 - Loss: 0.2500\n",
            "  Batch 5800/8989 - Loss: 0.2501\n",
            "  Batch 5900/8989 - Loss: 0.2501\n",
            "  Batch 6000/8989 - Loss: 0.2501\n",
            "  Batch 6100/8989 - Loss: 0.2502\n",
            "  Batch 6200/8989 - Loss: 0.2502\n",
            "  Batch 6300/8989 - Loss: 0.2502\n",
            "  Batch 6400/8989 - Loss: 0.2501\n",
            "  Batch 6500/8989 - Loss: 0.2502\n",
            "  Batch 6600/8989 - Loss: 0.2501\n",
            "  Batch 6700/8989 - Loss: 0.2502\n",
            "  Batch 6800/8989 - Loss: 0.2502\n",
            "  Batch 6900/8989 - Loss: 0.2502\n",
            "  Batch 7000/8989 - Loss: 0.2502\n",
            "  Batch 7100/8989 - Loss: 0.2503\n",
            "  Batch 7200/8989 - Loss: 0.2504\n",
            "  Batch 7300/8989 - Loss: 0.2504\n",
            "  Batch 7400/8989 - Loss: 0.2504\n",
            "  Batch 7500/8989 - Loss: 0.2503\n",
            "  Batch 7600/8989 - Loss: 0.2504\n",
            "  Batch 7700/8989 - Loss: 0.2504\n",
            "  Batch 7800/8989 - Loss: 0.2505\n",
            "  Batch 7900/8989 - Loss: 0.2505\n",
            "  Batch 8000/8989 - Loss: 0.2505\n",
            "  Batch 8100/8989 - Loss: 0.2505\n",
            "  Batch 8200/8989 - Loss: 0.2505\n",
            "  Batch 8300/8989 - Loss: 0.2506\n",
            "  Batch 8400/8989 - Loss: 0.2507\n",
            "  Batch 8500/8989 - Loss: 0.2507\n",
            "  Batch 8600/8989 - Loss: 0.2507\n",
            "  Batch 8700/8989 - Loss: 0.2506\n",
            "  Batch 8800/8989 - Loss: 0.2507\n",
            "  Batch 8900/8989 - Loss: 0.2507\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:45\n",
            "  Loss: 0.2507\n",
            "  HR@10: 0.7395\n",
            "  NDCG@10: 0.4549\n",
            "  ✓ New best model! (HR@10: 0.7395)\n",
            "  ✓ Model saved to /Users/abbas/Documents/Codes/thesis/recommender/src/../models/NeuMF-end.pth\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 4/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2298\n",
            "  Batch 200/8989 - Loss: 0.2302\n",
            "  Batch 300/8989 - Loss: 0.2289\n",
            "  Batch 400/8989 - Loss: 0.2293\n",
            "  Batch 500/8989 - Loss: 0.2298\n",
            "  Batch 600/8989 - Loss: 0.2305\n",
            "  Batch 700/8989 - Loss: 0.2311\n",
            "  Batch 800/8989 - Loss: 0.2314\n",
            "  Batch 900/8989 - Loss: 0.2319\n",
            "  Batch 1000/8989 - Loss: 0.2322\n",
            "  Batch 1100/8989 - Loss: 0.2319\n",
            "  Batch 1200/8989 - Loss: 0.2323\n",
            "  Batch 1300/8989 - Loss: 0.2323\n",
            "  Batch 1400/8989 - Loss: 0.2325\n",
            "  Batch 1500/8989 - Loss: 0.2326\n",
            "  Batch 1600/8989 - Loss: 0.2326\n",
            "  Batch 1700/8989 - Loss: 0.2329\n",
            "  Batch 1800/8989 - Loss: 0.2332\n",
            "  Batch 1900/8989 - Loss: 0.2335\n",
            "  Batch 2000/8989 - Loss: 0.2336\n",
            "  Batch 2100/8989 - Loss: 0.2337\n",
            "  Batch 2200/8989 - Loss: 0.2341\n",
            "  Batch 2300/8989 - Loss: 0.2340\n",
            "  Batch 2400/8989 - Loss: 0.2340\n",
            "  Batch 2500/8989 - Loss: 0.2341\n",
            "  Batch 2600/8989 - Loss: 0.2344\n",
            "  Batch 2700/8989 - Loss: 0.2346\n",
            "  Batch 2800/8989 - Loss: 0.2347\n",
            "  Batch 2900/8989 - Loss: 0.2346\n",
            "  Batch 3000/8989 - Loss: 0.2347\n",
            "  Batch 3100/8989 - Loss: 0.2348\n",
            "  Batch 3200/8989 - Loss: 0.2347\n",
            "  Batch 3300/8989 - Loss: 0.2350\n",
            "  Batch 3400/8989 - Loss: 0.2350\n",
            "  Batch 3500/8989 - Loss: 0.2350\n",
            "  Batch 3600/8989 - Loss: 0.2351\n",
            "  Batch 3700/8989 - Loss: 0.2351\n",
            "  Batch 3800/8989 - Loss: 0.2352\n",
            "  Batch 3900/8989 - Loss: 0.2354\n",
            "  Batch 4000/8989 - Loss: 0.2355\n",
            "  Batch 4100/8989 - Loss: 0.2356\n",
            "  Batch 4200/8989 - Loss: 0.2358\n",
            "  Batch 4300/8989 - Loss: 0.2358\n",
            "  Batch 4400/8989 - Loss: 0.2360\n",
            "  Batch 4500/8989 - Loss: 0.2360\n",
            "  Batch 4600/8989 - Loss: 0.2362\n",
            "  Batch 4700/8989 - Loss: 0.2362\n",
            "  Batch 4800/8989 - Loss: 0.2362\n",
            "  Batch 4900/8989 - Loss: 0.2363\n",
            "  Batch 5000/8989 - Loss: 0.2364\n",
            "  Batch 5100/8989 - Loss: 0.2365\n",
            "  Batch 5200/8989 - Loss: 0.2366\n",
            "  Batch 5300/8989 - Loss: 0.2366\n",
            "  Batch 5400/8989 - Loss: 0.2366\n",
            "  Batch 5500/8989 - Loss: 0.2367\n",
            "  Batch 5600/8989 - Loss: 0.2367\n",
            "  Batch 5700/8989 - Loss: 0.2367\n",
            "  Batch 5800/8989 - Loss: 0.2369\n",
            "  Batch 5900/8989 - Loss: 0.2370\n",
            "  Batch 6000/8989 - Loss: 0.2371\n",
            "  Batch 6100/8989 - Loss: 0.2371\n",
            "  Batch 6200/8989 - Loss: 0.2371\n",
            "  Batch 6300/8989 - Loss: 0.2371\n",
            "  Batch 6400/8989 - Loss: 0.2371\n",
            "  Batch 6500/8989 - Loss: 0.2372\n",
            "  Batch 6600/8989 - Loss: 0.2373\n",
            "  Batch 6700/8989 - Loss: 0.2373\n",
            "  Batch 6800/8989 - Loss: 0.2374\n",
            "  Batch 6900/8989 - Loss: 0.2374\n",
            "  Batch 7000/8989 - Loss: 0.2374\n",
            "  Batch 7100/8989 - Loss: 0.2375\n",
            "  Batch 7200/8989 - Loss: 0.2376\n",
            "  Batch 7300/8989 - Loss: 0.2376\n",
            "  Batch 7400/8989 - Loss: 0.2377\n",
            "  Batch 7500/8989 - Loss: 0.2378\n",
            "  Batch 7600/8989 - Loss: 0.2379\n",
            "  Batch 7700/8989 - Loss: 0.2379\n",
            "  Batch 7800/8989 - Loss: 0.2379\n",
            "  Batch 7900/8989 - Loss: 0.2379\n",
            "  Batch 8000/8989 - Loss: 0.2380\n",
            "  Batch 8100/8989 - Loss: 0.2380\n",
            "  Batch 8200/8989 - Loss: 0.2380\n",
            "  Batch 8300/8989 - Loss: 0.2381\n",
            "  Batch 8400/8989 - Loss: 0.2381\n",
            "  Batch 8500/8989 - Loss: 0.2382\n",
            "  Batch 8600/8989 - Loss: 0.2382\n",
            "  Batch 8700/8989 - Loss: 0.2382\n",
            "  Batch 8800/8989 - Loss: 0.2383\n",
            "  Batch 8900/8989 - Loss: 0.2384\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:45\n",
            "  Loss: 0.2384\n",
            "  HR@10: 0.7405\n",
            "  NDCG@10: 0.4564\n",
            "  ✓ New best model! (HR@10: 0.7405)\n",
            "  ✓ Model saved to /Users/abbas/Documents/Codes/thesis/recommender/src/../models/NeuMF-end.pth\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 5/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2166\n",
            "  Batch 200/8989 - Loss: 0.2174\n",
            "  Batch 300/8989 - Loss: 0.2181\n",
            "  Batch 400/8989 - Loss: 0.2182\n",
            "  Batch 500/8989 - Loss: 0.2191\n",
            "  Batch 600/8989 - Loss: 0.2190\n",
            "  Batch 700/8989 - Loss: 0.2185\n",
            "  Batch 800/8989 - Loss: 0.2188\n",
            "  Batch 900/8989 - Loss: 0.2194\n",
            "  Batch 1000/8989 - Loss: 0.2200\n",
            "  Batch 1100/8989 - Loss: 0.2203\n",
            "  Batch 1200/8989 - Loss: 0.2205\n",
            "  Batch 1300/8989 - Loss: 0.2206\n",
            "  Batch 1400/8989 - Loss: 0.2208\n",
            "  Batch 1500/8989 - Loss: 0.2209\n",
            "  Batch 1600/8989 - Loss: 0.2210\n",
            "  Batch 1700/8989 - Loss: 0.2211\n",
            "  Batch 1800/8989 - Loss: 0.2212\n",
            "  Batch 1900/8989 - Loss: 0.2214\n",
            "  Batch 2000/8989 - Loss: 0.2217\n",
            "  Batch 2100/8989 - Loss: 0.2217\n",
            "  Batch 2200/8989 - Loss: 0.2218\n",
            "  Batch 2300/8989 - Loss: 0.2223\n",
            "  Batch 2400/8989 - Loss: 0.2225\n",
            "  Batch 2500/8989 - Loss: 0.2229\n",
            "  Batch 2600/8989 - Loss: 0.2232\n",
            "  Batch 2700/8989 - Loss: 0.2233\n",
            "  Batch 2800/8989 - Loss: 0.2234\n",
            "  Batch 2900/8989 - Loss: 0.2236\n",
            "  Batch 3000/8989 - Loss: 0.2237\n",
            "  Batch 3100/8989 - Loss: 0.2238\n",
            "  Batch 3200/8989 - Loss: 0.2239\n",
            "  Batch 3300/8989 - Loss: 0.2240\n",
            "  Batch 3400/8989 - Loss: 0.2241\n",
            "  Batch 3500/8989 - Loss: 0.2242\n",
            "  Batch 3600/8989 - Loss: 0.2244\n",
            "  Batch 3700/8989 - Loss: 0.2246\n",
            "  Batch 3800/8989 - Loss: 0.2248\n",
            "  Batch 3900/8989 - Loss: 0.2249\n",
            "  Batch 4000/8989 - Loss: 0.2249\n",
            "  Batch 4100/8989 - Loss: 0.2251\n",
            "  Batch 4200/8989 - Loss: 0.2252\n",
            "  Batch 4300/8989 - Loss: 0.2253\n",
            "  Batch 4400/8989 - Loss: 0.2252\n",
            "  Batch 4500/8989 - Loss: 0.2254\n",
            "  Batch 4600/8989 - Loss: 0.2254\n",
            "  Batch 4700/8989 - Loss: 0.2256\n",
            "  Batch 4800/8989 - Loss: 0.2256\n",
            "  Batch 4900/8989 - Loss: 0.2258\n",
            "  Batch 5000/8989 - Loss: 0.2258\n",
            "  Batch 5100/8989 - Loss: 0.2259\n",
            "  Batch 5200/8989 - Loss: 0.2260\n",
            "  Batch 5300/8989 - Loss: 0.2262\n",
            "  Batch 5400/8989 - Loss: 0.2263\n",
            "  Batch 5500/8989 - Loss: 0.2265\n",
            "  Batch 5600/8989 - Loss: 0.2266\n",
            "  Batch 5700/8989 - Loss: 0.2268\n",
            "  Batch 5800/8989 - Loss: 0.2268\n",
            "  Batch 5900/8989 - Loss: 0.2270\n",
            "  Batch 6000/8989 - Loss: 0.2271\n",
            "  Batch 6100/8989 - Loss: 0.2272\n",
            "  Batch 6200/8989 - Loss: 0.2273\n",
            "  Batch 6300/8989 - Loss: 0.2273\n",
            "  Batch 6400/8989 - Loss: 0.2275\n",
            "  Batch 6500/8989 - Loss: 0.2275\n",
            "  Batch 6600/8989 - Loss: 0.2276\n",
            "  Batch 6700/8989 - Loss: 0.2278\n",
            "  Batch 6800/8989 - Loss: 0.2278\n",
            "  Batch 6900/8989 - Loss: 0.2279\n",
            "  Batch 7000/8989 - Loss: 0.2280\n",
            "  Batch 7100/8989 - Loss: 0.2281\n",
            "  Batch 7200/8989 - Loss: 0.2282\n",
            "  Batch 7300/8989 - Loss: 0.2283\n",
            "  Batch 7400/8989 - Loss: 0.2283\n",
            "  Batch 7500/8989 - Loss: 0.2284\n",
            "  Batch 7600/8989 - Loss: 0.2285\n",
            "  Batch 7700/8989 - Loss: 0.2286\n",
            "  Batch 7800/8989 - Loss: 0.2287\n",
            "  Batch 7900/8989 - Loss: 0.2288\n",
            "  Batch 8000/8989 - Loss: 0.2288\n",
            "  Batch 8100/8989 - Loss: 0.2289\n",
            "  Batch 8200/8989 - Loss: 0.2290\n",
            "  Batch 8300/8989 - Loss: 0.2291\n",
            "  Batch 8400/8989 - Loss: 0.2292\n",
            "  Batch 8500/8989 - Loss: 0.2293\n",
            "  Batch 8600/8989 - Loss: 0.2294\n",
            "  Batch 8700/8989 - Loss: 0.2294\n",
            "  Batch 8800/8989 - Loss: 0.2295\n",
            "  Batch 8900/8989 - Loss: 0.2296\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:45\n",
            "  Loss: 0.2297\n",
            "  HR@10: 0.7393\n",
            "  NDCG@10: 0.4538\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 6/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2100\n",
            "  Batch 200/8989 - Loss: 0.2110\n",
            "  Batch 300/8989 - Loss: 0.2125\n",
            "  Batch 400/8989 - Loss: 0.2127\n",
            "  Batch 500/8989 - Loss: 0.2132\n",
            "  Batch 600/8989 - Loss: 0.2129\n",
            "  Batch 700/8989 - Loss: 0.2130\n",
            "  Batch 800/8989 - Loss: 0.2129\n",
            "  Batch 900/8989 - Loss: 0.2133\n",
            "  Batch 1000/8989 - Loss: 0.2133\n",
            "  Batch 1100/8989 - Loss: 0.2136\n",
            "  Batch 1200/8989 - Loss: 0.2135\n",
            "  Batch 1300/8989 - Loss: 0.2135\n",
            "  Batch 1400/8989 - Loss: 0.2138\n",
            "  Batch 1500/8989 - Loss: 0.2140\n",
            "  Batch 1600/8989 - Loss: 0.2140\n",
            "  Batch 1700/8989 - Loss: 0.2142\n",
            "  Batch 1800/8989 - Loss: 0.2144\n",
            "  Batch 1900/8989 - Loss: 0.2146\n",
            "  Batch 2000/8989 - Loss: 0.2147\n",
            "  Batch 2100/8989 - Loss: 0.2149\n",
            "  Batch 2200/8989 - Loss: 0.2152\n",
            "  Batch 2300/8989 - Loss: 0.2154\n",
            "  Batch 2400/8989 - Loss: 0.2155\n",
            "  Batch 2500/8989 - Loss: 0.2158\n",
            "  Batch 2600/8989 - Loss: 0.2158\n",
            "  Batch 2700/8989 - Loss: 0.2160\n",
            "  Batch 2800/8989 - Loss: 0.2160\n",
            "  Batch 2900/8989 - Loss: 0.2163\n",
            "  Batch 3000/8989 - Loss: 0.2163\n",
            "  Batch 3100/8989 - Loss: 0.2164\n",
            "  Batch 3200/8989 - Loss: 0.2166\n",
            "  Batch 3300/8989 - Loss: 0.2169\n",
            "  Batch 3400/8989 - Loss: 0.2170\n",
            "  Batch 3500/8989 - Loss: 0.2173\n",
            "  Batch 3600/8989 - Loss: 0.2174\n",
            "  Batch 3700/8989 - Loss: 0.2176\n",
            "  Batch 3800/8989 - Loss: 0.2179\n",
            "  Batch 3900/8989 - Loss: 0.2180\n",
            "  Batch 4000/8989 - Loss: 0.2180\n",
            "  Batch 4100/8989 - Loss: 0.2182\n",
            "  Batch 4200/8989 - Loss: 0.2184\n",
            "  Batch 4300/8989 - Loss: 0.2187\n",
            "  Batch 4400/8989 - Loss: 0.2187\n",
            "  Batch 4500/8989 - Loss: 0.2188\n",
            "  Batch 4600/8989 - Loss: 0.2190\n",
            "  Batch 4700/8989 - Loss: 0.2191\n",
            "  Batch 4800/8989 - Loss: 0.2193\n",
            "  Batch 4900/8989 - Loss: 0.2195\n",
            "  Batch 5000/8989 - Loss: 0.2196\n",
            "  Batch 5100/8989 - Loss: 0.2199\n",
            "  Batch 5200/8989 - Loss: 0.2200\n",
            "  Batch 5300/8989 - Loss: 0.2202\n",
            "  Batch 5400/8989 - Loss: 0.2202\n",
            "  Batch 5500/8989 - Loss: 0.2203\n",
            "  Batch 5600/8989 - Loss: 0.2204\n",
            "  Batch 5700/8989 - Loss: 0.2205\n",
            "  Batch 5800/8989 - Loss: 0.2207\n",
            "  Batch 5900/8989 - Loss: 0.2208\n",
            "  Batch 6000/8989 - Loss: 0.2209\n",
            "  Batch 6100/8989 - Loss: 0.2210\n",
            "  Batch 6200/8989 - Loss: 0.2210\n",
            "  Batch 6300/8989 - Loss: 0.2211\n",
            "  Batch 6400/8989 - Loss: 0.2212\n",
            "  Batch 6500/8989 - Loss: 0.2213\n",
            "  Batch 6600/8989 - Loss: 0.2214\n",
            "  Batch 6700/8989 - Loss: 0.2216\n",
            "  Batch 6800/8989 - Loss: 0.2216\n",
            "  Batch 6900/8989 - Loss: 0.2216\n",
            "  Batch 7000/8989 - Loss: 0.2218\n",
            "  Batch 7100/8989 - Loss: 0.2219\n",
            "  Batch 7200/8989 - Loss: 0.2221\n",
            "  Batch 7300/8989 - Loss: 0.2221\n",
            "  Batch 7400/8989 - Loss: 0.2222\n",
            "  Batch 7500/8989 - Loss: 0.2223\n",
            "  Batch 7600/8989 - Loss: 0.2224\n",
            "  Batch 7700/8989 - Loss: 0.2225\n",
            "  Batch 7800/8989 - Loss: 0.2226\n",
            "  Batch 7900/8989 - Loss: 0.2227\n",
            "  Batch 8000/8989 - Loss: 0.2229\n",
            "  Batch 8100/8989 - Loss: 0.2230\n",
            "  Batch 8200/8989 - Loss: 0.2231\n",
            "  Batch 8300/8989 - Loss: 0.2232\n",
            "  Batch 8400/8989 - Loss: 0.2234\n",
            "  Batch 8500/8989 - Loss: 0.2235\n",
            "  Batch 8600/8989 - Loss: 0.2235\n",
            "  Batch 8700/8989 - Loss: 0.2236\n",
            "  Batch 8800/8989 - Loss: 0.2237\n",
            "  Batch 8900/8989 - Loss: 0.2237\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:44\n",
            "  Loss: 0.2238\n",
            "  HR@10: 0.7350\n",
            "  NDCG@10: 0.4523\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 7/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2051\n",
            "  Batch 200/8989 - Loss: 0.2029\n",
            "  Batch 300/8989 - Loss: 0.2051\n",
            "  Batch 400/8989 - Loss: 0.2048\n",
            "  Batch 500/8989 - Loss: 0.2047\n",
            "  Batch 600/8989 - Loss: 0.2051\n",
            "  Batch 700/8989 - Loss: 0.2057\n",
            "  Batch 800/8989 - Loss: 0.2064\n",
            "  Batch 900/8989 - Loss: 0.2070\n",
            "  Batch 1000/8989 - Loss: 0.2077\n",
            "  Batch 1100/8989 - Loss: 0.2078\n",
            "  Batch 1200/8989 - Loss: 0.2080\n",
            "  Batch 1300/8989 - Loss: 0.2080\n",
            "  Batch 1400/8989 - Loss: 0.2081\n",
            "  Batch 1500/8989 - Loss: 0.2084\n",
            "  Batch 1600/8989 - Loss: 0.2087\n",
            "  Batch 1700/8989 - Loss: 0.2092\n",
            "  Batch 1800/8989 - Loss: 0.2094\n",
            "  Batch 1900/8989 - Loss: 0.2095\n",
            "  Batch 2000/8989 - Loss: 0.2096\n",
            "  Batch 2100/8989 - Loss: 0.2098\n",
            "  Batch 2200/8989 - Loss: 0.2102\n",
            "  Batch 2300/8989 - Loss: 0.2105\n",
            "  Batch 2400/8989 - Loss: 0.2106\n",
            "  Batch 2500/8989 - Loss: 0.2109\n",
            "  Batch 2600/8989 - Loss: 0.2111\n",
            "  Batch 2700/8989 - Loss: 0.2112\n",
            "  Batch 2800/8989 - Loss: 0.2112\n",
            "  Batch 2900/8989 - Loss: 0.2114\n",
            "  Batch 3000/8989 - Loss: 0.2115\n",
            "  Batch 3100/8989 - Loss: 0.2118\n",
            "  Batch 3200/8989 - Loss: 0.2118\n",
            "  Batch 3300/8989 - Loss: 0.2120\n",
            "  Batch 3400/8989 - Loss: 0.2122\n",
            "  Batch 3500/8989 - Loss: 0.2124\n",
            "  Batch 3600/8989 - Loss: 0.2126\n",
            "  Batch 3700/8989 - Loss: 0.2128\n",
            "  Batch 3800/8989 - Loss: 0.2130\n",
            "  Batch 3900/8989 - Loss: 0.2132\n",
            "  Batch 4000/8989 - Loss: 0.2133\n",
            "  Batch 4100/8989 - Loss: 0.2135\n",
            "  Batch 4200/8989 - Loss: 0.2137\n",
            "  Batch 4300/8989 - Loss: 0.2138\n",
            "  Batch 4400/8989 - Loss: 0.2140\n",
            "  Batch 4500/8989 - Loss: 0.2142\n",
            "  Batch 4600/8989 - Loss: 0.2143\n",
            "  Batch 4700/8989 - Loss: 0.2145\n",
            "  Batch 4800/8989 - Loss: 0.2147\n",
            "  Batch 4900/8989 - Loss: 0.2149\n",
            "  Batch 5000/8989 - Loss: 0.2150\n",
            "  Batch 5100/8989 - Loss: 0.2151\n",
            "  Batch 5200/8989 - Loss: 0.2152\n",
            "  Batch 5300/8989 - Loss: 0.2153\n",
            "  Batch 5400/8989 - Loss: 0.2154\n",
            "  Batch 5500/8989 - Loss: 0.2155\n",
            "  Batch 5600/8989 - Loss: 0.2158\n",
            "  Batch 5700/8989 - Loss: 0.2158\n",
            "  Batch 5800/8989 - Loss: 0.2160\n",
            "  Batch 5900/8989 - Loss: 0.2161\n",
            "  Batch 6000/8989 - Loss: 0.2161\n",
            "  Batch 6100/8989 - Loss: 0.2162\n",
            "  Batch 6200/8989 - Loss: 0.2163\n",
            "  Batch 6300/8989 - Loss: 0.2164\n",
            "  Batch 6400/8989 - Loss: 0.2165\n",
            "  Batch 6500/8989 - Loss: 0.2166\n",
            "  Batch 6600/8989 - Loss: 0.2168\n",
            "  Batch 6700/8989 - Loss: 0.2169\n",
            "  Batch 6800/8989 - Loss: 0.2169\n",
            "  Batch 6900/8989 - Loss: 0.2171\n",
            "  Batch 7000/8989 - Loss: 0.2171\n",
            "  Batch 7100/8989 - Loss: 0.2172\n",
            "  Batch 7200/8989 - Loss: 0.2173\n",
            "  Batch 7300/8989 - Loss: 0.2173\n",
            "  Batch 7400/8989 - Loss: 0.2175\n",
            "  Batch 7500/8989 - Loss: 0.2176\n",
            "  Batch 7600/8989 - Loss: 0.2177\n",
            "  Batch 7700/8989 - Loss: 0.2177\n",
            "  Batch 7800/8989 - Loss: 0.2179\n",
            "  Batch 7900/8989 - Loss: 0.2180\n",
            "  Batch 8000/8989 - Loss: 0.2182\n",
            "  Batch 8100/8989 - Loss: 0.2183\n",
            "  Batch 8200/8989 - Loss: 0.2183\n",
            "  Batch 8300/8989 - Loss: 0.2184\n",
            "  Batch 8400/8989 - Loss: 0.2185\n",
            "  Batch 8500/8989 - Loss: 0.2186\n",
            "  Batch 8600/8989 - Loss: 0.2187\n",
            "  Batch 8700/8989 - Loss: 0.2188\n",
            "  Batch 8800/8989 - Loss: 0.2189\n",
            "  Batch 8900/8989 - Loss: 0.2189\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:45\n",
            "  Loss: 0.2190\n",
            "  HR@10: 0.7341\n",
            "  NDCG@10: 0.4505\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 8/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2120\n",
            "  Batch 200/8989 - Loss: 0.2066\n",
            "  Batch 300/8989 - Loss: 0.2052\n",
            "  Batch 400/8989 - Loss: 0.2042\n",
            "  Batch 500/8989 - Loss: 0.2047\n",
            "  Batch 600/8989 - Loss: 0.2054\n",
            "  Batch 700/8989 - Loss: 0.2053\n",
            "  Batch 800/8989 - Loss: 0.2053\n",
            "  Batch 900/8989 - Loss: 0.2054\n",
            "  Batch 1000/8989 - Loss: 0.2056\n",
            "  Batch 1100/8989 - Loss: 0.2059\n",
            "  Batch 1200/8989 - Loss: 0.2060\n",
            "  Batch 1300/8989 - Loss: 0.2061\n",
            "  Batch 1400/8989 - Loss: 0.2063\n",
            "  Batch 1500/8989 - Loss: 0.2063\n",
            "  Batch 1600/8989 - Loss: 0.2064\n",
            "  Batch 1700/8989 - Loss: 0.2066\n",
            "  Batch 1800/8989 - Loss: 0.2067\n",
            "  Batch 1900/8989 - Loss: 0.2067\n",
            "  Batch 2000/8989 - Loss: 0.2069\n",
            "  Batch 2100/8989 - Loss: 0.2071\n",
            "  Batch 2200/8989 - Loss: 0.2073\n",
            "  Batch 2300/8989 - Loss: 0.2077\n",
            "  Batch 2400/8989 - Loss: 0.2077\n",
            "  Batch 2500/8989 - Loss: 0.2078\n",
            "  Batch 2600/8989 - Loss: 0.2079\n",
            "  Batch 2700/8989 - Loss: 0.2080\n",
            "  Batch 2800/8989 - Loss: 0.2082\n",
            "  Batch 2900/8989 - Loss: 0.2084\n",
            "  Batch 3000/8989 - Loss: 0.2087\n",
            "  Batch 3100/8989 - Loss: 0.2089\n",
            "  Batch 3200/8989 - Loss: 0.2090\n",
            "  Batch 3300/8989 - Loss: 0.2091\n",
            "  Batch 3400/8989 - Loss: 0.2095\n",
            "  Batch 3500/8989 - Loss: 0.2098\n",
            "  Batch 3600/8989 - Loss: 0.2098\n",
            "  Batch 3700/8989 - Loss: 0.2100\n",
            "  Batch 3800/8989 - Loss: 0.2102\n",
            "  Batch 3900/8989 - Loss: 0.2102\n",
            "  Batch 4000/8989 - Loss: 0.2104\n",
            "  Batch 4100/8989 - Loss: 0.2104\n",
            "  Batch 4200/8989 - Loss: 0.2105\n",
            "  Batch 4300/8989 - Loss: 0.2107\n",
            "  Batch 4400/8989 - Loss: 0.2109\n",
            "  Batch 4500/8989 - Loss: 0.2110\n",
            "  Batch 4600/8989 - Loss: 0.2111\n",
            "  Batch 4700/8989 - Loss: 0.2112\n",
            "  Batch 4800/8989 - Loss: 0.2113\n",
            "  Batch 4900/8989 - Loss: 0.2114\n",
            "  Batch 5000/8989 - Loss: 0.2116\n",
            "  Batch 5100/8989 - Loss: 0.2118\n",
            "  Batch 5200/8989 - Loss: 0.2119\n",
            "  Batch 5300/8989 - Loss: 0.2120\n",
            "  Batch 5400/8989 - Loss: 0.2121\n",
            "  Batch 5500/8989 - Loss: 0.2123\n",
            "  Batch 5600/8989 - Loss: 0.2124\n",
            "  Batch 5700/8989 - Loss: 0.2124\n",
            "  Batch 5800/8989 - Loss: 0.2126\n",
            "  Batch 5900/8989 - Loss: 0.2127\n",
            "  Batch 6000/8989 - Loss: 0.2128\n",
            "  Batch 6100/8989 - Loss: 0.2129\n",
            "  Batch 6200/8989 - Loss: 0.2130\n",
            "  Batch 6300/8989 - Loss: 0.2132\n",
            "  Batch 6400/8989 - Loss: 0.2132\n",
            "  Batch 6500/8989 - Loss: 0.2134\n",
            "  Batch 6600/8989 - Loss: 0.2135\n",
            "  Batch 6700/8989 - Loss: 0.2136\n",
            "  Batch 6800/8989 - Loss: 0.2137\n",
            "  Batch 6900/8989 - Loss: 0.2137\n",
            "  Batch 7000/8989 - Loss: 0.2138\n",
            "  Batch 7100/8989 - Loss: 0.2139\n",
            "  Batch 7200/8989 - Loss: 0.2141\n",
            "  Batch 7300/8989 - Loss: 0.2142\n",
            "  Batch 7400/8989 - Loss: 0.2142\n",
            "  Batch 7500/8989 - Loss: 0.2143\n",
            "  Batch 7600/8989 - Loss: 0.2144\n",
            "  Batch 7700/8989 - Loss: 0.2145\n",
            "  Batch 7800/8989 - Loss: 0.2145\n",
            "  Batch 7900/8989 - Loss: 0.2146\n",
            "  Batch 8000/8989 - Loss: 0.2146\n",
            "  Batch 8100/8989 - Loss: 0.2147\n",
            "  Batch 8200/8989 - Loss: 0.2148\n",
            "  Batch 8300/8989 - Loss: 0.2149\n",
            "  Batch 8400/8989 - Loss: 0.2150\n",
            "  Batch 8500/8989 - Loss: 0.2151\n",
            "  Batch 8600/8989 - Loss: 0.2151\n",
            "  Batch 8700/8989 - Loss: 0.2152\n",
            "  Batch 8800/8989 - Loss: 0.2153\n",
            "  Batch 8900/8989 - Loss: 0.2153\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:45\n",
            "  Loss: 0.2153\n",
            "  HR@10: 0.7389\n",
            "  NDCG@10: 0.4544\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 9/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2006\n",
            "  Batch 200/8989 - Loss: 0.2027\n",
            "  Batch 300/8989 - Loss: 0.2010\n",
            "  Batch 400/8989 - Loss: 0.2012\n",
            "  Batch 500/8989 - Loss: 0.2012\n",
            "  Batch 600/8989 - Loss: 0.2009\n",
            "  Batch 700/8989 - Loss: 0.2013\n",
            "  Batch 800/8989 - Loss: 0.2014\n",
            "  Batch 900/8989 - Loss: 0.2015\n",
            "  Batch 1000/8989 - Loss: 0.2014\n",
            "  Batch 1100/8989 - Loss: 0.2012\n",
            "  Batch 1200/8989 - Loss: 0.2015\n",
            "  Batch 1300/8989 - Loss: 0.2015\n",
            "  Batch 1400/8989 - Loss: 0.2012\n",
            "  Batch 1500/8989 - Loss: 0.2016\n",
            "  Batch 1600/8989 - Loss: 0.2018\n",
            "  Batch 1700/8989 - Loss: 0.2021\n",
            "  Batch 1800/8989 - Loss: 0.2023\n",
            "  Batch 1900/8989 - Loss: 0.2024\n",
            "  Batch 2000/8989 - Loss: 0.2025\n",
            "  Batch 2100/8989 - Loss: 0.2029\n",
            "  Batch 2200/8989 - Loss: 0.2029\n",
            "  Batch 2300/8989 - Loss: 0.2033\n",
            "  Batch 2400/8989 - Loss: 0.2035\n",
            "  Batch 2500/8989 - Loss: 0.2037\n",
            "  Batch 2600/8989 - Loss: 0.2039\n",
            "  Batch 2700/8989 - Loss: 0.2039\n",
            "  Batch 2800/8989 - Loss: 0.2039\n",
            "  Batch 2900/8989 - Loss: 0.2041\n",
            "  Batch 3000/8989 - Loss: 0.2044\n",
            "  Batch 3100/8989 - Loss: 0.2046\n",
            "  Batch 3200/8989 - Loss: 0.2047\n",
            "  Batch 3300/8989 - Loss: 0.2050\n",
            "  Batch 3400/8989 - Loss: 0.2052\n",
            "  Batch 3500/8989 - Loss: 0.2054\n",
            "  Batch 3600/8989 - Loss: 0.2055\n",
            "  Batch 3700/8989 - Loss: 0.2056\n",
            "  Batch 3800/8989 - Loss: 0.2058\n",
            "  Batch 3900/8989 - Loss: 0.2059\n",
            "  Batch 4000/8989 - Loss: 0.2061\n",
            "  Batch 4100/8989 - Loss: 0.2062\n",
            "  Batch 4200/8989 - Loss: 0.2063\n",
            "  Batch 4300/8989 - Loss: 0.2064\n",
            "  Batch 4400/8989 - Loss: 0.2065\n",
            "  Batch 4500/8989 - Loss: 0.2067\n",
            "  Batch 4600/8989 - Loss: 0.2068\n",
            "  Batch 4700/8989 - Loss: 0.2070\n",
            "  Batch 4800/8989 - Loss: 0.2072\n",
            "  Batch 4900/8989 - Loss: 0.2075\n",
            "  Batch 5000/8989 - Loss: 0.2077\n",
            "  Batch 5100/8989 - Loss: 0.2077\n",
            "  Batch 5200/8989 - Loss: 0.2079\n",
            "  Batch 5300/8989 - Loss: 0.2081\n",
            "  Batch 5400/8989 - Loss: 0.2083\n",
            "  Batch 5500/8989 - Loss: 0.2083\n",
            "  Batch 5600/8989 - Loss: 0.2085\n",
            "  Batch 5700/8989 - Loss: 0.2085\n",
            "  Batch 5800/8989 - Loss: 0.2086\n",
            "  Batch 5900/8989 - Loss: 0.2089\n",
            "  Batch 6000/8989 - Loss: 0.2090\n",
            "  Batch 6100/8989 - Loss: 0.2091\n",
            "  Batch 6200/8989 - Loss: 0.2092\n",
            "  Batch 6300/8989 - Loss: 0.2093\n",
            "  Batch 6400/8989 - Loss: 0.2095\n",
            "  Batch 6500/8989 - Loss: 0.2096\n",
            "  Batch 6600/8989 - Loss: 0.2098\n",
            "  Batch 6700/8989 - Loss: 0.2099\n",
            "  Batch 6800/8989 - Loss: 0.2100\n",
            "  Batch 6900/8989 - Loss: 0.2102\n",
            "  Batch 7000/8989 - Loss: 0.2102\n",
            "  Batch 7100/8989 - Loss: 0.2103\n",
            "  Batch 7200/8989 - Loss: 0.2104\n",
            "  Batch 7300/8989 - Loss: 0.2106\n",
            "  Batch 7400/8989 - Loss: 0.2106\n",
            "  Batch 7500/8989 - Loss: 0.2107\n",
            "  Batch 7600/8989 - Loss: 0.2108\n",
            "  Batch 7700/8989 - Loss: 0.2108\n",
            "  Batch 7800/8989 - Loss: 0.2109\n",
            "  Batch 7900/8989 - Loss: 0.2110\n",
            "  Batch 8000/8989 - Loss: 0.2111\n",
            "  Batch 8100/8989 - Loss: 0.2112\n",
            "  Batch 8200/8989 - Loss: 0.2113\n",
            "  Batch 8300/8989 - Loss: 0.2113\n",
            "  Batch 8400/8989 - Loss: 0.2115\n",
            "  Batch 8500/8989 - Loss: 0.2115\n",
            "  Batch 8600/8989 - Loss: 0.2117\n",
            "  Batch 8700/8989 - Loss: 0.2118\n",
            "  Batch 8800/8989 - Loss: 0.2120\n",
            "  Batch 8900/8989 - Loss: 0.2120\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:48\n",
            "  Loss: 0.2121\n",
            "  HR@10: 0.7331\n",
            "  NDCG@10: 0.4492\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 10/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1958\n",
            "  Batch 200/8989 - Loss: 0.1960\n",
            "  Batch 300/8989 - Loss: 0.1973\n",
            "  Batch 400/8989 - Loss: 0.1990\n",
            "  Batch 500/8989 - Loss: 0.1990\n",
            "  Batch 600/8989 - Loss: 0.1986\n",
            "  Batch 700/8989 - Loss: 0.1980\n",
            "  Batch 800/8989 - Loss: 0.1984\n",
            "  Batch 900/8989 - Loss: 0.1990\n",
            "  Batch 1000/8989 - Loss: 0.1990\n",
            "  Batch 1100/8989 - Loss: 0.1996\n",
            "  Batch 1200/8989 - Loss: 0.1993\n",
            "  Batch 1300/8989 - Loss: 0.1996\n",
            "  Batch 1400/8989 - Loss: 0.2001\n",
            "  Batch 1500/8989 - Loss: 0.2002\n",
            "  Batch 1600/8989 - Loss: 0.2001\n",
            "  Batch 1700/8989 - Loss: 0.2004\n",
            "  Batch 1800/8989 - Loss: 0.2005\n",
            "  Batch 1900/8989 - Loss: 0.2007\n",
            "  Batch 2000/8989 - Loss: 0.2009\n",
            "  Batch 2100/8989 - Loss: 0.2013\n",
            "  Batch 2200/8989 - Loss: 0.2014\n",
            "  Batch 2300/8989 - Loss: 0.2016\n",
            "  Batch 2400/8989 - Loss: 0.2018\n",
            "  Batch 2500/8989 - Loss: 0.2020\n",
            "  Batch 2600/8989 - Loss: 0.2021\n",
            "  Batch 2700/8989 - Loss: 0.2022\n",
            "  Batch 2800/8989 - Loss: 0.2024\n",
            "  Batch 2900/8989 - Loss: 0.2025\n",
            "  Batch 3000/8989 - Loss: 0.2027\n",
            "  Batch 3100/8989 - Loss: 0.2028\n",
            "  Batch 3200/8989 - Loss: 0.2030\n",
            "  Batch 3300/8989 - Loss: 0.2032\n",
            "  Batch 3400/8989 - Loss: 0.2033\n",
            "  Batch 3500/8989 - Loss: 0.2035\n",
            "  Batch 3600/8989 - Loss: 0.2035\n",
            "  Batch 3700/8989 - Loss: 0.2037\n",
            "  Batch 3800/8989 - Loss: 0.2038\n",
            "  Batch 3900/8989 - Loss: 0.2040\n",
            "  Batch 4000/8989 - Loss: 0.2041\n",
            "  Batch 4100/8989 - Loss: 0.2043\n",
            "  Batch 4200/8989 - Loss: 0.2044\n",
            "  Batch 4300/8989 - Loss: 0.2044\n",
            "  Batch 4400/8989 - Loss: 0.2045\n",
            "  Batch 4500/8989 - Loss: 0.2046\n",
            "  Batch 4600/8989 - Loss: 0.2047\n",
            "  Batch 4700/8989 - Loss: 0.2047\n",
            "  Batch 4800/8989 - Loss: 0.2048\n",
            "  Batch 4900/8989 - Loss: 0.2049\n",
            "  Batch 5000/8989 - Loss: 0.2050\n",
            "  Batch 5100/8989 - Loss: 0.2051\n",
            "  Batch 5200/8989 - Loss: 0.2053\n",
            "  Batch 5300/8989 - Loss: 0.2053\n",
            "  Batch 5400/8989 - Loss: 0.2055\n",
            "  Batch 5500/8989 - Loss: 0.2056\n",
            "  Batch 5600/8989 - Loss: 0.2057\n",
            "  Batch 5700/8989 - Loss: 0.2058\n",
            "  Batch 5800/8989 - Loss: 0.2059\n",
            "  Batch 5900/8989 - Loss: 0.2060\n",
            "  Batch 6000/8989 - Loss: 0.2062\n",
            "  Batch 6100/8989 - Loss: 0.2063\n",
            "  Batch 6200/8989 - Loss: 0.2064\n",
            "  Batch 6300/8989 - Loss: 0.2065\n",
            "  Batch 6400/8989 - Loss: 0.2066\n",
            "  Batch 6500/8989 - Loss: 0.2067\n",
            "  Batch 6600/8989 - Loss: 0.2068\n",
            "  Batch 6700/8989 - Loss: 0.2069\n",
            "  Batch 6800/8989 - Loss: 0.2070\n",
            "  Batch 6900/8989 - Loss: 0.2071\n",
            "  Batch 7000/8989 - Loss: 0.2073\n",
            "  Batch 7100/8989 - Loss: 0.2074\n",
            "  Batch 7200/8989 - Loss: 0.2075\n",
            "  Batch 7300/8989 - Loss: 0.2076\n",
            "  Batch 7400/8989 - Loss: 0.2077\n",
            "  Batch 7500/8989 - Loss: 0.2078\n",
            "  Batch 7600/8989 - Loss: 0.2079\n",
            "  Batch 7700/8989 - Loss: 0.2080\n",
            "  Batch 7800/8989 - Loss: 0.2082\n",
            "  Batch 7900/8989 - Loss: 0.2082\n",
            "  Batch 8000/8989 - Loss: 0.2083\n",
            "  Batch 8100/8989 - Loss: 0.2085\n",
            "  Batch 8200/8989 - Loss: 0.2085\n",
            "  Batch 8300/8989 - Loss: 0.2086\n",
            "  Batch 8400/8989 - Loss: 0.2087\n",
            "  Batch 8500/8989 - Loss: 0.2088\n",
            "  Batch 8600/8989 - Loss: 0.2089\n",
            "  Batch 8700/8989 - Loss: 0.2090\n",
            "  Batch 8800/8989 - Loss: 0.2090\n",
            "  Batch 8900/8989 - Loss: 0.2091\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:48\n",
            "  Loss: 0.2091\n",
            "  HR@10: 0.7339\n",
            "  NDCG@10: 0.4507\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 11/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1931\n",
            "  Batch 200/8989 - Loss: 0.1939\n",
            "  Batch 300/8989 - Loss: 0.1922\n",
            "  Batch 400/8989 - Loss: 0.1935\n",
            "  Batch 500/8989 - Loss: 0.1944\n",
            "  Batch 600/8989 - Loss: 0.1949\n",
            "  Batch 700/8989 - Loss: 0.1949\n",
            "  Batch 800/8989 - Loss: 0.1950\n",
            "  Batch 900/8989 - Loss: 0.1955\n",
            "  Batch 1000/8989 - Loss: 0.1958\n",
            "  Batch 1100/8989 - Loss: 0.1959\n",
            "  Batch 1200/8989 - Loss: 0.1959\n",
            "  Batch 1300/8989 - Loss: 0.1962\n",
            "  Batch 1400/8989 - Loss: 0.1961\n",
            "  Batch 1500/8989 - Loss: 0.1957\n",
            "  Batch 1600/8989 - Loss: 0.1957\n",
            "  Batch 1700/8989 - Loss: 0.1960\n",
            "  Batch 1800/8989 - Loss: 0.1965\n",
            "  Batch 1900/8989 - Loss: 0.1965\n",
            "  Batch 2000/8989 - Loss: 0.1967\n",
            "  Batch 2100/8989 - Loss: 0.1966\n",
            "  Batch 2200/8989 - Loss: 0.1967\n",
            "  Batch 2300/8989 - Loss: 0.1969\n",
            "  Batch 2400/8989 - Loss: 0.1971\n",
            "  Batch 2500/8989 - Loss: 0.1974\n",
            "  Batch 2600/8989 - Loss: 0.1975\n",
            "  Batch 2700/8989 - Loss: 0.1976\n",
            "  Batch 2800/8989 - Loss: 0.1977\n",
            "  Batch 2900/8989 - Loss: 0.1982\n",
            "  Batch 3000/8989 - Loss: 0.1984\n",
            "  Batch 3100/8989 - Loss: 0.1987\n",
            "  Batch 3200/8989 - Loss: 0.1990\n",
            "  Batch 3300/8989 - Loss: 0.1992\n",
            "  Batch 3400/8989 - Loss: 0.1994\n",
            "  Batch 3500/8989 - Loss: 0.1995\n",
            "  Batch 3600/8989 - Loss: 0.1997\n",
            "  Batch 3700/8989 - Loss: 0.1999\n",
            "  Batch 3800/8989 - Loss: 0.2001\n",
            "  Batch 3900/8989 - Loss: 0.2003\n",
            "  Batch 4000/8989 - Loss: 0.2005\n",
            "  Batch 4100/8989 - Loss: 0.2006\n",
            "  Batch 4200/8989 - Loss: 0.2008\n",
            "  Batch 4300/8989 - Loss: 0.2010\n",
            "  Batch 4400/8989 - Loss: 0.2010\n",
            "  Batch 4500/8989 - Loss: 0.2012\n",
            "  Batch 4600/8989 - Loss: 0.2013\n",
            "  Batch 4700/8989 - Loss: 0.2015\n",
            "  Batch 4800/8989 - Loss: 0.2016\n",
            "  Batch 4900/8989 - Loss: 0.2017\n",
            "  Batch 5000/8989 - Loss: 0.2019\n",
            "  Batch 5100/8989 - Loss: 0.2021\n",
            "  Batch 5200/8989 - Loss: 0.2022\n",
            "  Batch 5300/8989 - Loss: 0.2024\n",
            "  Batch 5400/8989 - Loss: 0.2024\n",
            "  Batch 5500/8989 - Loss: 0.2026\n",
            "  Batch 5600/8989 - Loss: 0.2026\n",
            "  Batch 5700/8989 - Loss: 0.2027\n",
            "  Batch 5800/8989 - Loss: 0.2027\n",
            "  Batch 5900/8989 - Loss: 0.2029\n",
            "  Batch 6000/8989 - Loss: 0.2029\n",
            "  Batch 6100/8989 - Loss: 0.2030\n",
            "  Batch 6200/8989 - Loss: 0.2032\n",
            "  Batch 6300/8989 - Loss: 0.2033\n",
            "  Batch 6400/8989 - Loss: 0.2034\n",
            "  Batch 6500/8989 - Loss: 0.2035\n",
            "  Batch 6600/8989 - Loss: 0.2036\n",
            "  Batch 6700/8989 - Loss: 0.2037\n",
            "  Batch 6800/8989 - Loss: 0.2038\n",
            "  Batch 6900/8989 - Loss: 0.2039\n",
            "  Batch 7000/8989 - Loss: 0.2041\n",
            "  Batch 7100/8989 - Loss: 0.2042\n",
            "  Batch 7200/8989 - Loss: 0.2043\n",
            "  Batch 7300/8989 - Loss: 0.2043\n",
            "  Batch 7400/8989 - Loss: 0.2044\n",
            "  Batch 7500/8989 - Loss: 0.2045\n",
            "  Batch 7600/8989 - Loss: 0.2047\n",
            "  Batch 7700/8989 - Loss: 0.2049\n",
            "  Batch 7800/8989 - Loss: 0.2049\n",
            "  Batch 7900/8989 - Loss: 0.2050\n",
            "  Batch 8000/8989 - Loss: 0.2051\n",
            "  Batch 8100/8989 - Loss: 0.2052\n",
            "  Batch 8200/8989 - Loss: 0.2053\n",
            "  Batch 8300/8989 - Loss: 0.2055\n",
            "  Batch 8400/8989 - Loss: 0.2055\n",
            "  Batch 8500/8989 - Loss: 0.2056\n",
            "  Batch 8600/8989 - Loss: 0.2057\n",
            "  Batch 8700/8989 - Loss: 0.2057\n",
            "  Batch 8800/8989 - Loss: 0.2058\n",
            "  Batch 8900/8989 - Loss: 0.2060\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:46\n",
            "  Loss: 0.2060\n",
            "  HR@10: 0.7343\n",
            "  NDCG@10: 0.4501\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 12/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1906\n",
            "  Batch 200/8989 - Loss: 0.1904\n",
            "  Batch 300/8989 - Loss: 0.1915\n",
            "  Batch 400/8989 - Loss: 0.1923\n",
            "  Batch 500/8989 - Loss: 0.1933\n",
            "  Batch 600/8989 - Loss: 0.1934\n",
            "  Batch 700/8989 - Loss: 0.1934\n",
            "  Batch 800/8989 - Loss: 0.1935\n",
            "  Batch 900/8989 - Loss: 0.1935\n",
            "  Batch 1000/8989 - Loss: 0.1942\n",
            "  Batch 1100/8989 - Loss: 0.1940\n",
            "  Batch 1200/8989 - Loss: 0.1941\n",
            "  Batch 1300/8989 - Loss: 0.1942\n",
            "  Batch 1400/8989 - Loss: 0.1940\n",
            "  Batch 1500/8989 - Loss: 0.1945\n",
            "  Batch 1600/8989 - Loss: 0.1948\n",
            "  Batch 1700/8989 - Loss: 0.1950\n",
            "  Batch 1800/8989 - Loss: 0.1951\n",
            "  Batch 1900/8989 - Loss: 0.1953\n",
            "  Batch 2000/8989 - Loss: 0.1955\n",
            "  Batch 2100/8989 - Loss: 0.1954\n",
            "  Batch 2200/8989 - Loss: 0.1957\n",
            "  Batch 2300/8989 - Loss: 0.1959\n",
            "  Batch 2400/8989 - Loss: 0.1962\n",
            "  Batch 2500/8989 - Loss: 0.1964\n",
            "  Batch 2600/8989 - Loss: 0.1968\n",
            "  Batch 2700/8989 - Loss: 0.1969\n",
            "  Batch 2800/8989 - Loss: 0.1972\n",
            "  Batch 2900/8989 - Loss: 0.1971\n",
            "  Batch 3000/8989 - Loss: 0.1973\n",
            "  Batch 3100/8989 - Loss: 0.1975\n",
            "  Batch 3200/8989 - Loss: 0.1976\n",
            "  Batch 3300/8989 - Loss: 0.1977\n",
            "  Batch 3400/8989 - Loss: 0.1979\n",
            "  Batch 3500/8989 - Loss: 0.1981\n",
            "  Batch 3600/8989 - Loss: 0.1983\n",
            "  Batch 3700/8989 - Loss: 0.1984\n",
            "  Batch 3800/8989 - Loss: 0.1986\n",
            "  Batch 3900/8989 - Loss: 0.1987\n",
            "  Batch 4000/8989 - Loss: 0.1988\n",
            "  Batch 4100/8989 - Loss: 0.1990\n",
            "  Batch 4200/8989 - Loss: 0.1989\n",
            "  Batch 4300/8989 - Loss: 0.1991\n",
            "  Batch 4400/8989 - Loss: 0.1992\n",
            "  Batch 4500/8989 - Loss: 0.1994\n",
            "  Batch 4600/8989 - Loss: 0.1995\n",
            "  Batch 4700/8989 - Loss: 0.1996\n",
            "  Batch 4800/8989 - Loss: 0.1997\n",
            "  Batch 4900/8989 - Loss: 0.1999\n",
            "  Batch 5000/8989 - Loss: 0.2000\n",
            "  Batch 5100/8989 - Loss: 0.2002\n",
            "  Batch 5200/8989 - Loss: 0.2003\n",
            "  Batch 5300/8989 - Loss: 0.2003\n",
            "  Batch 5400/8989 - Loss: 0.2003\n",
            "  Batch 5500/8989 - Loss: 0.2003\n",
            "  Batch 5600/8989 - Loss: 0.2004\n",
            "  Batch 5700/8989 - Loss: 0.2005\n",
            "  Batch 5800/8989 - Loss: 0.2007\n",
            "  Batch 5900/8989 - Loss: 0.2007\n",
            "  Batch 6000/8989 - Loss: 0.2008\n",
            "  Batch 6100/8989 - Loss: 0.2009\n",
            "  Batch 6200/8989 - Loss: 0.2010\n",
            "  Batch 6300/8989 - Loss: 0.2011\n",
            "  Batch 6400/8989 - Loss: 0.2013\n",
            "  Batch 6500/8989 - Loss: 0.2014\n",
            "  Batch 6600/8989 - Loss: 0.2015\n",
            "  Batch 6700/8989 - Loss: 0.2015\n",
            "  Batch 6800/8989 - Loss: 0.2017\n",
            "  Batch 6900/8989 - Loss: 0.2018\n",
            "  Batch 7000/8989 - Loss: 0.2018\n",
            "  Batch 7100/8989 - Loss: 0.2019\n",
            "  Batch 7200/8989 - Loss: 0.2021\n",
            "  Batch 7300/8989 - Loss: 0.2021\n",
            "  Batch 7400/8989 - Loss: 0.2023\n",
            "  Batch 7500/8989 - Loss: 0.2023\n",
            "  Batch 7600/8989 - Loss: 0.2024\n",
            "  Batch 7700/8989 - Loss: 0.2024\n",
            "  Batch 7800/8989 - Loss: 0.2025\n",
            "  Batch 7900/8989 - Loss: 0.2025\n",
            "  Batch 8000/8989 - Loss: 0.2026\n",
            "  Batch 8100/8989 - Loss: 0.2026\n",
            "  Batch 8200/8989 - Loss: 0.2028\n",
            "  Batch 8300/8989 - Loss: 0.2028\n",
            "  Batch 8400/8989 - Loss: 0.2029\n",
            "  Batch 8500/8989 - Loss: 0.2029\n",
            "  Batch 8600/8989 - Loss: 0.2030\n",
            "  Batch 8700/8989 - Loss: 0.2031\n",
            "  Batch 8800/8989 - Loss: 0.2032\n",
            "  Batch 8900/8989 - Loss: 0.2033\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:45\n",
            "  Loss: 0.2034\n",
            "  HR@10: 0.7327\n",
            "  NDCG@10: 0.4495\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 13/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1910\n",
            "  Batch 200/8989 - Loss: 0.1908\n",
            "  Batch 300/8989 - Loss: 0.1906\n",
            "  Batch 400/8989 - Loss: 0.1911\n",
            "  Batch 500/8989 - Loss: 0.1906\n",
            "  Batch 600/8989 - Loss: 0.1897\n",
            "  Batch 700/8989 - Loss: 0.1901\n",
            "  Batch 800/8989 - Loss: 0.1899\n",
            "  Batch 900/8989 - Loss: 0.1904\n",
            "  Batch 1000/8989 - Loss: 0.1901\n",
            "  Batch 1100/8989 - Loss: 0.1899\n",
            "  Batch 1200/8989 - Loss: 0.1903\n",
            "  Batch 1300/8989 - Loss: 0.1903\n",
            "  Batch 1400/8989 - Loss: 0.1906\n",
            "  Batch 1500/8989 - Loss: 0.1908\n",
            "  Batch 1600/8989 - Loss: 0.1909\n",
            "  Batch 1700/8989 - Loss: 0.1909\n",
            "  Batch 1800/8989 - Loss: 0.1911\n",
            "  Batch 1900/8989 - Loss: 0.1913\n",
            "  Batch 2000/8989 - Loss: 0.1913\n",
            "  Batch 2100/8989 - Loss: 0.1914\n",
            "  Batch 2200/8989 - Loss: 0.1915\n",
            "  Batch 2300/8989 - Loss: 0.1917\n",
            "  Batch 2400/8989 - Loss: 0.1919\n",
            "  Batch 2500/8989 - Loss: 0.1921\n",
            "  Batch 2600/8989 - Loss: 0.1921\n",
            "  Batch 2700/8989 - Loss: 0.1923\n",
            "  Batch 2800/8989 - Loss: 0.1924\n",
            "  Batch 2900/8989 - Loss: 0.1926\n",
            "  Batch 3000/8989 - Loss: 0.1929\n",
            "  Batch 3100/8989 - Loss: 0.1932\n",
            "  Batch 3200/8989 - Loss: 0.1935\n",
            "  Batch 3300/8989 - Loss: 0.1937\n",
            "  Batch 3400/8989 - Loss: 0.1937\n",
            "  Batch 3500/8989 - Loss: 0.1939\n",
            "  Batch 3600/8989 - Loss: 0.1942\n",
            "  Batch 3700/8989 - Loss: 0.1944\n",
            "  Batch 3800/8989 - Loss: 0.1947\n",
            "  Batch 3900/8989 - Loss: 0.1948\n",
            "  Batch 4000/8989 - Loss: 0.1951\n",
            "  Batch 4100/8989 - Loss: 0.1953\n",
            "  Batch 4200/8989 - Loss: 0.1956\n",
            "  Batch 4300/8989 - Loss: 0.1957\n",
            "  Batch 4400/8989 - Loss: 0.1959\n",
            "  Batch 4500/8989 - Loss: 0.1960\n",
            "  Batch 4600/8989 - Loss: 0.1961\n",
            "  Batch 4700/8989 - Loss: 0.1963\n",
            "  Batch 4800/8989 - Loss: 0.1965\n",
            "  Batch 4900/8989 - Loss: 0.1966\n",
            "  Batch 5000/8989 - Loss: 0.1968\n",
            "  Batch 5100/8989 - Loss: 0.1969\n",
            "  Batch 5200/8989 - Loss: 0.1971\n",
            "  Batch 5300/8989 - Loss: 0.1972\n",
            "  Batch 5400/8989 - Loss: 0.1972\n",
            "  Batch 5500/8989 - Loss: 0.1973\n",
            "  Batch 5600/8989 - Loss: 0.1974\n",
            "  Batch 5700/8989 - Loss: 0.1975\n",
            "  Batch 5800/8989 - Loss: 0.1977\n",
            "  Batch 5900/8989 - Loss: 0.1978\n",
            "  Batch 6000/8989 - Loss: 0.1980\n",
            "  Batch 6100/8989 - Loss: 0.1981\n",
            "  Batch 6200/8989 - Loss: 0.1982\n",
            "  Batch 6300/8989 - Loss: 0.1982\n",
            "  Batch 6400/8989 - Loss: 0.1984\n",
            "  Batch 6500/8989 - Loss: 0.1985\n",
            "  Batch 6600/8989 - Loss: 0.1987\n",
            "  Batch 6700/8989 - Loss: 0.1988\n",
            "  Batch 6800/8989 - Loss: 0.1990\n",
            "  Batch 6900/8989 - Loss: 0.1992\n",
            "  Batch 7000/8989 - Loss: 0.1991\n",
            "  Batch 7100/8989 - Loss: 0.1992\n",
            "  Batch 7200/8989 - Loss: 0.1993\n",
            "  Batch 7300/8989 - Loss: 0.1994\n",
            "  Batch 7400/8989 - Loss: 0.1996\n",
            "  Batch 7500/8989 - Loss: 0.1996\n",
            "  Batch 7600/8989 - Loss: 0.1997\n",
            "  Batch 7700/8989 - Loss: 0.1998\n",
            "  Batch 7800/8989 - Loss: 0.1999\n",
            "  Batch 7900/8989 - Loss: 0.2000\n",
            "  Batch 8000/8989 - Loss: 0.2001\n",
            "  Batch 8100/8989 - Loss: 0.2001\n",
            "  Batch 8200/8989 - Loss: 0.2002\n",
            "  Batch 8300/8989 - Loss: 0.2003\n",
            "  Batch 8400/8989 - Loss: 0.2004\n",
            "  Batch 8500/8989 - Loss: 0.2005\n",
            "  Batch 8600/8989 - Loss: 0.2006\n",
            "  Batch 8700/8989 - Loss: 0.2007\n",
            "  Batch 8800/8989 - Loss: 0.2008\n",
            "  Batch 8900/8989 - Loss: 0.2009\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:43\n",
            "  Loss: 0.2010\n",
            "  HR@10: 0.7363\n",
            "  NDCG@10: 0.4536\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 14/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1880\n",
            "  Batch 200/8989 - Loss: 0.1864\n",
            "  Batch 300/8989 - Loss: 0.1874\n",
            "  Batch 400/8989 - Loss: 0.1864\n",
            "  Batch 500/8989 - Loss: 0.1863\n",
            "  Batch 600/8989 - Loss: 0.1867\n",
            "  Batch 700/8989 - Loss: 0.1866\n",
            "  Batch 800/8989 - Loss: 0.1870\n",
            "  Batch 900/8989 - Loss: 0.1874\n",
            "  Batch 1000/8989 - Loss: 0.1875\n",
            "  Batch 1100/8989 - Loss: 0.1878\n",
            "  Batch 1200/8989 - Loss: 0.1882\n",
            "  Batch 1300/8989 - Loss: 0.1882\n",
            "  Batch 1400/8989 - Loss: 0.1884\n",
            "  Batch 1500/8989 - Loss: 0.1887\n",
            "  Batch 1600/8989 - Loss: 0.1891\n",
            "  Batch 1700/8989 - Loss: 0.1894\n",
            "  Batch 1800/8989 - Loss: 0.1898\n",
            "  Batch 1900/8989 - Loss: 0.1897\n",
            "  Batch 2000/8989 - Loss: 0.1899\n",
            "  Batch 2100/8989 - Loss: 0.1899\n",
            "  Batch 2200/8989 - Loss: 0.1901\n",
            "  Batch 2300/8989 - Loss: 0.1906\n",
            "  Batch 2400/8989 - Loss: 0.1907\n",
            "  Batch 2500/8989 - Loss: 0.1908\n",
            "  Batch 2600/8989 - Loss: 0.1911\n",
            "  Batch 2700/8989 - Loss: 0.1912\n",
            "  Batch 2800/8989 - Loss: 0.1915\n",
            "  Batch 2900/8989 - Loss: 0.1914\n",
            "  Batch 3000/8989 - Loss: 0.1917\n",
            "  Batch 3100/8989 - Loss: 0.1918\n",
            "  Batch 3200/8989 - Loss: 0.1920\n",
            "  Batch 3300/8989 - Loss: 0.1921\n",
            "  Batch 3400/8989 - Loss: 0.1924\n",
            "  Batch 3500/8989 - Loss: 0.1925\n",
            "  Batch 3600/8989 - Loss: 0.1927\n",
            "  Batch 3700/8989 - Loss: 0.1929\n",
            "  Batch 3800/8989 - Loss: 0.1931\n",
            "  Batch 3900/8989 - Loss: 0.1931\n",
            "  Batch 4000/8989 - Loss: 0.1933\n",
            "  Batch 4100/8989 - Loss: 0.1934\n",
            "  Batch 4200/8989 - Loss: 0.1936\n",
            "  Batch 4300/8989 - Loss: 0.1938\n",
            "  Batch 4400/8989 - Loss: 0.1939\n",
            "  Batch 4500/8989 - Loss: 0.1941\n",
            "  Batch 4600/8989 - Loss: 0.1942\n",
            "  Batch 4700/8989 - Loss: 0.1942\n",
            "  Batch 4800/8989 - Loss: 0.1944\n",
            "  Batch 4900/8989 - Loss: 0.1945\n",
            "  Batch 5000/8989 - Loss: 0.1945\n",
            "  Batch 5100/8989 - Loss: 0.1947\n",
            "  Batch 5200/8989 - Loss: 0.1949\n",
            "  Batch 5300/8989 - Loss: 0.1950\n",
            "  Batch 5400/8989 - Loss: 0.1951\n",
            "  Batch 5500/8989 - Loss: 0.1953\n",
            "  Batch 5600/8989 - Loss: 0.1954\n",
            "  Batch 5700/8989 - Loss: 0.1955\n",
            "  Batch 5800/8989 - Loss: 0.1957\n",
            "  Batch 5900/8989 - Loss: 0.1958\n",
            "  Batch 6000/8989 - Loss: 0.1959\n",
            "  Batch 6100/8989 - Loss: 0.1960\n",
            "  Batch 6200/8989 - Loss: 0.1961\n",
            "  Batch 6300/8989 - Loss: 0.1962\n",
            "  Batch 6400/8989 - Loss: 0.1963\n",
            "  Batch 6500/8989 - Loss: 0.1964\n",
            "  Batch 6600/8989 - Loss: 0.1965\n",
            "  Batch 6700/8989 - Loss: 0.1966\n",
            "  Batch 6800/8989 - Loss: 0.1968\n",
            "  Batch 6900/8989 - Loss: 0.1969\n",
            "  Batch 7000/8989 - Loss: 0.1970\n",
            "  Batch 7100/8989 - Loss: 0.1972\n",
            "  Batch 7200/8989 - Loss: 0.1973\n",
            "  Batch 7300/8989 - Loss: 0.1974\n",
            "  Batch 7400/8989 - Loss: 0.1975\n",
            "  Batch 7500/8989 - Loss: 0.1976\n",
            "  Batch 7600/8989 - Loss: 0.1977\n",
            "  Batch 7700/8989 - Loss: 0.1978\n",
            "  Batch 7800/8989 - Loss: 0.1978\n",
            "  Batch 7900/8989 - Loss: 0.1979\n",
            "  Batch 8000/8989 - Loss: 0.1980\n",
            "  Batch 8100/8989 - Loss: 0.1981\n",
            "  Batch 8200/8989 - Loss: 0.1982\n",
            "  Batch 8300/8989 - Loss: 0.1983\n",
            "  Batch 8400/8989 - Loss: 0.1984\n",
            "  Batch 8500/8989 - Loss: 0.1985\n",
            "  Batch 8600/8989 - Loss: 0.1987\n",
            "  Batch 8700/8989 - Loss: 0.1988\n",
            "  Batch 8800/8989 - Loss: 0.1989\n",
            "  Batch 8900/8989 - Loss: 0.1990\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:42\n",
            "  Loss: 0.1991\n",
            "  HR@10: 0.7338\n",
            "  NDCG@10: 0.4493\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 15/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1791\n",
            "  Batch 200/8989 - Loss: 0.1816\n",
            "  Batch 300/8989 - Loss: 0.1813\n",
            "  Batch 400/8989 - Loss: 0.1808\n",
            "  Batch 500/8989 - Loss: 0.1827\n",
            "  Batch 600/8989 - Loss: 0.1828\n",
            "  Batch 700/8989 - Loss: 0.1835\n",
            "  Batch 800/8989 - Loss: 0.1839\n",
            "  Batch 900/8989 - Loss: 0.1841\n",
            "  Batch 1000/8989 - Loss: 0.1845\n",
            "  Batch 1100/8989 - Loss: 0.1850\n",
            "  Batch 1200/8989 - Loss: 0.1853\n",
            "  Batch 1300/8989 - Loss: 0.1856\n",
            "  Batch 1400/8989 - Loss: 0.1860\n",
            "  Batch 1500/8989 - Loss: 0.1862\n",
            "  Batch 1600/8989 - Loss: 0.1869\n",
            "  Batch 1700/8989 - Loss: 0.1870\n",
            "  Batch 1800/8989 - Loss: 0.1869\n",
            "  Batch 1900/8989 - Loss: 0.1871\n",
            "  Batch 2000/8989 - Loss: 0.1871\n",
            "  Batch 2100/8989 - Loss: 0.1870\n",
            "  Batch 2200/8989 - Loss: 0.1872\n",
            "  Batch 2300/8989 - Loss: 0.1875\n",
            "  Batch 2400/8989 - Loss: 0.1878\n",
            "  Batch 2500/8989 - Loss: 0.1880\n",
            "  Batch 2600/8989 - Loss: 0.1884\n",
            "  Batch 2700/8989 - Loss: 0.1886\n",
            "  Batch 2800/8989 - Loss: 0.1887\n",
            "  Batch 2900/8989 - Loss: 0.1889\n",
            "  Batch 3000/8989 - Loss: 0.1892\n",
            "  Batch 3100/8989 - Loss: 0.1892\n",
            "  Batch 3200/8989 - Loss: 0.1895\n",
            "  Batch 3300/8989 - Loss: 0.1897\n",
            "  Batch 3400/8989 - Loss: 0.1899\n",
            "  Batch 3500/8989 - Loss: 0.1900\n",
            "  Batch 3600/8989 - Loss: 0.1902\n",
            "  Batch 3700/8989 - Loss: 0.1902\n",
            "  Batch 3800/8989 - Loss: 0.1905\n",
            "  Batch 3900/8989 - Loss: 0.1907\n",
            "  Batch 4000/8989 - Loss: 0.1910\n",
            "  Batch 4100/8989 - Loss: 0.1911\n",
            "  Batch 4200/8989 - Loss: 0.1913\n",
            "  Batch 4300/8989 - Loss: 0.1913\n",
            "  Batch 4400/8989 - Loss: 0.1915\n",
            "  Batch 4500/8989 - Loss: 0.1917\n",
            "  Batch 4600/8989 - Loss: 0.1918\n",
            "  Batch 4700/8989 - Loss: 0.1921\n",
            "  Batch 4800/8989 - Loss: 0.1922\n",
            "  Batch 4900/8989 - Loss: 0.1923\n",
            "  Batch 5000/8989 - Loss: 0.1924\n",
            "  Batch 5100/8989 - Loss: 0.1925\n",
            "  Batch 5200/8989 - Loss: 0.1927\n",
            "  Batch 5300/8989 - Loss: 0.1928\n",
            "  Batch 5400/8989 - Loss: 0.1929\n",
            "  Batch 5500/8989 - Loss: 0.1931\n",
            "  Batch 5600/8989 - Loss: 0.1932\n",
            "  Batch 5700/8989 - Loss: 0.1934\n",
            "  Batch 5800/8989 - Loss: 0.1935\n",
            "  Batch 5900/8989 - Loss: 0.1936\n",
            "  Batch 6000/8989 - Loss: 0.1938\n",
            "  Batch 6100/8989 - Loss: 0.1939\n",
            "  Batch 6200/8989 - Loss: 0.1941\n",
            "  Batch 6300/8989 - Loss: 0.1943\n",
            "  Batch 6400/8989 - Loss: 0.1944\n",
            "  Batch 6500/8989 - Loss: 0.1945\n",
            "  Batch 6600/8989 - Loss: 0.1946\n",
            "  Batch 6700/8989 - Loss: 0.1947\n",
            "  Batch 6800/8989 - Loss: 0.1948\n",
            "  Batch 6900/8989 - Loss: 0.1950\n",
            "  Batch 7000/8989 - Loss: 0.1951\n",
            "  Batch 7100/8989 - Loss: 0.1951\n",
            "  Batch 7200/8989 - Loss: 0.1952\n",
            "  Batch 7300/8989 - Loss: 0.1954\n",
            "  Batch 7400/8989 - Loss: 0.1955\n",
            "  Batch 7500/8989 - Loss: 0.1956\n",
            "  Batch 7600/8989 - Loss: 0.1957\n",
            "  Batch 7700/8989 - Loss: 0.1959\n",
            "  Batch 7800/8989 - Loss: 0.1960\n",
            "  Batch 7900/8989 - Loss: 0.1961\n",
            "  Batch 8000/8989 - Loss: 0.1961\n",
            "  Batch 8100/8989 - Loss: 0.1962\n",
            "  Batch 8200/8989 - Loss: 0.1964\n",
            "  Batch 8300/8989 - Loss: 0.1965\n",
            "  Batch 8400/8989 - Loss: 0.1966\n",
            "  Batch 8500/8989 - Loss: 0.1967\n",
            "  Batch 8600/8989 - Loss: 0.1968\n",
            "  Batch 8700/8989 - Loss: 0.1970\n",
            "  Batch 8800/8989 - Loss: 0.1971\n",
            "  Batch 8900/8989 - Loss: 0.1971\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:43\n",
            "  Loss: 0.1973\n",
            "  HR@10: 0.7339\n",
            "  NDCG@10: 0.4492\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 16/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1833\n",
            "  Batch 200/8989 - Loss: 0.1854\n",
            "  Batch 300/8989 - Loss: 0.1852\n",
            "  Batch 400/8989 - Loss: 0.1852\n",
            "  Batch 500/8989 - Loss: 0.1858\n",
            "  Batch 600/8989 - Loss: 0.1857\n",
            "  Batch 700/8989 - Loss: 0.1860\n",
            "  Batch 800/8989 - Loss: 0.1862\n",
            "  Batch 900/8989 - Loss: 0.1867\n",
            "  Batch 1000/8989 - Loss: 0.1871\n",
            "  Batch 1100/8989 - Loss: 0.1869\n",
            "  Batch 1200/8989 - Loss: 0.1867\n",
            "  Batch 1300/8989 - Loss: 0.1867\n",
            "  Batch 1400/8989 - Loss: 0.1870\n",
            "  Batch 1500/8989 - Loss: 0.1871\n",
            "  Batch 1600/8989 - Loss: 0.1872\n",
            "  Batch 1700/8989 - Loss: 0.1873\n",
            "  Batch 1800/8989 - Loss: 0.1871\n",
            "  Batch 1900/8989 - Loss: 0.1871\n",
            "  Batch 2000/8989 - Loss: 0.1873\n",
            "  Batch 2100/8989 - Loss: 0.1874\n",
            "  Batch 2200/8989 - Loss: 0.1878\n",
            "  Batch 2300/8989 - Loss: 0.1880\n",
            "  Batch 2400/8989 - Loss: 0.1883\n",
            "  Batch 2500/8989 - Loss: 0.1883\n",
            "  Batch 2600/8989 - Loss: 0.1883\n",
            "  Batch 2700/8989 - Loss: 0.1885\n",
            "  Batch 2800/8989 - Loss: 0.1886\n",
            "  Batch 2900/8989 - Loss: 0.1887\n",
            "  Batch 3000/8989 - Loss: 0.1889\n",
            "  Batch 3100/8989 - Loss: 0.1893\n",
            "  Batch 3200/8989 - Loss: 0.1893\n",
            "  Batch 3300/8989 - Loss: 0.1895\n",
            "  Batch 3400/8989 - Loss: 0.1895\n",
            "  Batch 3500/8989 - Loss: 0.1898\n",
            "  Batch 3600/8989 - Loss: 0.1900\n",
            "  Batch 3700/8989 - Loss: 0.1903\n",
            "  Batch 3800/8989 - Loss: 0.1904\n",
            "  Batch 3900/8989 - Loss: 0.1906\n",
            "  Batch 4000/8989 - Loss: 0.1906\n",
            "  Batch 4100/8989 - Loss: 0.1906\n",
            "  Batch 4200/8989 - Loss: 0.1907\n",
            "  Batch 4300/8989 - Loss: 0.1908\n",
            "  Batch 4400/8989 - Loss: 0.1909\n",
            "  Batch 4500/8989 - Loss: 0.1910\n",
            "  Batch 4600/8989 - Loss: 0.1911\n",
            "  Batch 4700/8989 - Loss: 0.1911\n",
            "  Batch 4800/8989 - Loss: 0.1912\n",
            "  Batch 4900/8989 - Loss: 0.1913\n",
            "  Batch 5000/8989 - Loss: 0.1915\n",
            "  Batch 5100/8989 - Loss: 0.1916\n",
            "  Batch 5200/8989 - Loss: 0.1917\n",
            "  Batch 5300/8989 - Loss: 0.1918\n",
            "  Batch 5400/8989 - Loss: 0.1918\n",
            "  Batch 5500/8989 - Loss: 0.1920\n",
            "  Batch 5600/8989 - Loss: 0.1921\n",
            "  Batch 5700/8989 - Loss: 0.1923\n",
            "  Batch 5800/8989 - Loss: 0.1924\n",
            "  Batch 5900/8989 - Loss: 0.1925\n",
            "  Batch 6000/8989 - Loss: 0.1926\n",
            "  Batch 6100/8989 - Loss: 0.1927\n",
            "  Batch 6200/8989 - Loss: 0.1927\n",
            "  Batch 6300/8989 - Loss: 0.1929\n",
            "  Batch 6400/8989 - Loss: 0.1930\n",
            "  Batch 6500/8989 - Loss: 0.1932\n",
            "  Batch 6600/8989 - Loss: 0.1932\n",
            "  Batch 6700/8989 - Loss: 0.1934\n",
            "  Batch 6800/8989 - Loss: 0.1934\n",
            "  Batch 6900/8989 - Loss: 0.1935\n",
            "  Batch 7000/8989 - Loss: 0.1935\n",
            "  Batch 7100/8989 - Loss: 0.1935\n",
            "  Batch 7200/8989 - Loss: 0.1936\n",
            "  Batch 7300/8989 - Loss: 0.1938\n",
            "  Batch 7400/8989 - Loss: 0.1940\n",
            "  Batch 7500/8989 - Loss: 0.1941\n",
            "  Batch 7600/8989 - Loss: 0.1942\n",
            "  Batch 7700/8989 - Loss: 0.1943\n",
            "  Batch 7800/8989 - Loss: 0.1943\n",
            "  Batch 7900/8989 - Loss: 0.1945\n",
            "  Batch 8000/8989 - Loss: 0.1945\n",
            "  Batch 8100/8989 - Loss: 0.1946\n",
            "  Batch 8200/8989 - Loss: 0.1947\n",
            "  Batch 8300/8989 - Loss: 0.1947\n",
            "  Batch 8400/8989 - Loss: 0.1948\n",
            "  Batch 8500/8989 - Loss: 0.1948\n",
            "  Batch 8600/8989 - Loss: 0.1948\n",
            "  Batch 8700/8989 - Loss: 0.1949\n",
            "  Batch 8800/8989 - Loss: 0.1950\n",
            "  Batch 8900/8989 - Loss: 0.1950\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:45\n",
            "  Loss: 0.1952\n",
            "  HR@10: 0.7377\n",
            "  NDCG@10: 0.4554\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 17/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1816\n",
            "  Batch 200/8989 - Loss: 0.1785\n",
            "  Batch 300/8989 - Loss: 0.1792\n",
            "  Batch 400/8989 - Loss: 0.1791\n",
            "  Batch 500/8989 - Loss: 0.1801\n",
            "  Batch 600/8989 - Loss: 0.1810\n",
            "  Batch 700/8989 - Loss: 0.1816\n",
            "  Batch 800/8989 - Loss: 0.1817\n",
            "  Batch 900/8989 - Loss: 0.1821\n",
            "  Batch 1000/8989 - Loss: 0.1823\n",
            "  Batch 1100/8989 - Loss: 0.1826\n",
            "  Batch 1200/8989 - Loss: 0.1831\n",
            "  Batch 1300/8989 - Loss: 0.1834\n",
            "  Batch 1400/8989 - Loss: 0.1838\n",
            "  Batch 1500/8989 - Loss: 0.1842\n",
            "  Batch 1600/8989 - Loss: 0.1846\n",
            "  Batch 1700/8989 - Loss: 0.1847\n",
            "  Batch 1800/8989 - Loss: 0.1849\n",
            "  Batch 1900/8989 - Loss: 0.1848\n",
            "  Batch 2000/8989 - Loss: 0.1852\n",
            "  Batch 2100/8989 - Loss: 0.1853\n",
            "  Batch 2200/8989 - Loss: 0.1856\n",
            "  Batch 2300/8989 - Loss: 0.1858\n",
            "  Batch 2400/8989 - Loss: 0.1862\n",
            "  Batch 2500/8989 - Loss: 0.1864\n",
            "  Batch 2600/8989 - Loss: 0.1867\n",
            "  Batch 2700/8989 - Loss: 0.1869\n",
            "  Batch 2800/8989 - Loss: 0.1870\n",
            "  Batch 2900/8989 - Loss: 0.1871\n",
            "  Batch 3000/8989 - Loss: 0.1872\n",
            "  Batch 3100/8989 - Loss: 0.1875\n",
            "  Batch 3200/8989 - Loss: 0.1876\n",
            "  Batch 3300/8989 - Loss: 0.1877\n",
            "  Batch 3400/8989 - Loss: 0.1877\n",
            "  Batch 3500/8989 - Loss: 0.1878\n",
            "  Batch 3600/8989 - Loss: 0.1880\n",
            "  Batch 3700/8989 - Loss: 0.1880\n",
            "  Batch 3800/8989 - Loss: 0.1880\n",
            "  Batch 3900/8989 - Loss: 0.1881\n",
            "  Batch 4000/8989 - Loss: 0.1882\n",
            "  Batch 4100/8989 - Loss: 0.1884\n",
            "  Batch 4200/8989 - Loss: 0.1885\n",
            "  Batch 4300/8989 - Loss: 0.1887\n",
            "  Batch 4400/8989 - Loss: 0.1888\n",
            "  Batch 4500/8989 - Loss: 0.1890\n",
            "  Batch 4600/8989 - Loss: 0.1891\n",
            "  Batch 4700/8989 - Loss: 0.1893\n",
            "  Batch 4800/8989 - Loss: 0.1895\n",
            "  Batch 4900/8989 - Loss: 0.1896\n",
            "  Batch 5000/8989 - Loss: 0.1897\n",
            "  Batch 5100/8989 - Loss: 0.1898\n",
            "  Batch 5200/8989 - Loss: 0.1899\n",
            "  Batch 5300/8989 - Loss: 0.1900\n",
            "  Batch 5400/8989 - Loss: 0.1900\n",
            "  Batch 5500/8989 - Loss: 0.1902\n",
            "  Batch 5600/8989 - Loss: 0.1903\n",
            "  Batch 5700/8989 - Loss: 0.1904\n",
            "  Batch 5800/8989 - Loss: 0.1905\n",
            "  Batch 5900/8989 - Loss: 0.1906\n",
            "  Batch 6000/8989 - Loss: 0.1907\n",
            "  Batch 6100/8989 - Loss: 0.1908\n",
            "  Batch 6200/8989 - Loss: 0.1909\n",
            "  Batch 6300/8989 - Loss: 0.1909\n",
            "  Batch 6400/8989 - Loss: 0.1910\n",
            "  Batch 6500/8989 - Loss: 0.1910\n",
            "  Batch 6600/8989 - Loss: 0.1912\n",
            "  Batch 6700/8989 - Loss: 0.1913\n",
            "  Batch 6800/8989 - Loss: 0.1913\n",
            "  Batch 6900/8989 - Loss: 0.1914\n",
            "  Batch 7000/8989 - Loss: 0.1915\n",
            "  Batch 7100/8989 - Loss: 0.1917\n",
            "  Batch 7200/8989 - Loss: 0.1917\n",
            "  Batch 7300/8989 - Loss: 0.1918\n",
            "  Batch 7400/8989 - Loss: 0.1918\n",
            "  Batch 7500/8989 - Loss: 0.1919\n",
            "  Batch 7600/8989 - Loss: 0.1920\n",
            "  Batch 7700/8989 - Loss: 0.1922\n",
            "  Batch 7800/8989 - Loss: 0.1923\n",
            "  Batch 7900/8989 - Loss: 0.1924\n",
            "  Batch 8000/8989 - Loss: 0.1925\n",
            "  Batch 8100/8989 - Loss: 0.1926\n",
            "  Batch 8200/8989 - Loss: 0.1927\n",
            "  Batch 8300/8989 - Loss: 0.1929\n",
            "  Batch 8400/8989 - Loss: 0.1929\n",
            "  Batch 8500/8989 - Loss: 0.1930\n",
            "  Batch 8600/8989 - Loss: 0.1931\n",
            "  Batch 8700/8989 - Loss: 0.1933\n",
            "  Batch 8800/8989 - Loss: 0.1933\n",
            "  Batch 8900/8989 - Loss: 0.1934\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:43\n",
            "  Loss: 0.1934\n",
            "  HR@10: 0.7353\n",
            "  NDCG@10: 0.4511\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 18/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1806\n",
            "  Batch 200/8989 - Loss: 0.1801\n",
            "  Batch 300/8989 - Loss: 0.1813\n",
            "  Batch 400/8989 - Loss: 0.1807\n",
            "  Batch 500/8989 - Loss: 0.1808\n",
            "  Batch 600/8989 - Loss: 0.1797\n",
            "  Batch 700/8989 - Loss: 0.1808\n",
            "  Batch 800/8989 - Loss: 0.1810\n",
            "  Batch 900/8989 - Loss: 0.1812\n",
            "  Batch 1000/8989 - Loss: 0.1819\n",
            "  Batch 1100/8989 - Loss: 0.1818\n",
            "  Batch 1200/8989 - Loss: 0.1823\n",
            "  Batch 1300/8989 - Loss: 0.1822\n",
            "  Batch 1400/8989 - Loss: 0.1823\n",
            "  Batch 1500/8989 - Loss: 0.1828\n",
            "  Batch 1600/8989 - Loss: 0.1830\n",
            "  Batch 1700/8989 - Loss: 0.1829\n",
            "  Batch 1800/8989 - Loss: 0.1830\n",
            "  Batch 1900/8989 - Loss: 0.1831\n",
            "  Batch 2000/8989 - Loss: 0.1833\n",
            "  Batch 2100/8989 - Loss: 0.1835\n",
            "  Batch 2200/8989 - Loss: 0.1836\n",
            "  Batch 2300/8989 - Loss: 0.1838\n",
            "  Batch 2400/8989 - Loss: 0.1840\n",
            "  Batch 2500/8989 - Loss: 0.1842\n",
            "  Batch 2600/8989 - Loss: 0.1840\n",
            "  Batch 2700/8989 - Loss: 0.1842\n",
            "  Batch 2800/8989 - Loss: 0.1843\n",
            "  Batch 2900/8989 - Loss: 0.1846\n",
            "  Batch 3000/8989 - Loss: 0.1847\n",
            "  Batch 3100/8989 - Loss: 0.1848\n",
            "  Batch 3200/8989 - Loss: 0.1851\n",
            "  Batch 3300/8989 - Loss: 0.1852\n",
            "  Batch 3400/8989 - Loss: 0.1853\n",
            "  Batch 3500/8989 - Loss: 0.1854\n",
            "  Batch 3600/8989 - Loss: 0.1858\n",
            "  Batch 3700/8989 - Loss: 0.1860\n",
            "  Batch 3800/8989 - Loss: 0.1861\n",
            "  Batch 3900/8989 - Loss: 0.1863\n",
            "  Batch 4000/8989 - Loss: 0.1864\n",
            "  Batch 4100/8989 - Loss: 0.1866\n",
            "  Batch 4200/8989 - Loss: 0.1868\n",
            "  Batch 4300/8989 - Loss: 0.1869\n",
            "  Batch 4400/8989 - Loss: 0.1871\n",
            "  Batch 4500/8989 - Loss: 0.1873\n",
            "  Batch 4600/8989 - Loss: 0.1874\n",
            "  Batch 4700/8989 - Loss: 0.1874\n",
            "  Batch 4800/8989 - Loss: 0.1876\n",
            "  Batch 4900/8989 - Loss: 0.1877\n",
            "  Batch 5000/8989 - Loss: 0.1878\n",
            "  Batch 5100/8989 - Loss: 0.1878\n",
            "  Batch 5200/8989 - Loss: 0.1880\n",
            "  Batch 5300/8989 - Loss: 0.1881\n",
            "  Batch 5400/8989 - Loss: 0.1882\n",
            "  Batch 5500/8989 - Loss: 0.1884\n",
            "  Batch 5600/8989 - Loss: 0.1886\n",
            "  Batch 5700/8989 - Loss: 0.1888\n",
            "  Batch 5800/8989 - Loss: 0.1890\n",
            "  Batch 5900/8989 - Loss: 0.1891\n",
            "  Batch 6000/8989 - Loss: 0.1892\n",
            "  Batch 6100/8989 - Loss: 0.1893\n",
            "  Batch 6200/8989 - Loss: 0.1894\n",
            "  Batch 6300/8989 - Loss: 0.1894\n",
            "  Batch 6400/8989 - Loss: 0.1896\n",
            "  Batch 6500/8989 - Loss: 0.1897\n",
            "  Batch 6600/8989 - Loss: 0.1898\n",
            "  Batch 6700/8989 - Loss: 0.1900\n",
            "  Batch 6800/8989 - Loss: 0.1901\n",
            "  Batch 6900/8989 - Loss: 0.1902\n",
            "  Batch 7000/8989 - Loss: 0.1903\n",
            "  Batch 7100/8989 - Loss: 0.1904\n",
            "  Batch 7200/8989 - Loss: 0.1905\n",
            "  Batch 7300/8989 - Loss: 0.1906\n",
            "  Batch 7400/8989 - Loss: 0.1907\n",
            "  Batch 7500/8989 - Loss: 0.1907\n",
            "  Batch 7600/8989 - Loss: 0.1908\n",
            "  Batch 7700/8989 - Loss: 0.1908\n",
            "  Batch 7800/8989 - Loss: 0.1909\n",
            "  Batch 7900/8989 - Loss: 0.1910\n",
            "  Batch 8000/8989 - Loss: 0.1911\n",
            "  Batch 8100/8989 - Loss: 0.1912\n",
            "  Batch 8200/8989 - Loss: 0.1912\n",
            "  Batch 8300/8989 - Loss: 0.1914\n",
            "  Batch 8400/8989 - Loss: 0.1914\n",
            "  Batch 8500/8989 - Loss: 0.1915\n",
            "  Batch 8600/8989 - Loss: 0.1915\n",
            "  Batch 8700/8989 - Loss: 0.1917\n",
            "  Batch 8800/8989 - Loss: 0.1918\n",
            "  Batch 8900/8989 - Loss: 0.1918\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:43\n",
            "  Loss: 0.1919\n",
            "  HR@10: 0.7360\n",
            "  NDCG@10: 0.4528\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 19/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1798\n",
            "  Batch 200/8989 - Loss: 0.1809\n",
            "  Batch 300/8989 - Loss: 0.1785\n",
            "  Batch 400/8989 - Loss: 0.1796\n",
            "  Batch 500/8989 - Loss: 0.1803\n",
            "  Batch 600/8989 - Loss: 0.1811\n",
            "  Batch 700/8989 - Loss: 0.1812\n",
            "  Batch 800/8989 - Loss: 0.1813\n",
            "  Batch 900/8989 - Loss: 0.1815\n",
            "  Batch 1000/8989 - Loss: 0.1819\n",
            "  Batch 1100/8989 - Loss: 0.1817\n",
            "  Batch 1200/8989 - Loss: 0.1818\n",
            "  Batch 1300/8989 - Loss: 0.1817\n",
            "  Batch 1400/8989 - Loss: 0.1819\n",
            "  Batch 1500/8989 - Loss: 0.1819\n",
            "  Batch 1600/8989 - Loss: 0.1820\n",
            "  Batch 1700/8989 - Loss: 0.1821\n",
            "  Batch 1800/8989 - Loss: 0.1823\n",
            "  Batch 1900/8989 - Loss: 0.1824\n",
            "  Batch 2000/8989 - Loss: 0.1825\n",
            "  Batch 2100/8989 - Loss: 0.1825\n",
            "  Batch 2200/8989 - Loss: 0.1826\n",
            "  Batch 2300/8989 - Loss: 0.1828\n",
            "  Batch 2400/8989 - Loss: 0.1830\n",
            "  Batch 2500/8989 - Loss: 0.1831\n",
            "  Batch 2600/8989 - Loss: 0.1832\n",
            "  Batch 2700/8989 - Loss: 0.1835\n",
            "  Batch 2800/8989 - Loss: 0.1837\n",
            "  Batch 2900/8989 - Loss: 0.1838\n",
            "  Batch 3000/8989 - Loss: 0.1838\n",
            "  Batch 3100/8989 - Loss: 0.1839\n",
            "  Batch 3200/8989 - Loss: 0.1841\n",
            "  Batch 3300/8989 - Loss: 0.1841\n",
            "  Batch 3400/8989 - Loss: 0.1843\n",
            "  Batch 3500/8989 - Loss: 0.1844\n",
            "  Batch 3600/8989 - Loss: 0.1845\n",
            "  Batch 3700/8989 - Loss: 0.1845\n",
            "  Batch 3800/8989 - Loss: 0.1846\n",
            "  Batch 3900/8989 - Loss: 0.1847\n",
            "  Batch 4000/8989 - Loss: 0.1849\n",
            "  Batch 4100/8989 - Loss: 0.1850\n",
            "  Batch 4200/8989 - Loss: 0.1851\n",
            "  Batch 4300/8989 - Loss: 0.1853\n",
            "  Batch 4400/8989 - Loss: 0.1855\n",
            "  Batch 4500/8989 - Loss: 0.1856\n",
            "  Batch 4600/8989 - Loss: 0.1858\n",
            "  Batch 4700/8989 - Loss: 0.1858\n",
            "  Batch 4800/8989 - Loss: 0.1860\n",
            "  Batch 4900/8989 - Loss: 0.1861\n",
            "  Batch 5000/8989 - Loss: 0.1862\n",
            "  Batch 5100/8989 - Loss: 0.1864\n",
            "  Batch 5200/8989 - Loss: 0.1865\n",
            "  Batch 5300/8989 - Loss: 0.1866\n",
            "  Batch 5400/8989 - Loss: 0.1867\n",
            "  Batch 5500/8989 - Loss: 0.1869\n",
            "  Batch 5600/8989 - Loss: 0.1869\n",
            "  Batch 5700/8989 - Loss: 0.1870\n",
            "  Batch 5800/8989 - Loss: 0.1871\n",
            "  Batch 5900/8989 - Loss: 0.1872\n",
            "  Batch 6000/8989 - Loss: 0.1873\n",
            "  Batch 6100/8989 - Loss: 0.1874\n",
            "  Batch 6200/8989 - Loss: 0.1875\n",
            "  Batch 6300/8989 - Loss: 0.1876\n",
            "  Batch 6400/8989 - Loss: 0.1877\n",
            "  Batch 6500/8989 - Loss: 0.1878\n",
            "  Batch 6600/8989 - Loss: 0.1880\n",
            "  Batch 6700/8989 - Loss: 0.1881\n",
            "  Batch 6800/8989 - Loss: 0.1881\n",
            "  Batch 6900/8989 - Loss: 0.1881\n",
            "  Batch 7000/8989 - Loss: 0.1883\n",
            "  Batch 7100/8989 - Loss: 0.1884\n",
            "  Batch 7200/8989 - Loss: 0.1885\n",
            "  Batch 7300/8989 - Loss: 0.1886\n",
            "  Batch 7400/8989 - Loss: 0.1887\n",
            "  Batch 7500/8989 - Loss: 0.1888\n",
            "  Batch 7600/8989 - Loss: 0.1889\n",
            "  Batch 7700/8989 - Loss: 0.1890\n",
            "  Batch 7800/8989 - Loss: 0.1891\n",
            "  Batch 7900/8989 - Loss: 0.1893\n",
            "  Batch 8000/8989 - Loss: 0.1894\n",
            "  Batch 8100/8989 - Loss: 0.1895\n",
            "  Batch 8200/8989 - Loss: 0.1896\n",
            "  Batch 8300/8989 - Loss: 0.1896\n",
            "  Batch 8400/8989 - Loss: 0.1897\n",
            "  Batch 8500/8989 - Loss: 0.1897\n",
            "  Batch 8600/8989 - Loss: 0.1898\n",
            "  Batch 8700/8989 - Loss: 0.1898\n",
            "  Batch 8800/8989 - Loss: 0.1899\n",
            "  Batch 8900/8989 - Loss: 0.1900\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:46\n",
            "  Loss: 0.1901\n",
            "  HR@10: 0.7341\n",
            "  NDCG@10: 0.4507\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 20/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1765\n",
            "  Batch 200/8989 - Loss: 0.1761\n",
            "  Batch 300/8989 - Loss: 0.1770\n",
            "  Batch 400/8989 - Loss: 0.1783\n",
            "  Batch 500/8989 - Loss: 0.1787\n",
            "  Batch 600/8989 - Loss: 0.1791\n",
            "  Batch 700/8989 - Loss: 0.1796\n",
            "  Batch 800/8989 - Loss: 0.1794\n",
            "  Batch 900/8989 - Loss: 0.1797\n",
            "  Batch 1000/8989 - Loss: 0.1800\n",
            "  Batch 1100/8989 - Loss: 0.1804\n",
            "  Batch 1200/8989 - Loss: 0.1807\n",
            "  Batch 1300/8989 - Loss: 0.1810\n",
            "  Batch 1400/8989 - Loss: 0.1809\n",
            "  Batch 1500/8989 - Loss: 0.1810\n",
            "  Batch 1600/8989 - Loss: 0.1809\n",
            "  Batch 1700/8989 - Loss: 0.1813\n",
            "  Batch 1800/8989 - Loss: 0.1813\n",
            "  Batch 1900/8989 - Loss: 0.1815\n",
            "  Batch 2000/8989 - Loss: 0.1817\n",
            "  Batch 2100/8989 - Loss: 0.1816\n",
            "  Batch 2200/8989 - Loss: 0.1817\n",
            "  Batch 2300/8989 - Loss: 0.1818\n",
            "  Batch 2400/8989 - Loss: 0.1818\n",
            "  Batch 2500/8989 - Loss: 0.1819\n",
            "  Batch 2600/8989 - Loss: 0.1819\n",
            "  Batch 2700/8989 - Loss: 0.1819\n",
            "  Batch 2800/8989 - Loss: 0.1821\n",
            "  Batch 2900/8989 - Loss: 0.1823\n",
            "  Batch 3000/8989 - Loss: 0.1824\n",
            "  Batch 3100/8989 - Loss: 0.1826\n",
            "  Batch 3200/8989 - Loss: 0.1827\n",
            "  Batch 3300/8989 - Loss: 0.1828\n",
            "  Batch 3400/8989 - Loss: 0.1829\n",
            "  Batch 3500/8989 - Loss: 0.1831\n",
            "  Batch 3600/8989 - Loss: 0.1833\n",
            "  Batch 3700/8989 - Loss: 0.1834\n",
            "  Batch 3800/8989 - Loss: 0.1836\n",
            "  Batch 3900/8989 - Loss: 0.1837\n",
            "  Batch 4000/8989 - Loss: 0.1838\n",
            "  Batch 4100/8989 - Loss: 0.1841\n",
            "  Batch 4200/8989 - Loss: 0.1842\n",
            "  Batch 4300/8989 - Loss: 0.1842\n",
            "  Batch 4400/8989 - Loss: 0.1843\n",
            "  Batch 4500/8989 - Loss: 0.1843\n",
            "  Batch 4600/8989 - Loss: 0.1844\n",
            "  Batch 4700/8989 - Loss: 0.1846\n",
            "  Batch 4800/8989 - Loss: 0.1848\n",
            "  Batch 4900/8989 - Loss: 0.1848\n",
            "  Batch 5000/8989 - Loss: 0.1850\n",
            "  Batch 5100/8989 - Loss: 0.1850\n",
            "  Batch 5200/8989 - Loss: 0.1851\n",
            "  Batch 5300/8989 - Loss: 0.1852\n",
            "  Batch 5400/8989 - Loss: 0.1853\n",
            "  Batch 5500/8989 - Loss: 0.1854\n",
            "  Batch 5600/8989 - Loss: 0.1854\n",
            "  Batch 5700/8989 - Loss: 0.1855\n",
            "  Batch 5800/8989 - Loss: 0.1856\n",
            "  Batch 5900/8989 - Loss: 0.1857\n",
            "  Batch 6000/8989 - Loss: 0.1858\n",
            "  Batch 6100/8989 - Loss: 0.1860\n",
            "  Batch 6200/8989 - Loss: 0.1861\n",
            "  Batch 6300/8989 - Loss: 0.1863\n",
            "  Batch 6400/8989 - Loss: 0.1864\n",
            "  Batch 6500/8989 - Loss: 0.1864\n",
            "  Batch 6600/8989 - Loss: 0.1865\n",
            "  Batch 6700/8989 - Loss: 0.1867\n",
            "  Batch 6800/8989 - Loss: 0.1868\n",
            "  Batch 6900/8989 - Loss: 0.1870\n",
            "  Batch 7000/8989 - Loss: 0.1870\n",
            "  Batch 7100/8989 - Loss: 0.1871\n",
            "  Batch 7200/8989 - Loss: 0.1872\n",
            "  Batch 7300/8989 - Loss: 0.1873\n",
            "  Batch 7400/8989 - Loss: 0.1874\n",
            "  Batch 7500/8989 - Loss: 0.1875\n",
            "  Batch 7600/8989 - Loss: 0.1876\n",
            "  Batch 7700/8989 - Loss: 0.1877\n",
            "  Batch 7800/8989 - Loss: 0.1878\n",
            "  Batch 7900/8989 - Loss: 0.1879\n",
            "  Batch 8000/8989 - Loss: 0.1879\n",
            "  Batch 8100/8989 - Loss: 0.1880\n",
            "  Batch 8200/8989 - Loss: 0.1880\n",
            "  Batch 8300/8989 - Loss: 0.1881\n",
            "  Batch 8400/8989 - Loss: 0.1883\n",
            "  Batch 8500/8989 - Loss: 0.1883\n",
            "  Batch 8600/8989 - Loss: 0.1885\n",
            "  Batch 8700/8989 - Loss: 0.1886\n",
            "  Batch 8800/8989 - Loss: 0.1887\n",
            "  Batch 8900/8989 - Loss: 0.1889\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:43\n",
            "  Loss: 0.1890\n",
            "  HR@10: 0.7334\n",
            "  NDCG@10: 0.4512\n",
            "  (Best: HR@10: 0.7405 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETE!\n",
            "======================================================================\n",
            "Best model at epoch 4:\n",
            "  HR@10: 0.7405\n",
            "  NDCG@10: 0.4564\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 7.2 TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.2: Starting Training\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Training for {epochs} epochs...\")\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Track best performance\n",
        "best_hr = 0.0\n",
        "best_ndcg = 0.0\n",
        "best_epoch = 0\n",
        "training_history = {\n",
        "    'epoch': [],\n",
        "    'loss': [],\n",
        "    'hr': [],\n",
        "    'ndcg': [],\n",
        "    'time': []\n",
        "}\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # ========================================================================\n",
        "    # TRAINING PHASE\n",
        "    # ========================================================================\n",
        "    \n",
        "    # Set model to training mode\n",
        "    # This enables dropout and other training-specific behaviors\n",
        "    ncf_model.train()\n",
        "    \n",
        "    # Start timer for this epoch\n",
        "    epoch_start_time = time.time()\n",
        "    \n",
        "    # Generate negative samples for this epoch\n",
        "    # Important: We generate fresh negatives each epoch for better learning\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "    print(\"-\" * 70)\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    # Track loss for this epoch\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    # Iterate through training batches\n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        # Move data to device (GPU or CPU)\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        # ================================================================\n",
        "        # FORWARD PASS\n",
        "        # ================================================================\n",
        "        # Clear gradients from previous iteration\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Get model predictions (raw scores/logits)\n",
        "        prediction = ncf_model(user, item)  # [batch_size]\n",
        "        \n",
        "        # ================================================================\n",
        "        # COMPUTE LOSS\n",
        "        # ================================================================\n",
        "        # Compare predictions with true labels (1 for positive, 0 for negative)\n",
        "        loss = loss_function(prediction, label)\n",
        "        \n",
        "        # ================================================================\n",
        "        # BACKWARD PASS\n",
        "        # ================================================================\n",
        "        # Compute gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update model weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Track loss\n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        # Print progress every 100 batches\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            print(f\"  Batch {batch_idx+1}/{len(train_loader)} - Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    # Calculate average loss for this epoch\n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # ========================================================================\n",
        "    # EVALUATION PHASE\n",
        "    # ========================================================================\n",
        "    \n",
        "    # Set model to evaluation mode\n",
        "    # This disables dropout and uses deterministic behavior\n",
        "    ncf_model.eval()\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    print(\"  Evaluating on test set...\")\n",
        "    HR, NDCG = evaluate_metrics(ncf_model, test_loader, top_k, device)\n",
        "    \n",
        "    # Calculate elapsed time\n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    # Store history\n",
        "    training_history['epoch'].append(epoch + 1)\n",
        "    training_history['loss'].append(avg_loss)\n",
        "    training_history['hr'].append(HR)\n",
        "    training_history['ndcg'].append(NDCG)\n",
        "    training_history['time'].append(elapsed_time)\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"  Time: {time_str}\")\n",
        "    print(f\"  Loss: {avg_loss:.4f}\")\n",
        "    print(f\"  HR@{top_k}: {HR:.4f}\")\n",
        "    print(f\"  NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    # ========================================================================\n",
        "    # SAVE BEST MODEL\n",
        "    # ========================================================================\n",
        "    \n",
        "    # Check if this is the best model so far\n",
        "    if HR > best_hr:\n",
        "        best_hr = HR\n",
        "        best_ndcg = NDCG\n",
        "        best_epoch = epoch + 1\n",
        "        \n",
        "        print(f\"  ✓ New best model! (HR@{top_k}: {HR:.4f})\")\n",
        "        \n",
        "        # Save model if enabled\n",
        "        if save_model:\n",
        "            if not os.path.exists(model_path):\n",
        "                os.makedirs(model_path)\n",
        "            \n",
        "            model_filename = os.path.join(model_path, f'{model_name}.pth')\n",
        "            torch.save(ncf_model, model_filename)\n",
        "            print(f\"  ✓ Model saved to {model_filename}\")\n",
        "    else:\n",
        "        print(f\"  (Best: HR@{top_k}: {best_hr:.4f} at epoch {best_epoch})\")\n",
        "    \n",
        "    print(\"-\" * 70)\n",
        "\n",
        "# ========================================================================\n",
        "# TRAINING COMPLETE\n",
        "# ========================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Best model at epoch {best_epoch}:\")\n",
        "print(f\"  HR@{top_k}: {best_hr:.4f}\")\n",
        "print(f\"  NDCG@{top_k}: {best_ndcg:.4f}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Store the NeuMF-end model for later comparison\n",
        "ncf_model_neumf_end = ncf_model\n",
        "best_hr_neumf_end = best_hr\n",
        "best_ndcg_neumf_end = best_ndcg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 7.5: Train Models Separately (GMF, MLP, NeuMF-pre)\n",
        "\n",
        "In this step, we'll train each model architecture separately:\n",
        "1. **Train GMF model** - Generalized Matrix Factorization only\n",
        "2. **Train MLP model** - Multi-Layer Perceptron only  \n",
        "3. **Train NeuMF-pre** - Neural Matrix Factorization using pre-trained GMF and MLP weights\n",
        "\n",
        "This approach (NeuMF-pre) typically gives the best performance as it leverages pre-trained components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 7.5: Training Models Separately\n",
            "======================================================================\n",
            "This will train:\n",
            "  1. GMF model (Generalized Matrix Factorization)\n",
            "  2. MLP model (Multi-Layer Perceptron)\n",
            "  3. NeuMF-pre model (using pre-trained GMF and MLP weights)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "STEP 7.5.1: Training GMF Model\n",
            "======================================================================\n",
            "Training GMF for 20 epochs...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 1/20 - Time: 00:00:22 - Loss: 0.3595 - HR@10: 0.6414 - NDCG@10: 0.3759\n",
            "  ✓ Saved best GMF model (HR@10: 0.6414)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 2/20 - Time: 00:00:20 - Loss: 0.3002 - HR@10: 0.7026 - NDCG@10: 0.4228\n",
            "  ✓ Saved best GMF model (HR@10: 0.7026)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 3/20 - Time: 00:00:20 - Loss: 0.2785 - HR@10: 0.7252 - NDCG@10: 0.4437\n",
            "  ✓ Saved best GMF model (HR@10: 0.7252)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 4/20 - Time: 00:00:19 - Loss: 0.2660 - HR@10: 0.7369 - NDCG@10: 0.4543\n",
            "  ✓ Saved best GMF model (HR@10: 0.7369)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 5/20 - Time: 00:00:20 - Loss: 0.2570 - HR@10: 0.7441 - NDCG@10: 0.4602\n",
            "  ✓ Saved best GMF model (HR@10: 0.7441)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 6/20 - Time: 00:00:20 - Loss: 0.2494 - HR@10: 0.7487 - NDCG@10: 0.4643\n",
            "  ✓ Saved best GMF model (HR@10: 0.7487)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 7/20 - Time: 00:00:19 - Loss: 0.2416 - HR@10: 0.7494 - NDCG@10: 0.4659\n",
            "  ✓ Saved best GMF model (HR@10: 0.7494)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 8/20 - Time: 00:00:21 - Loss: 0.2347 - HR@10: 0.7495 - NDCG@10: 0.4656\n",
            "  ✓ Saved best GMF model (HR@10: 0.7495)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 9/20 - Time: 00:00:20 - Loss: 0.2294 - HR@10: 0.7479 - NDCG@10: 0.4648\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 10/20 - Time: 00:00:19 - Loss: 0.2253 - HR@10: 0.7490 - NDCG@10: 0.4644\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 11/20 - Time: 00:00:19 - Loss: 0.2218 - HR@10: 0.7480 - NDCG@10: 0.4640\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 12/20 - Time: 00:00:21 - Loss: 0.2188 - HR@10: 0.7466 - NDCG@10: 0.4628\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 13/20 - Time: 00:00:20 - Loss: 0.2165 - HR@10: 0.7462 - NDCG@10: 0.4631\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 14/20 - Time: 00:00:20 - Loss: 0.2147 - HR@10: 0.7477 - NDCG@10: 0.4635\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 15/20 - Time: 00:00:20 - Loss: 0.2129 - HR@10: 0.7470 - NDCG@10: 0.4627\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 16/20 - Time: 00:00:20 - Loss: 0.2112 - HR@10: 0.7471 - NDCG@10: 0.4621\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 17/20 - Time: 00:00:20 - Loss: 0.2094 - HR@10: 0.7464 - NDCG@10: 0.4622\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 18/20 - Time: 00:00:21 - Loss: 0.2085 - HR@10: 0.7476 - NDCG@10: 0.4630\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 19/20 - Time: 00:00:20 - Loss: 0.2074 - HR@10: 0.7477 - NDCG@10: 0.4632\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 20/20 - Time: 00:00:21 - Loss: 0.2063 - HR@10: 0.7476 - NDCG@10: 0.4637\n",
            "\n",
            "✓ GMF Training Complete!\n",
            "  Best epoch: 8\n",
            "  Best HR@10: 0.7495\n",
            "  Best NDCG@10: 0.4656\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 7.5: TRAIN MODELS SEPARATELY (GMF, MLP, NeuMF-pre)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 7.5: Training Models Separately\")\n",
        "print(\"=\" * 70)\n",
        "print(\"This will train:\")\n",
        "print(\"  1. GMF model (Generalized Matrix Factorization)\")\n",
        "print(\"  2. MLP model (Multi-Layer Perceptron)\")\n",
        "print(\"  3. NeuMF-pre model (using pre-trained GMF and MLP weights)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Store trained models\n",
        "trained_models = {}\n",
        "\n",
        "# ============================================================================\n",
        "# 7.5.1 TRAIN GMF MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.5.1: Training GMF Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create GMF model\n",
        "gmf_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name='GMF',\n",
        "    GMF_model=None,\n",
        "    MLP_model=None\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gmf_model = gmf_model.cuda()\n",
        "\n",
        "# Setup optimizer and loss\n",
        "gmf_optimizer = optim.Adam(gmf_model.parameters(), lr=learning_rate)\n",
        "gmf_loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop for GMF\n",
        "print(f\"Training GMF for {epochs} epochs...\")\n",
        "best_hr_gmf = 0.0\n",
        "best_ndcg_gmf = 0.0\n",
        "best_epoch_gmf = 0\n",
        "gmf_training_history = {\n",
        "    'epoch': [],\n",
        "    'loss': [],\n",
        "    'hr': [],\n",
        "    'ndcg': []\n",
        "}\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    gmf_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        gmf_optimizer.zero_grad()\n",
        "        prediction = gmf_model(user, item)\n",
        "        loss = gmf_loss_function(prediction, label)\n",
        "        loss.backward()\n",
        "        gmf_optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # Evaluate\n",
        "    gmf_model.eval()\n",
        "    HR, NDCG = evaluate_metrics(gmf_model, test_loader, top_k, device)\n",
        "    \n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    # Store history\n",
        "    gmf_training_history['epoch'].append(epoch + 1)\n",
        "    gmf_training_history['loss'].append(avg_loss)\n",
        "    gmf_training_history['hr'].append(HR)\n",
        "    gmf_training_history['ndcg'].append(NDCG)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Time: {time_str} - Loss: {avg_loss:.4f} - HR@{top_k}: {HR:.4f} - NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    if HR > best_hr_gmf:\n",
        "        best_hr_gmf = HR\n",
        "        best_ndcg_gmf = NDCG\n",
        "        best_epoch_gmf = epoch + 1\n",
        "        if save_model:\n",
        "            torch.save(gmf_model, GMF_model_path)\n",
        "            print(f\"  ✓ Saved best GMF model (HR@{top_k}: {HR:.4f})\")\n",
        "\n",
        "print(f\"\\n✓ GMF Training Complete!\")\n",
        "print(f\"  Best epoch: {best_epoch_gmf}\")\n",
        "print(f\"  Best HR@{top_k}: {best_hr_gmf:.4f}\")\n",
        "print(f\"  Best NDCG@{top_k}: {best_ndcg_gmf:.4f}\")\n",
        "\n",
        "trained_models['GMF'] = {\n",
        "    'model': gmf_model,\n",
        "    'hr': best_hr_gmf,\n",
        "    'ndcg': best_ndcg_gmf,\n",
        "    'epoch': best_epoch_gmf\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 7.5.2: Training MLP Model\n",
            "======================================================================\n",
            "Training MLP for 20 epochs...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 1/20 - Time: 00:00:39 - Loss: 0.3506 - HR@10: 0.5687 - NDCG@10: 0.3259\n",
            "  ✓ Saved best MLP model (HR@10: 0.5687)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 2/20 - Time: 00:00:40 - Loss: 0.3280 - HR@10: 0.6370 - NDCG@10: 0.3727\n",
            "  ✓ Saved best MLP model (HR@10: 0.6370)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 3/20 - Time: 00:00:39 - Loss: 0.3074 - HR@10: 0.6706 - NDCG@10: 0.3976\n",
            "  ✓ Saved best MLP model (HR@10: 0.6706)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 4/20 - Time: 00:00:36 - Loss: 0.2939 - HR@10: 0.6924 - NDCG@10: 0.4146\n",
            "  ✓ Saved best MLP model (HR@10: 0.6924)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 5/20 - Time: 00:00:35 - Loss: 0.2853 - HR@10: 0.7063 - NDCG@10: 0.4253\n",
            "  ✓ Saved best MLP model (HR@10: 0.7063)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 6/20 - Time: 00:00:36 - Loss: 0.2786 - HR@10: 0.7148 - NDCG@10: 0.4340\n",
            "  ✓ Saved best MLP model (HR@10: 0.7148)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 7/20 - Time: 00:00:35 - Loss: 0.2734 - HR@10: 0.7220 - NDCG@10: 0.4410\n",
            "  ✓ Saved best MLP model (HR@10: 0.7220)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 8/20 - Time: 00:00:36 - Loss: 0.2685 - HR@10: 0.7303 - NDCG@10: 0.4469\n",
            "  ✓ Saved best MLP model (HR@10: 0.7303)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 9/20 - Time: 00:00:36 - Loss: 0.2642 - HR@10: 0.7351 - NDCG@10: 0.4520\n",
            "  ✓ Saved best MLP model (HR@10: 0.7351)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 10/20 - Time: 00:00:35 - Loss: 0.2603 - HR@10: 0.7399 - NDCG@10: 0.4564\n",
            "  ✓ Saved best MLP model (HR@10: 0.7399)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 11/20 - Time: 00:00:35 - Loss: 0.2577 - HR@10: 0.7430 - NDCG@10: 0.4591\n",
            "  ✓ Saved best MLP model (HR@10: 0.7430)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 12/20 - Time: 00:00:36 - Loss: 0.2552 - HR@10: 0.7456 - NDCG@10: 0.4608\n",
            "  ✓ Saved best MLP model (HR@10: 0.7456)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 13/20 - Time: 00:00:35 - Loss: 0.2527 - HR@10: 0.7470 - NDCG@10: 0.4626\n",
            "  ✓ Saved best MLP model (HR@10: 0.7470)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 14/20 - Time: 00:00:36 - Loss: 0.2508 - HR@10: 0.7493 - NDCG@10: 0.4654\n",
            "  ✓ Saved best MLP model (HR@10: 0.7493)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 15/20 - Time: 00:00:36 - Loss: 0.2484 - HR@10: 0.7508 - NDCG@10: 0.4671\n",
            "  ✓ Saved best MLP model (HR@10: 0.7508)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 16/20 - Time: 00:00:38 - Loss: 0.2473 - HR@10: 0.7511 - NDCG@10: 0.4669\n",
            "  ✓ Saved best MLP model (HR@10: 0.7511)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 17/20 - Time: 00:00:39 - Loss: 0.2449 - HR@10: 0.7524 - NDCG@10: 0.4681\n",
            "  ✓ Saved best MLP model (HR@10: 0.7524)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 18/20 - Time: 00:00:40 - Loss: 0.2435 - HR@10: 0.7535 - NDCG@10: 0.4698\n",
            "  ✓ Saved best MLP model (HR@10: 0.7535)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 19/20 - Time: 00:00:38 - Loss: 0.2419 - HR@10: 0.7541 - NDCG@10: 0.4698\n",
            "  ✓ Saved best MLP model (HR@10: 0.7541)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 20/20 - Time: 00:00:38 - Loss: 0.2406 - HR@10: 0.7544 - NDCG@10: 0.4712\n",
            "  ✓ Saved best MLP model (HR@10: 0.7544)\n",
            "\n",
            "✓ MLP Training Complete!\n",
            "  Best epoch: 20\n",
            "  Best HR@10: 0.7544\n",
            "  Best NDCG@10: 0.4712\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 7.5.2 TRAIN MLP MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.5.2: Training MLP Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create MLP model\n",
        "mlp_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name='MLP',\n",
        "    GMF_model=None,\n",
        "    MLP_model=None\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    mlp_model = mlp_model.cuda()\n",
        "\n",
        "# Setup optimizer and loss\n",
        "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=learning_rate)\n",
        "mlp_loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop for MLP\n",
        "print(f\"Training MLP for {epochs} epochs...\")\n",
        "best_hr_mlp = 0.0\n",
        "best_ndcg_mlp = 0.0\n",
        "best_epoch_mlp = 0\n",
        "mlp_training_history = {\n",
        "    'epoch': [],\n",
        "    'loss': [],\n",
        "    'hr': [],\n",
        "    'ndcg': []\n",
        "}\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    mlp_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        mlp_optimizer.zero_grad()\n",
        "        prediction = mlp_model(user, item)\n",
        "        loss = mlp_loss_function(prediction, label)\n",
        "        loss.backward()\n",
        "        mlp_optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # Evaluate\n",
        "    mlp_model.eval()\n",
        "    HR, NDCG = evaluate_metrics(mlp_model, test_loader, top_k, device)\n",
        "    \n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    # Store history\n",
        "    mlp_training_history['epoch'].append(epoch + 1)\n",
        "    mlp_training_history['loss'].append(avg_loss)\n",
        "    mlp_training_history['hr'].append(HR)\n",
        "    mlp_training_history['ndcg'].append(NDCG)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Time: {time_str} - Loss: {avg_loss:.4f} - HR@{top_k}: {HR:.4f} - NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    if HR > best_hr_mlp:\n",
        "        best_hr_mlp = HR\n",
        "        best_ndcg_mlp = NDCG\n",
        "        best_epoch_mlp = epoch + 1\n",
        "        if save_model:\n",
        "            torch.save(mlp_model, MLP_model_path)\n",
        "            print(f\"  ✓ Saved best MLP model (HR@{top_k}: {HR:.4f})\")\n",
        "\n",
        "print(f\"\\n✓ MLP Training Complete!\")\n",
        "print(f\"  Best epoch: {best_epoch_mlp}\")\n",
        "print(f\"  Best HR@{top_k}: {best_hr_mlp:.4f}\")\n",
        "print(f\"  Best NDCG@{top_k}: {best_ndcg_mlp:.4f}\")\n",
        "\n",
        "trained_models['MLP'] = {\n",
        "    'model': mlp_model,\n",
        "    'hr': best_hr_mlp,\n",
        "    'ndcg': best_ndcg_mlp,\n",
        "    'epoch': best_epoch_mlp\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 7.5.3: Training NeuMF-pre Model\n",
            "======================================================================\n",
            "Creating NeuMF model with pre-trained GMF and MLP weights...\n",
            "Training NeuMF-pre for 20 epochs...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 1/20 - Time: 00:00:33 - Loss: 0.2009 - HR@10: 0.7709 - NDCG@10: 0.4863\n",
            "  ✓ Saved best NeuMF-pre model (HR@10: 0.7709)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 2/20 - Time: 00:00:34 - Loss: 0.2006 - HR@10: 0.7705 - NDCG@10: 0.4859\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 3/20 - Time: 00:00:33 - Loss: 0.1997 - HR@10: 0.7703 - NDCG@10: 0.4853\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 4/20 - Time: 00:00:33 - Loss: 0.1993 - HR@10: 0.7703 - NDCG@10: 0.4853\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 5/20 - Time: 00:00:34 - Loss: 0.1989 - HR@10: 0.7700 - NDCG@10: 0.4853\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 6/20 - Time: 00:00:32 - Loss: 0.1985 - HR@10: 0.7697 - NDCG@10: 0.4847\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 7/20 - Time: 00:00:32 - Loss: 0.1981 - HR@10: 0.7690 - NDCG@10: 0.4844\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 8/20 - Time: 00:00:34 - Loss: 0.1977 - HR@10: 0.7688 - NDCG@10: 0.4842\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 9/20 - Time: 00:00:33 - Loss: 0.1978 - HR@10: 0.7684 - NDCG@10: 0.4837\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 10/20 - Time: 00:00:33 - Loss: 0.1975 - HR@10: 0.7679 - NDCG@10: 0.4836\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 11/20 - Time: 00:00:33 - Loss: 0.1973 - HR@10: 0.7675 - NDCG@10: 0.4830\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 12/20 - Time: 00:00:34 - Loss: 0.1973 - HR@10: 0.7674 - NDCG@10: 0.4830\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 13/20 - Time: 00:00:32 - Loss: 0.1972 - HR@10: 0.7675 - NDCG@10: 0.4833\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 14/20 - Time: 00:00:33 - Loss: 0.1966 - HR@10: 0.7671 - NDCG@10: 0.4824\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 15/20 - Time: 00:00:32 - Loss: 0.1968 - HR@10: 0.7671 - NDCG@10: 0.4822\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 16/20 - Time: 00:00:31 - Loss: 0.1964 - HR@10: 0.7668 - NDCG@10: 0.4821\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 17/20 - Time: 00:00:31 - Loss: 0.1964 - HR@10: 0.7667 - NDCG@10: 0.4816\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 18/20 - Time: 00:00:32 - Loss: 0.1965 - HR@10: 0.7666 - NDCG@10: 0.4818\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 19/20 - Time: 00:00:32 - Loss: 0.1965 - HR@10: 0.7665 - NDCG@10: 0.4815\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 20/20 - Time: 00:00:32 - Loss: 0.1964 - HR@10: 0.7665 - NDCG@10: 0.4813\n",
            "\n",
            "✓ NeuMF-pre Training Complete!\n",
            "  Best epoch: 1\n",
            "  Best HR@10: 0.7709\n",
            "  Best NDCG@10: 0.4863\n",
            "\n",
            "======================================================================\n",
            "MODEL COMPARISON SUMMARY\n",
            "======================================================================\n",
            "Model           HR@{top_k}   NDCG@{top_k} Best Epoch  \n",
            "----------------------------------------------------------------------\n",
            "GMF             0.7495       0.4656       8           \n",
            "MLP             0.7544       0.4712       20          \n",
            "NeuMF-end       0.7405       0.4564       4           \n",
            "NeuMF-pre       0.7709       0.4863       1           \n",
            "======================================================================\n",
            "\n",
            "🏆 Best Model: NeuMF-pre\n",
            "   HR@10: 0.7709\n",
            "   NDCG@10: 0.4863\n",
            "\n",
            "✓ Using NeuMF-pre model for recommendations\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 7.5.3 TRAIN NeuMF-pre MODEL (Using Pre-trained GMF and MLP)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.5.3: Training NeuMF-pre Model\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Creating NeuMF model with pre-trained GMF and MLP weights...\")\n",
        "\n",
        "# Create NeuMF-pre model using pre-trained GMF and MLP\n",
        "neumf_pre_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name='NeuMF-pre',\n",
        "    GMF_model=gmf_model,\n",
        "    MLP_model=mlp_model\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    neumf_pre_model = neumf_pre_model.cuda()\n",
        "\n",
        "# Setup optimizer (SGD is typically used for NeuMF-pre)\n",
        "neumf_pre_optimizer = optim.SGD(neumf_pre_model.parameters(), lr=learning_rate)\n",
        "neumf_pre_loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop for NeuMF-pre\n",
        "print(f\"Training NeuMF-pre for {epochs} epochs...\")\n",
        "best_hr_neumf_pre = 0.0\n",
        "best_ndcg_neumf_pre = 0.0\n",
        "best_epoch_neumf_pre = 0\n",
        "neumf_pre_training_history = {\n",
        "    'epoch': [],\n",
        "    'loss': [],\n",
        "    'hr': [],\n",
        "    'ndcg': []\n",
        "}\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    neumf_pre_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        neumf_pre_optimizer.zero_grad()\n",
        "        prediction = neumf_pre_model(user, item)\n",
        "        loss = neumf_pre_loss_function(prediction, label)\n",
        "        loss.backward()\n",
        "        neumf_pre_optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # Evaluate\n",
        "    neumf_pre_model.eval()\n",
        "    HR, NDCG = evaluate_metrics(neumf_pre_model, test_loader, top_k, device)\n",
        "    \n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    # Store history\n",
        "    neumf_pre_training_history['epoch'].append(epoch + 1)\n",
        "    neumf_pre_training_history['loss'].append(avg_loss)\n",
        "    neumf_pre_training_history['hr'].append(HR)\n",
        "    neumf_pre_training_history['ndcg'].append(NDCG)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Time: {time_str} - Loss: {avg_loss:.4f} - HR@{top_k}: {HR:.4f} - NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    if HR > best_hr_neumf_pre:\n",
        "        best_hr_neumf_pre = HR\n",
        "        best_ndcg_neumf_pre = NDCG\n",
        "        best_epoch_neumf_pre = epoch + 1\n",
        "        if save_model:\n",
        "            torch.save(neumf_pre_model, NeuMF_model_path)\n",
        "            print(f\"  ✓ Saved best NeuMF-pre model (HR@{top_k}: {HR:.4f})\")\n",
        "\n",
        "print(f\"\\n✓ NeuMF-pre Training Complete!\")\n",
        "print(f\"  Best epoch: {best_epoch_neumf_pre}\")\n",
        "print(f\"  Best HR@{top_k}: {best_hr_neumf_pre:.4f}\")\n",
        "print(f\"  Best NDCG@{top_k}: {best_ndcg_neumf_pre:.4f}\")\n",
        "\n",
        "trained_models['NeuMF-pre'] = {\n",
        "    'model': neumf_pre_model,\n",
        "    'hr': best_hr_neumf_pre,\n",
        "    'ndcg': best_ndcg_neumf_pre,\n",
        "    'epoch': best_epoch_neumf_pre\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# COMPARISON OF ALL MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Model':<15} {'HR@{top_k}':<12} {'NDCG@{top_k}':<12} {'Best Epoch':<12}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'GMF':<15} {best_hr_gmf:<12.4f} {best_ndcg_gmf:<12.4f} {best_epoch_gmf:<12}\")\n",
        "print(f\"{'MLP':<15} {best_hr_mlp:<12.4f} {best_ndcg_mlp:<12.4f} {best_epoch_mlp:<12}\")\n",
        "print(f\"{'NeuMF-end':<15} {best_hr_neumf_end:<12.4f} {best_ndcg_neumf_end:<12.4f} {best_epoch:<12}\")\n",
        "print(f\"{'NeuMF-pre':<15} {best_hr_neumf_pre:<12.4f} {best_ndcg_neumf_pre:<12.4f} {best_epoch_neumf_pre:<12}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Find best model\n",
        "all_results = [\n",
        "    ('GMF', best_hr_gmf, best_ndcg_gmf),\n",
        "    ('MLP', best_hr_mlp, best_ndcg_mlp),\n",
        "    ('NeuMF-end', best_hr_neumf_end, best_ndcg_neumf_end),\n",
        "    ('NeuMF-pre', best_hr_neumf_pre, best_ndcg_neumf_pre)\n",
        "]\n",
        "best_model_name, best_hr_overall, best_ndcg_overall = max(all_results, key=lambda x: x[1])\n",
        "\n",
        "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
        "print(f\"   HR@{top_k}: {best_hr_overall:.4f}\")\n",
        "print(f\"   NDCG@{top_k}: {best_ndcg_overall:.4f}\")\n",
        "\n",
        "# Set the best model as the main model for recommendations\n",
        "if best_model_name == 'GMF':\n",
        "    ncf_model = gmf_model\n",
        "elif best_model_name == 'MLP':\n",
        "    ncf_model = mlp_model\n",
        "elif best_model_name == 'NeuMF-pre':\n",
        "    ncf_model = neumf_pre_model\n",
        "else:\n",
        "    ncf_model = ncf_model_neumf_end\n",
        "\n",
        "print(f\"\\n✓ Using {best_model_name} model for recommendations\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 8: Creating Visualizations\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "8.1: Table 4.1 - Comparative Performance Metrics\n",
            "======================================================================\n",
            "Calculating RMSE for NCF models...\n",
            "\n",
            "Note: AutoRec and Hybrid metrics should be loaded from their respective notebooks.\n",
            "Attempting to load saved metrics or calculate from saved models...\n",
            "AutoRec model found. Note: AutoRec uses different data format.\n",
            "  Please manually add AutoRec RMSE, HR@10, and NDCG@10 values from the AutoRec notebook.\n",
            "  You can find these in the training output or set them manually below.\n",
            "Hybrid model found. Note: Hybrid uses different data format.\n",
            "  Please manually add Hybrid RMSE, HR@10, and NDCG@10 values from the Hybrid notebook.\n",
            "  You can find these in the training output or set them manually below.\n",
            "\n",
            "======================================================================\n",
            "Table 4.1: Comparative Performance Metrics\n",
            "======================================================================\n",
            "          Model RMSE  HR@10 NDCG@10\n",
            "       MF (GMF)  N/A 0.7495  0.4656\n",
            "            MLP  N/A 0.7544  0.4712\n",
            "NCF (NeuMF-end)  N/A 0.7405  0.4564\n",
            "NCF (NeuMF-pre)  N/A 0.7709  0.4863\n",
            "        AutoRec  N/A    N/A     N/A\n",
            "         Hybrid  N/A    N/A     N/A\n",
            "\n",
            "✓ Table 4.1 saved to: /Users/abbas/Documents/Codes/thesis/recommender/src/../models/table_4.1_comparative_metrics.png\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "8.2: Figure 4.1 - Training Loss vs. Epochs\n",
            "======================================================================\n",
            "Note: Hybrid training loss should be loaded from hybrid notebook.\n",
            "For now, using NCF (NeuMF-end) and NCF (NeuMF-pre) for comparison.\n",
            "✓ Figure 4.1 saved to: /Users/abbas/Documents/Codes/thesis/recommender/src/../models/figure_4.1_training_loss.png\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "8.3: Figure 4.2 - Impact of Embedding Size on Model Performance\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 8: VISUALIZATIONS\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This step creates visualizations for the thesis:\n",
        "- Table 4.1: Comparative Performance Metrics\n",
        "- Figure 4.1: Training Loss vs. Epochs (NCF vs Hybrid)\n",
        "- Figure 4.2: Impact of Embedding Size on Model Performance (RMSE, HR@10, NDCG@10)\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.rcParams['figure.figsize'] = (12, 8)\n",
        "matplotlib.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 8: Creating Visualizations\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================================================\n",
        "# 8.1 TABLE 4.1: COMPARATIVE PERFORMANCE METRICS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"8.1: Table 4.1 - Comparative Performance Metrics\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Calculate RMSE for NCF models\n",
        "print(\"Calculating RMSE for NCF models...\")\n",
        "\n",
        "\n",
        "# For AutoRec and Hybrid, we'll try to load from saved models or use placeholder values\n",
        "# You may need to adjust these based on your actual results\n",
        "print(\"\\nNote: AutoRec and Hybrid metrics should be loaded from their respective notebooks.\")\n",
        "print(\"Attempting to load saved metrics or calculate from saved models...\")\n",
        "\n",
        "# Try to load AutoRec results\n",
        "rmse_autorec = None\n",
        "hr_autorec = None\n",
        "ndcg_autorec = None\n",
        "\n",
        "try:\n",
        "    autorec_model_path = os.path.join(model_path, 'AutoRec-best.pth')\n",
        "    if os.path.exists(autorec_model_path):\n",
        "        print(\"AutoRec model found. Note: AutoRec uses different data format.\")\n",
        "        print(\"  Please manually add AutoRec RMSE, HR@10, and NDCG@10 values from the AutoRec notebook.\")\n",
        "        print(\"  You can find these in the training output or set them manually below.\")\n",
        "        # You can manually set these values if you know them:\n",
        "        # rmse_autorec = 1.0662  # Example: replace with actual value\n",
        "        # hr_autorec = 0.7500     # Example: replace with actual value\n",
        "        # ndcg_autorec = 0.4500   # Example: replace with actual value\n",
        "    else:\n",
        "        print(\"AutoRec model not found. Please add metrics manually if available.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not check AutoRec model: {e}\")\n",
        "\n",
        "# Try to load Hybrid results\n",
        "rmse_hybrid = None\n",
        "hr_hybrid = None\n",
        "ndcg_hybrid = None\n",
        "\n",
        "try:\n",
        "    hybrid_model_path = os.path.join(model_path, 'HybridAutoRecNCF.pth')\n",
        "    if os.path.exists(hybrid_model_path):\n",
        "        print(\"Hybrid model found. Note: Hybrid uses different data format.\")\n",
        "        print(\"  Please manually add Hybrid RMSE, HR@10, and NDCG@10 values from the Hybrid notebook.\")\n",
        "        print(\"  You can find these in the training output or set them manually below.\")\n",
        "        # You can manually set these values if you know them:\n",
        "        # rmse_hybrid = 1.0662  # Example: replace with actual value from training output\n",
        "        # hr_hybrid = 0.7500     # Example: replace with actual value\n",
        "        # ndcg_hybrid = 0.4500   # Example: replace with actual value\n",
        "    else:\n",
        "        print(\"Hybrid model not found. Please add metrics manually if available.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not check Hybrid model: {e}\")\n",
        "\n",
        "# Manual override: If you know the values, uncomment and set them here:\n",
        "# rmse_autorec = 1.0662  # Replace with actual AutoRec RMSE\n",
        "# hr_autorec = 0.7500    # Replace with actual AutoRec HR@10\n",
        "# ndcg_autorec = 0.4500  # Replace with actual AutoRec NDCG@10\n",
        "\n",
        "# rmse_hybrid = 1.0662   # Replace with actual Hybrid RMSE (from training output: \"Best model at epoch X with RMSE: Y\")\n",
        "# hr_hybrid = 0.7500     # Replace with actual Hybrid HR@10\n",
        "# ndcg_hybrid = 0.4500   # Replace with actual Hybrid NDCG@10\n",
        "\n",
        "# Create comparison table\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Table 4.1: Comparative Performance Metrics\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Prepare data for table\n",
        "# Handle NaN or None values properly\n",
        "def format_metric(value):\n",
        "    \"\"\"Format metric value, handling None and NaN\"\"\"\n",
        "    if value is None:\n",
        "        return \"N/A\"\n",
        "    try:\n",
        "        if np.isnan(value) or np.isinf(value):\n",
        "            return \"N/A\"\n",
        "        return f\"{value:.4f}\"\n",
        "    except (TypeError, ValueError):\n",
        "        return \"N/A\"\n",
        "\n",
        "comparison_data = {\n",
        "    'Model': ['MF (GMF)', 'MLP', 'NCF (NeuMF-end)', 'NCF (NeuMF-pre)', 'AutoRec', 'Hybrid'],\n",
        "    'HR@10': [\n",
        "        format_metric(best_hr_gmf),\n",
        "        format_metric(best_hr_mlp),\n",
        "        format_metric(best_hr_neumf_end),\n",
        "        format_metric(best_hr_neumf_pre),\n",
        "        format_metric(hr_autorec),\n",
        "        format_metric(hr_hybrid)\n",
        "    ],\n",
        "    'NDCG@10': [\n",
        "        format_metric(best_ndcg_gmf),\n",
        "        format_metric(best_ndcg_mlp),\n",
        "        format_metric(best_ndcg_neumf_end),\n",
        "        format_metric(best_ndcg_neumf_pre),\n",
        "        format_metric(ndcg_autorec),\n",
        "        format_metric(ndcg_hybrid)\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Print table\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "print(df_comparison.to_string(index=False))\n",
        "\n",
        "# Create visual table\n",
        "fig_table, ax_table = plt.subplots(figsize=(12, 5))\n",
        "ax_table.axis('tight')\n",
        "ax_table.axis('off')\n",
        "table = ax_table.table(\n",
        "    cellText=df_comparison.values,\n",
        "    colLabels=df_comparison.columns,\n",
        "    cellLoc='center',\n",
        "    loc='center',\n",
        "    bbox=[0, 0, 1, 1]\n",
        ")\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(11)\n",
        "table.scale(1.2, 1.5)\n",
        "\n",
        "# Style the table\n",
        "for i in range(len(df_comparison.columns)):\n",
        "    table[(0, i)].set_facecolor('#4CAF50')\n",
        "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "plt.title('Table 4.1: Comparative Performance Metrics', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(model_path, 'table_4.1_comparative_metrics.png'), dpi=300, bbox_inches='tight')\n",
        "print(f\"\\n✓ Table 4.1 saved to: {os.path.join(model_path, 'table_4.1_comparative_metrics.png')}\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# 8.2 FIGURE 4.1: TRAINING LOSS VS EPOCHS (NCF vs HYBRID)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"8.2: Figure 4.1 - Training Loss vs. Epochs\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Get NCF training loss (use NeuMF-end as representative)\n",
        "ncf_epochs = training_history['epoch']\n",
        "ncf_losses = training_history['loss']\n",
        "\n",
        "# For Hybrid model, try to load from saved data or use placeholder\n",
        "# You may need to load this from the hybrid notebook\n",
        "print(\"Note: Hybrid training loss should be loaded from hybrid notebook.\")\n",
        "print(\"For now, using NCF (NeuMF-end) and NCF (NeuMF-pre) for comparison.\")\n",
        "\n",
        "# Create figure\n",
        "fig_loss, ax_loss = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plot NCF (NeuMF-end)\n",
        "ax_loss.plot(ncf_epochs, ncf_losses, 'b-', linewidth=2, label='NCF (NeuMF-end)', marker='o', markersize=4)\n",
        "\n",
        "# Plot NCF (NeuMF-pre) if available\n",
        "if 'neumf_pre_training_history' in locals() and len(neumf_pre_training_history['loss']) > 0:\n",
        "    ax_loss.plot(\n",
        "        neumf_pre_training_history['epoch'],\n",
        "        neumf_pre_training_history['loss'],\n",
        "        'g-', linewidth=2, label='NCF (NeuMF-pre)', marker='s', markersize=4\n",
        "    )\n",
        "\n",
        "# If you have Hybrid training history, add it here\n",
        "# ax_loss.plot(hybrid_epochs, hybrid_losses, 'r-', linewidth=2, label='Hybrid', marker='^', markersize=4)\n",
        "\n",
        "ax_loss.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "ax_loss.set_ylabel('Training Loss', fontsize=12, fontweight='bold')\n",
        "ax_loss.set_title('Figure 4.1: Training Loss vs. Epochs', fontsize=14, fontweight='bold')\n",
        "ax_loss.legend(loc='best', fontsize=11)\n",
        "ax_loss.grid(True, alpha=0.3)\n",
        "ax_loss.set_xlim(left=1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(model_path, 'figure_4.1_training_loss.png'), dpi=300, bbox_inches='tight')\n",
        "print(f\"✓ Figure 4.1 saved to: {os.path.join(model_path, 'figure_4.1_training_loss.png')}\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# 8.3 FIGURE 4.2: IMPACT OF EMBEDDING SIZE ON NDCG@10\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"8.3: Figure 4.2 - Impact of Embedding Size on Model Performance\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training models with different embedding sizes: [8, 16, 32, 64, 128]\n",
            "Note: This may take some time. Using reduced epochs for speed.\n",
            "  Training with embedding size: 8...\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (8) must match the size of tensor b (32) at non-singleton dimension 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[44], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m ndcg_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m embedding_sizes:\n\u001b[0;32m---> 80\u001b[0m     hr_val, ndcg_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_embedding_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Reduced epochs for speed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     hr_results\u001b[38;5;241m.\u001b[39mappend(hr_val)\n\u001b[1;32m     82\u001b[0m     ndcg_results\u001b[38;5;241m.\u001b[39mappend(ndcg_val)\n",
            "Cell \u001b[0;32mIn[44], line 9\u001b[0m, in \u001b[0;36mtrain_and_evaluate_embedding_size\u001b[0;34m(factor_num, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Training with embedding size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfactor_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create model with specific embedding size\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m test_model \u001b[38;5;241m=\u001b[39m \u001b[43mNCF\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mitem_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfactor_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfactor_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNeuMF-pre\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mGMF_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgmf_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMLP_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_model\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     21\u001b[0m     test_model \u001b[38;5;241m=\u001b[39m test_model\u001b[38;5;241m.\u001b[39mcuda()\n",
            "File \u001b[0;32m~/Documents/Codes/thesis/recommender/src/helpers/ncf_model.py:116\u001b[0m, in \u001b[0;36mNCF.__init__\u001b[0;34m(self, user_num, item_num, factor_num, num_layers, dropout, model_name, GMF_model, MLP_model)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(predict_size, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Initialize weights\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_weight_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/Codes/thesis/recommender/src/helpers/ncf_model.py:163\u001b[0m, in \u001b[0;36mNCF._init_weight_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m             m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Pre-trained initialization (for NeuMF-pre)\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Copy weights from pre-trained GMF and MLP models\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# Copy embedding weights\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_user_GMF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGMF_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_user_GMF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_item_GMF\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcopy_(\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGMF_model\u001b[38;5;241m.\u001b[39membed_item_GMF\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_user_MLP\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcopy_(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMLP_model\u001b[38;5;241m.\u001b[39membed_user_MLP\u001b[38;5;241m.\u001b[39mweight)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (32) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "def train_and_evaluate_embedding_size(factor_num, epochs=10):\n",
        "    \"\"\"\n",
        "    Train a model with a specific embedding size and return RMSE, HR@10, and NDCG@10.\n",
        "    This is a simplified version for quick evaluation.\n",
        "    \"\"\"\n",
        "    print(f\"  Training with embedding size: {factor_num}...\")\n",
        "    \n",
        "    # Create model with specific embedding size\n",
        "    test_model = NCF(\n",
        "        user_num=user_num,\n",
        "        item_num=item_num,\n",
        "        factor_num=factor_num,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout_rate,\n",
        "        model_name='GMF',\n",
        "        GMF_model=None,\n",
        "        MLP_model=None\n",
        "    )\n",
        "    \n",
        "    if device == 'cuda' and torch.cuda.is_available():\n",
        "        test_model = test_model.cuda()\n",
        "    \n",
        "    # Setup optimizer\n",
        "    test_optimizer = optim.Adam(test_model.parameters(), lr=learning_rate)\n",
        "    test_loss_function = nn.BCEWithLogitsLoss()\n",
        "    \n",
        "    # Train for fewer epochs for speed\n",
        "    best_ndcg = 0.0\n",
        "    best_hr = 0.0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        test_model.train()\n",
        "        train_dataset.ng_sample()\n",
        "        \n",
        "        epoch_loss = 0.0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "            if device == 'cuda' and torch.cuda.is_available():\n",
        "                user = user.cuda()\n",
        "                item = item.cuda()\n",
        "                label = label.float().cuda()\n",
        "            else:\n",
        "                user = user\n",
        "                item = item\n",
        "                label = label.float()\n",
        "            \n",
        "            test_optimizer.zero_grad()\n",
        "            prediction = test_model(user, item)\n",
        "            loss = test_loss_function(prediction, label)\n",
        "            loss.backward()\n",
        "            test_optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "        \n",
        "        # Evaluate periodically\n",
        "        if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n",
        "            test_model.eval()\n",
        "            \n",
        "            HR, NDCG = evaluate_metrics(test_model, test_loader, top_k, device)\n",
        "            # Track best metrics\n",
        "            if NDCG > best_ndcg:\n",
        "                best_ndcg = NDCG\n",
        "            if HR > best_hr:\n",
        "                best_hr = HR\n",
        "    \n",
        "    print(f\" HR@10: {best_hr:.4f}, NDCG@10: {best_ndcg:.4f}\")\n",
        "    return  best_hr, best_ndcg\n",
        "\n",
        "# Test different embedding sizes\n",
        "embedding_sizes = [8, 16, 32, 64, 128]\n",
        "print(f\"\\nTraining models with different embedding sizes: {embedding_sizes}\")\n",
        "print(\"Note: This may take some time. Using reduced epochs for speed.\")\n",
        "\n",
        "hr_results = []\n",
        "ndcg_results = []\n",
        "\n",
        "for size in embedding_sizes:\n",
        "    hr_val, ndcg_val = train_and_evaluate_embedding_size(size, epochs=10)  # Reduced epochs for speed\n",
        "    hr_results.append(hr_val)\n",
        "    ndcg_results.append(ndcg_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training models with different embedding sizes: [8, 16, 32, 64, 128]\n",
            "Note: This may take some time. Using reduced epochs for speed.\n",
            "  Training with embedding size: 8...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Training with embedding size: 16...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Training with embedding size: 32...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 91\u001b[0m\n\u001b[1;32m     87\u001b[0m ndcg_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m embedding_sizes:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# rmse_val, hr_val, ndcg_val = train_and_evaluate_embedding_size(size, epochs=10)  # Reduced epochs for speed\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mtrain_and_evaluate_embedding_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Reduced epochs for speed\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# rmse_results.append(rmse_val)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# hr_results.append(hr_val)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# ndcg_results.append(ndcg_val)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# print(\"✓ All visualizations complete!\")\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# print(\"=\" * 70)\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[23], line 53\u001b[0m, in \u001b[0;36mtrain_and_evaluate_embedding_size\u001b[0;34m(factor_num, epochs)\u001b[0m\n\u001b[1;32m     51\u001b[0m loss \u001b[38;5;241m=\u001b[39m test_loss_function(prediction, label)\n\u001b[1;32m     52\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 53\u001b[0m \u001b[43mtest_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     56\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m~/Documents/Codes/thesis/recommender/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:516\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    513\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m             )\n\u001b[0;32m--> 516\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/Codes/thesis/recommender/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:81\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     80\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 81\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[0;32m~/Documents/Codes/thesis/recommender/.venv/lib/python3.9/site-packages/torch/optim/adam.py:247\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    235\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    237\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    238\u001b[0m         group,\n\u001b[1;32m    239\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m         state_steps,\n\u001b[1;32m    245\u001b[0m     )\n\u001b[0;32m--> 247\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m~/Documents/Codes/thesis/recommender/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:149\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/Codes/thesis/recommender/.venv/lib/python3.9/site-packages/torch/optim/adam.py:949\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 949\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/Codes/thesis/recommender/.venv/lib/python3.9/site-packages/torch/optim/adam.py:406\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    399\u001b[0m         param\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m step_t\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m capturable_supported_devices\n\u001b[1;32m    401\u001b[0m     ), (\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf capturable=True, params and state_steps must be on supported devices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcapturable_supported_devices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m     )\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# update step\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoupled_weight_decay:\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Create figure with subplots for all three metrics\n",
        "fig_embedding, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
        "\n",
        "# Plot 2: HR@10\n",
        "axes[0].plot(embedding_sizes, hr_results, 'g-o', linewidth=2, markersize=8, \n",
        "             markerfacecolor='lightgreen', markeredgewidth=2)\n",
        "axes[0].set_xlabel('Embedding Size', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('HR@10', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('HR@10 vs Embedding Size', fontsize=13, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_xticks(embedding_sizes)\n",
        "axes[0].set_ylim([min(hr_results) * 0.95, max(hr_results) * 1.05])\n",
        "\n",
        "# Add value labels on points\n",
        "for i, (size, hr) in enumerate(zip(embedding_sizes, hr_results)):\n",
        "    axes[0].annotate(f'{hr:.3f}', (size, hr), textcoords=\"offset points\", \n",
        "                    xytext=(0,10), ha='center', fontsize=9)\n",
        "\n",
        "# Plot 3: NDCG@10\n",
        "axes[1].plot(embedding_sizes, ndcg_results, 'b-o', linewidth=2, markersize=8, \n",
        "             markerfacecolor='lightblue', markeredgewidth=2)\n",
        "axes[1].set_xlabel('Embedding Size', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('NDCG@10', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('NDCG@10 vs Embedding Size', fontsize=13, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_xticks(embedding_sizes)\n",
        "axes[1].set_ylim([min(ndcg_results) * 0.95, max(ndcg_results) * 1.05])\n",
        "\n",
        "# Add value labels on points\n",
        "for i, (size, ndcg) in enumerate(zip(embedding_sizes, ndcg_results)):\n",
        "    axes[1].annotate(f'{ndcg:.3f}', (size, ndcg), textcoords=\"offset points\", \n",
        "                    xytext=(0,10), ha='center', fontsize=9)\n",
        "\n",
        "plt.suptitle('Figure 4.2: Impact of Embedding Size on Model Performance', \n",
        "             fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(model_path, 'figure_4.2_embedding_size.png'), dpi=300, bbox_inches='tight')\n",
        "print(f\"\\n✓ Figure 4.2 saved to: {os.path.join(model_path, 'figure_4.2_embedding_size.png')}\")\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Embedding Size Impact Summary\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Embedding Size':<15} {'RMSE':<10} {'HR@10':<10} {'NDCG@10':<10}\")\n",
        "print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

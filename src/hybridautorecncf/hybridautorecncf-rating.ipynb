{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hybrid AutoRec-NCF for Rating Prediction (Explicit Feedback)\n",
        "\n",
        "## Motivation\n",
        "\n",
        "This notebook adapts the Hybrid AutoRec-NCF model for **explicit feedback** rating prediction tasks.\n",
        "\n",
        "### Key Features:\n",
        "\n",
        "1. **Data**: Actual ratings (1-5 scale)\n",
        "2. **Loss**: MSE loss for regression\n",
        "3. **No Negative Sampling**: Use all ratings directly\n",
        "4. **Evaluation**: RMSE and MAE metrics\n",
        "\n",
        "### Approach:\n",
        "\n",
        "- Use actual ratings (1-5 scale)\n",
        "- Train with mean squared error loss\n",
        "- Evaluate using RMSE and MAE metrics\n",
        "\n",
        "---\n",
        "\n",
        "## Architecture Overview: Decoder vs NCF\n",
        "\n",
        "### Important Distinction\n",
        "\n",
        "**Decoder** and **NCF** serve different roles:\n",
        "\n",
        "1. **Decoder (Reconstruction Task)**:\n",
        "   - Input: `user_z` (latent representation)\n",
        "   - Output: `user_recon` (full rating vector for ALL items)\n",
        "   - Purpose: Learn good representations (unsupervised)\n",
        "   - Example: Predicts ratings for items [0, 1, 2, ..., 3705] for user 42\n",
        "\n",
        "2. **NCF (Prediction Task)**:\n",
        "   - Input: `user_z` + `item_z` (both latent representations)\n",
        "   - Output: `pred` (single rating for ONE user-item pair)\n",
        "   - Purpose: Make accurate predictions (supervised)\n",
        "   - Example: Predicts rating(42, 789) = 4.2\n",
        "\n",
        "**Why both?**\n",
        "- Decoder ensures the encoder learns meaningful patterns (reconstruction loss)\n",
        "- NCF combines user and item representations for accurate predictions (interaction loss)\n",
        "- They complement each other: decoder learns features, NCF uses them for prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hybrid AutoRec-NCF for Rating Prediction (Explicit Feedback)\n",
        "\n",
        "## Motivation\n",
        "\n",
        "This notebook adapts the Hybrid AutoRec-NCF model for **explicit feedback** rating prediction tasks.\n",
        "\n",
        "### Key Features:\n",
        "\n",
        "1. **Data**: Actual ratings (1-5 scale)\n",
        "2. **Loss**: MSE loss for regression\n",
        "3. **No Negative Sampling**: Use all ratings directly\n",
        "4. **Evaluation**: RMSE and MAE metrics\n",
        "\n",
        "### Approach:\n",
        "\n",
        "- Use actual ratings (1-5 scale)\n",
        "- Train with mean squared error loss\n",
        "- Evaluate using RMSE and MAE metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/abbas/Documents/Codes/thesis/recommender/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# Walk up the directory tree until we find 'src'\n",
        "path = current_dir\n",
        "src_path = None\n",
        "\n",
        "while True:\n",
        "    if os.path.basename(path) == \"src\":\n",
        "        src_path = path\n",
        "        break\n",
        "    parent = os.path.dirname(path)\n",
        "    if parent == path:  # reached filesystem root\n",
        "        break\n",
        "    path = parent\n",
        "\n",
        "# Add src to sys.path if found\n",
        "if src_path and src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Local imports\n",
        "from helpers.data_downloader import download_ml1m_dataset\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        latent_dim: int,\n",
        "        hidden_dims=(256, 128),\n",
        "        dropout_rate: float = 0.1,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim: dimensionality of input vector (e.g. user rating vector)\n",
        "            latent_dim: size of bottleneck (MUST match NCF latent_dim)\n",
        "            hidden_dims: encoder hidden layer sizes\n",
        "            dropout_rate: dropout applied to hidden layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # =======================\n",
        "        # Encoder\n",
        "        # =======================\n",
        "        encoder_layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for h in hidden_dims:\n",
        "            encoder_layers.append(nn.Linear(prev_dim, h))\n",
        "            encoder_layers.append(nn.ReLU())\n",
        "            encoder_layers.append(nn.Dropout(dropout_rate))\n",
        "            prev_dim = h\n",
        "\n",
        "        # Bottleneck (NO activation)\n",
        "        encoder_layers.append(nn.Linear(prev_dim, latent_dim))\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # =======================\n",
        "        # Decoder\n",
        "        # =======================\n",
        "        decoder_layers = []\n",
        "        prev_dim = latent_dim\n",
        "\n",
        "        for h in reversed(hidden_dims):\n",
        "            decoder_layers.append(nn.Linear(prev_dim, h))\n",
        "            decoder_layers.append(nn.ReLU())\n",
        "            decoder_layers.append(nn.Dropout(dropout_rate))\n",
        "            prev_dim = h\n",
        "\n",
        "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
        "        # No activation for rating prediction (raw reconstruction)\n",
        "\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        recon = self.decode(z)\n",
        "        return recon, z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NCF(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        latent_dim,\n",
        "        mlp_layers=(128, 64),\n",
        "        dropout_rate=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # =======================\n",
        "        # GMF (pure interaction)\n",
        "        # =======================\n",
        "        # No parameters here by design\n",
        "\n",
        "        # =======================\n",
        "        # MLP\n",
        "        # =======================\n",
        "        mlp_modules = []\n",
        "        input_dim = latent_dim * 2\n",
        "\n",
        "        for h in mlp_layers:\n",
        "            mlp_modules.append(nn.Linear(input_dim, h))\n",
        "            mlp_modules.append(nn.ReLU())\n",
        "            mlp_modules.append(nn.Dropout(dropout_rate))\n",
        "            input_dim = h\n",
        "\n",
        "        self.mlp = nn.Sequential(*mlp_modules)\n",
        "\n",
        "        # =======================\n",
        "        # Final prediction layer\n",
        "        # =======================\n",
        "        mlp_out_dim = mlp_layers[-1] if len(mlp_layers) > 0 else input_dim\n",
        "        self.output = nn.Linear(latent_dim + mlp_out_dim, 1)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, user_z, item_z):\n",
        "        # GMF branch\n",
        "        gmf_out = user_z * item_z  # pure element-wise product\n",
        "\n",
        "        # MLP branch\n",
        "        mlp_input = torch.cat([user_z, item_z], dim=1)\n",
        "        mlp_out = self.mlp(mlp_input)\n",
        "\n",
        "        # Final prediction\n",
        "        concat = torch.cat([gmf_out, mlp_out], dim=1)\n",
        "        pred = self.output(concat)\n",
        "\n",
        "        # Return raw prediction (use MSE loss)\n",
        "        return pred.squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HybridAutoRecNCF(nn.Module):\n",
        "    def __init__(self, num_users, num_items, latent_dim, mlp_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.user_autorec = AutoEncoder(num_items, latent_dim)\n",
        "        self.item_autorec = AutoEncoder(num_users, latent_dim)\n",
        "\n",
        "        self.ncf = NCF(latent_dim, mlp_layers)\n",
        "\n",
        "    def forward(self, user_vecs, item_vecs, user_ids, item_ids):\n",
        "        # AutoRec forward\n",
        "        # user_vecs: (batch_size, num_items) - each row is a user's rating vector\n",
        "        # item_vecs: (batch_size, num_users) - each row is an item's rating vector\n",
        "        user_recon, user_z = self.user_autorec(user_vecs)\n",
        "        item_recon, item_z = self.item_autorec(item_vecs)\n",
        "\n",
        "        # Each element in the batch corresponds to a (user, item) pair\n",
        "        # user_z[i] is the latent for the user in pair i\n",
        "        # item_z[i] is the latent for the item in pair i\n",
        "        pred = self.ncf(user_z, item_z)\n",
        "        return pred, user_recon, item_recon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reconstruction_loss(pred, target, mask):\n",
        "    \"\"\"MSE loss for reconstruction (explicit ratings)\"\"\"\n",
        "    diff = (pred - target) * mask\n",
        "    return torch.sum(diff ** 2) / torch.sum(mask)\n",
        "\n",
        "\n",
        "def interaction_loss(pred, rating):\n",
        "    \"\"\"MSE loss for interaction prediction\"\"\"\n",
        "    return F.mse_loss(pred, rating)\n",
        "\n",
        "\n",
        "def total_loss(pred, rating,\n",
        "               user_recon, user_vecs, user_mask,\n",
        "               item_recon, item_vecs, item_mask,\n",
        "               alpha=1.0, beta=1.0):\n",
        "    rec_u = reconstruction_loss(user_recon, user_vecs, user_mask)\n",
        "    rec_i = reconstruction_loss(item_recon, item_vecs, item_mask)\n",
        "    inter = interaction_loss(pred, rating)\n",
        "    return alpha * (rec_u + rec_i) + beta * inter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        (user_ids, item_ids, ratings,\n",
        "         user_vecs, item_vecs,\n",
        "         user_mask, item_mask) = batch\n",
        "\n",
        "        user_ids = user_ids.to(device).squeeze()\n",
        "        item_ids = item_ids.to(device).squeeze()\n",
        "        ratings = ratings.to(device).squeeze()\n",
        "        user_vecs = user_vecs.to(device)\n",
        "        item_vecs = item_vecs.to(device)\n",
        "        user_mask = user_mask.to(device)\n",
        "        item_mask = item_mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred, user_recon, item_recon = model(\n",
        "            user_vecs, item_vecs, user_ids, item_ids\n",
        "        )\n",
        "\n",
        "        loss = total_loss(\n",
        "            pred, ratings,\n",
        "            user_recon, user_vecs, user_mask,\n",
        "            item_recon, item_vecs, item_mask\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "\n",
        "    return total / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading ratings data...\n",
            "======================================================================\n",
            "Loading MovieLens 1M Dataset\n",
            "======================================================================\n",
            "Data path: /Users/abbas/Documents/Codes/thesis/recommender/src/../data/ml-1m/ratings.dat\n",
            "✓ Successfully loaded 1,000,209 ratings\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "data_dir = os.path.join(os.path.dirname(os.getcwd()), '..', 'data')\n",
        "data_path = os.path.join(data_dir, 'ml-1m', 'ratings.dat')\n",
        "\n",
        "# Check if file exists\n",
        "if not os.path.exists(data_path):\n",
        "    download_ml1m_dataset(data_dir=data_dir)\n",
        "\n",
        "def load_ml_1m_data(data_path=data_path) -> pd.DataFrame:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Loading MovieLens 1M Dataset\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Data path: {data_path}\")\n",
        "    return pd.read_csv(\n",
        "        data_path,\n",
        "        sep='::',\n",
        "        header=None,\n",
        "        names=['user_id', 'item_id', 'rating', 'timestamp'],\n",
        "        engine='python',\n",
        "        dtype={\n",
        "            'user_id': np.int32,\n",
        "            'item_id': np.int32,\n",
        "            'rating': np.float32,\n",
        "            'timestamp': np.int32\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "# Load ratings data\n",
        "print(\"\\nLoading ratings data...\")\n",
        "ratings_df = load_ml_1m_data()\n",
        "print(f\"✓ Successfully loaded {len(ratings_df):,} ratings\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing data...\n",
            "✓ Preprocessing complete!\n",
            "  - Users: 6040, Items: 3706\n",
            "  - Train interactions: 800,167\n",
            "  - Test interactions: 200,042\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing: remap IDs and create train/test split\n",
        "def preprocess_data(ratings_df, test_size=0.2, random_state=42):\n",
        "    \"\"\"Preprocess data: remap IDs to be contiguous and split train/test\"\"\"\n",
        "    # Create a copy\n",
        "    data = ratings_df.copy()\n",
        "    \n",
        "    # Remap user IDs to be contiguous (0-indexed)\n",
        "    unique_users = sorted(data['user_id'].unique())\n",
        "    user_map = {old_id: new_id for new_id, old_id in enumerate(unique_users)}\n",
        "    data['user_id'] = data['user_id'].map(user_map)\n",
        "    num_users = len(unique_users)\n",
        "    \n",
        "    # Remap item IDs to be contiguous (0-indexed)\n",
        "    unique_items = sorted(data['item_id'].unique())\n",
        "    item_map = {old_id: new_id for new_id, old_id in enumerate(unique_items)}\n",
        "    data['item_id'] = data['item_id'].map(item_map)\n",
        "    num_items = len(unique_items)\n",
        "    \n",
        "    # Split into train and test\n",
        "    train_df, test_df = train_test_split(\n",
        "        data[['user_id', 'item_id', 'rating']],\n",
        "        test_size=test_size,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    \n",
        "    # Create rating matrices\n",
        "    train_mat = np.zeros((num_users, num_items), dtype=np.float32)\n",
        "    test_mat = np.zeros((num_users, num_items), dtype=np.float32)\n",
        "    \n",
        "    # Fill train matrix\n",
        "    for _, row in train_df.iterrows():\n",
        "        train_mat[int(row['user_id']), int(row['item_id'])] = float(row['rating'])\n",
        "    \n",
        "    # Fill test matrix\n",
        "    for _, row in test_df.iterrows():\n",
        "        test_mat[int(row['user_id']), int(row['item_id'])] = float(row['rating'])\n",
        "    \n",
        "    return train_mat, test_mat, train_df, test_df, num_users, num_items\n",
        "\n",
        "print(\"Preprocessing data...\")\n",
        "train_mat, test_mat, train_df, test_df, num_users, num_items = preprocess_data(\n",
        "    ratings_df, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"✓ Preprocessing complete!\")\n",
        "print(f\"  - Users: {num_users}, Items: {num_items}\")\n",
        "print(f\"  - Train interactions: {len(train_df):,}\")\n",
        "print(f\"  - Test interactions: {len(test_df):,}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Datasets created\n",
            "  - Train samples: 800,167\n",
            "  - Test samples: 200,042\n"
          ]
        }
      ],
      "source": [
        "# Dataset class for hybrid model\n",
        "class HybridDataset(data.Dataset):\n",
        "    def __init__(self, rating_matrix, interactions_df):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            rating_matrix: (num_users, num_items) rating matrix\n",
        "            interactions_df: DataFrame with columns ['user_id', 'item_id', 'rating']\n",
        "        \"\"\"\n",
        "        self.rating_matrix = rating_matrix\n",
        "        self.interactions_df = interactions_df.reset_index(drop=True)\n",
        "        \n",
        "        # Create masks (1 where rating exists, 0 otherwise)\n",
        "        self.user_mask = (rating_matrix > 0).astype(np.float32)\n",
        "        self.item_mask = (rating_matrix.T > 0).astype(np.float32)  # Transpose for items\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.interactions_df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.interactions_df.iloc[idx]\n",
        "        user_id = int(row['user_id'])\n",
        "        item_id = int(row['item_id'])\n",
        "        rating = float(row['rating'])\n",
        "        \n",
        "        # Get user vector (ratings across all items)\n",
        "        user_vec = torch.FloatTensor(self.rating_matrix[user_id])\n",
        "        \n",
        "        # Get item vector (ratings across all users)\n",
        "        item_vec = torch.FloatTensor(self.rating_matrix[:, item_id])\n",
        "        \n",
        "        # Get masks\n",
        "        user_mask = torch.FloatTensor(self.user_mask[user_id])\n",
        "        item_mask = torch.FloatTensor(self.item_mask[item_id])\n",
        "        \n",
        "        return (\n",
        "            torch.LongTensor([user_id]),\n",
        "            torch.LongTensor([item_id]),\n",
        "            torch.FloatTensor([rating]),\n",
        "            user_vec,\n",
        "            item_vec,\n",
        "            user_mask,\n",
        "            item_mask\n",
        "        )\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = HybridDataset(train_mat, train_df)\n",
        "test_dataset = HybridDataset(test_mat, test_df)\n",
        "\n",
        "print(f\"✓ Datasets created\")\n",
        "print(f\"  - Train samples: {len(train_dataset):,}\")\n",
        "print(f\"  - Test samples: {len(test_dataset):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Data loaders created (batch_size=256)\n"
          ]
        }
      ],
      "source": [
        "# Create data loaders\n",
        "batch_size = 256\n",
        "train_loader = data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "test_loader = data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "print(f\"✓ Data loaders created (batch_size={batch_size})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "✓ Model initialized\n",
            "  - Latent dim: 64\n",
            "  - MLP layers: [128, 64]\n",
            "  - Total parameters: 5,190,099\n"
          ]
        }
      ],
      "source": [
        "# Initialize model and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "latent_dim = 64\n",
        "mlp_layers = [128, 64]\n",
        "\n",
        "model = HybridAutoRecNCF(\n",
        "    num_users, num_items, latent_dim, mlp_layers\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(), lr=1e-3, weight_decay=1e-5\n",
        ")\n",
        "\n",
        "print(f\"✓ Model initialized\")\n",
        "print(f\"  - Latent dim: {latent_dim}\")\n",
        "print(f\"  - MLP layers: {mlp_layers}\")\n",
        "print(f\"  - Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation function for RMSE and MAE\n",
        "def evaluate(model, dataloader, device, return_predictions=False):\n",
        "    model.eval()\n",
        "    accumulated_loss = 0.0\n",
        "    accumulated_rmse = 0.0\n",
        "    accumulated_mae = 0.0\n",
        "    num_samples = 0\n",
        "    \n",
        "    all_preds = []\n",
        "    all_ratings = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            (user_ids, item_ids, ratings,\n",
        "             user_vecs, item_vecs,\n",
        "             user_mask, item_mask) = batch\n",
        "\n",
        "            user_ids = user_ids.to(device).squeeze()\n",
        "            item_ids = item_ids.to(device).squeeze()\n",
        "            ratings = ratings.to(device).squeeze()\n",
        "            user_vecs = user_vecs.to(device)\n",
        "            item_vecs = item_vecs.to(device)\n",
        "            user_mask = user_mask.to(device)\n",
        "            item_mask = item_mask.to(device)\n",
        "\n",
        "            pred, user_recon, item_recon = model(\n",
        "                user_vecs, item_vecs, user_ids, item_ids\n",
        "            )\n",
        "\n",
        "            loss = total_loss(\n",
        "                pred, ratings,\n",
        "                user_recon, user_vecs, user_mask,\n",
        "                item_recon, item_vecs, item_mask\n",
        "            )\n",
        "            \n",
        "            # Calculate RMSE and MAE for predictions\n",
        "            rmse = torch.sqrt(F.mse_loss(pred, ratings))\n",
        "            mae = F.l1_loss(pred, ratings)\n",
        "            \n",
        "            accumulated_loss += loss.item() * len(ratings)\n",
        "            accumulated_rmse += rmse.item() * len(ratings)\n",
        "            accumulated_mae += mae.item() * len(ratings)\n",
        "            num_samples += len(ratings)\n",
        "            \n",
        "            if return_predictions:\n",
        "                all_preds.extend(pred.cpu().numpy())\n",
        "                all_ratings.extend(ratings.cpu().numpy())\n",
        "    \n",
        "    if return_predictions:\n",
        "        return (accumulated_loss / num_samples, accumulated_rmse / num_samples, \n",
        "                accumulated_mae / num_samples, np.array(all_preds), np.array(all_ratings))\n",
        "    else:\n",
        "        return accumulated_loss / num_samples, accumulated_rmse / num_samples, accumulated_mae / num_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Starting Training\n",
            "======================================================================\n",
            "Max Epochs: 50\n",
            "Early Stopping Patience: 10\n",
            "Batch size: 256\n",
            "Learning rate: 0.001\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs:  26%|██▌       | 13/50 [28:29<1:21:45, 132.59s/it, train_loss=1.2757, test_rmse=1.0795, test_mae=0.8994, best_rmse=1.0323, patience=2/10]"
          ]
        }
      ],
      "source": [
        "# Training loop with metrics tracking and early stopping\n",
        "num_epochs = 50\n",
        "best_test_rmse = float('inf')\n",
        "best_epoch = 0\n",
        "EARLY_STOPPING_PATIENCE = 10  # Number of epochs to wait before early stopping\n",
        "patience_counter = 0\n",
        "MIN_DELTA = 0.0001\n",
        "\n",
        "# Create model save directory\n",
        "model_dir = os.path.join(os.path.dirname(os.getcwd()), '..', 'models')\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "model_path = os.path.join(model_dir, 'HybridAutoRecNCF-Rating.pth')\n",
        "\n",
        "# Track metrics for visualization\n",
        "training_history = {\n",
        "    'epoch': [],\n",
        "    'train_loss': [],\n",
        "    'test_loss': [],\n",
        "    'test_rmse': [],\n",
        "    'test_mae': []\n",
        "}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Starting Training\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Max Epochs: {num_epochs}\")\n",
        "print(f\"Early Stopping Patience: {EARLY_STOPPING_PATIENCE}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "try:\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Learning rate: {lr}\")\n",
        "except NameError:\n",
        "    print(\"Learning rate: 0.001 (default)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "epoch_pbar = tqdm(range(num_epochs), desc=\"Epochs\")\n",
        "for epoch in epoch_pbar:\n",
        "    # Training\n",
        "    train_loss = train(model, train_loader, optimizer, device)\n",
        "    \n",
        "    # Evaluation (RMSE and MAE)\n",
        "    test_loss, test_rmse, test_mae = evaluate(model, test_loader, device)\n",
        "    \n",
        "    # Early stopping check with minimum delta\n",
        "    improved = False\n",
        "    if test_rmse < best_test_rmse - MIN_DELTA:\n",
        "        best_test_rmse = test_rmse\n",
        "        best_epoch = epoch\n",
        "        patience_counter = 0\n",
        "        improved = True\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    \n",
        "    # Track metrics\n",
        "    training_history['epoch'].append(epoch + 1)\n",
        "    training_history['train_loss'].append(train_loss)\n",
        "    training_history['test_loss'].append(test_loss)\n",
        "    training_history['test_rmse'].append(test_rmse)\n",
        "    training_history['test_mae'].append(test_mae)\n",
        "    \n",
        "    # Update epoch progress bar\n",
        "    epoch_pbar.set_postfix({\n",
        "        'train_loss': f'{train_loss:.4f}',\n",
        "        'test_rmse': f'{test_rmse:.4f}',\n",
        "        'test_mae': f'{test_mae:.4f}',\n",
        "        'best_rmse': f'{best_test_rmse:.4f}',\n",
        "        'patience': f'{patience_counter}/{EARLY_STOPPING_PATIENCE}'\n",
        "    })\n",
        "    \n",
        "    # Early stopping\n",
        "    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
        "        print(f\"No improvement for {EARLY_STOPPING_PATIENCE} epochs\")\n",
        "        break\n",
        "\n",
        "epoch_pbar.close()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Training Complete!\")\n",
        "print(f\"Best model at epoch {best_epoch+1} with RMSE: {best_test_rmse:.4f}\")\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==================== COMPREHENSIVE VISUALIZATIONS ====================\n",
        "\n",
        "# 1. Training and Test Loss Comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "epochs = training_history['epoch']\n",
        "\n",
        "# Plot 1: Training vs Test Loss\n",
        "axes[0, 0].plot(epochs, training_history['train_loss'], 'b-o', label='Train Loss', linewidth=2, markersize=4, alpha=0.7)\n",
        "axes[0, 0].plot(epochs, training_history['test_loss'], 'r-s', label='Test Loss', linewidth=2, markersize=4, alpha=0.7)\n",
        "axes[0, 0].axvline(x=best_epoch + 1, color='g', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch + 1})')\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0, 0].set_title('Training vs Test Loss', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].set_xticks(epochs[::5] if len(epochs) > 10 else epochs)\n",
        "\n",
        "# Plot 2: RMSE Over Epochs\n",
        "axes[0, 1].plot(epochs, training_history['test_rmse'], 'b-o', label='Test RMSE', linewidth=2, markersize=4)\n",
        "axes[0, 1].axvline(x=best_epoch + 1, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch + 1})')\n",
        "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0, 1].set_ylabel('RMSE', fontsize=12)\n",
        "axes[0, 1].set_title('Test RMSE Over Epochs', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].set_xticks(epochs[::5] if len(epochs) > 10 else epochs)\n",
        "\n",
        "# Plot 3: MAE Over Epochs\n",
        "axes[1, 0].plot(epochs, training_history['test_mae'], 'r-o', label='Test MAE', linewidth=2, markersize=4)\n",
        "axes[1, 0].axvline(x=best_epoch + 1, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch + 1})')\n",
        "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1, 0].set_ylabel('MAE', fontsize=12)\n",
        "axes[1, 0].set_title('Test MAE Over Epochs', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].set_xticks(epochs[::5] if len(epochs) > 10 else epochs)\n",
        "\n",
        "# Plot 4: RMSE vs MAE Comparison\n",
        "axes[1, 1].plot(epochs, training_history['test_rmse'], 'b-o', label='RMSE', linewidth=2, markersize=4)\n",
        "axes[1, 1].plot(epochs, training_history['test_mae'], 'r-s', label='MAE', linewidth=2, markersize=4)\n",
        "axes[1, 1].axvline(x=best_epoch + 1, color='g', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch + 1})')\n",
        "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Error', fontsize=12)\n",
        "axes[1, 1].set_title('RMSE vs MAE Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].set_xticks(epochs[::5] if len(epochs) > 10 else epochs)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Get predictions for detailed analysis\n",
        "print(\"\\\\nGenerating predictions for detailed analysis...\")\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "test_loss, test_rmse, test_mae, test_preds, test_ratings = evaluate(model, test_loader, device, return_predictions=True)\n",
        "\n",
        "# 3. Predicted vs Actual Scatter Plot\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Predicted vs Actual (Scatter)\n",
        "axes[0, 0].scatter(test_ratings, test_preds, alpha=0.3, s=1)\n",
        "axes[0, 0].plot([1, 5], [1, 5], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "axes[0, 0].set_xlabel('Actual Rating', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Predicted Rating', fontsize=12)\n",
        "axes[0, 0].set_title('Predicted vs Actual Ratings', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].set_xlim([0.5, 5.5])\n",
        "axes[0, 0].set_ylim([0.5, 5.5])\n",
        "\n",
        "# Plot 2: Error Distribution\n",
        "errors = test_preds - test_ratings\n",
        "axes[0, 1].hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0, 1].axvline(x=0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
        "axes[0, 1].set_xlabel('Prediction Error (Predicted - Actual)', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0, 1].set_title('Error Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].legend()\n",
        "mean_error = np.mean(errors)\n",
        "std_error = np.std(errors)\n",
        "axes[0, 1].text(0.05, 0.95, f'Mean: {mean_error:.4f}\\\\nStd: {std_error:.4f}', \n",
        "                transform=axes[0, 1].transAxes, verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "# Plot 3: Absolute Error Distribution\n",
        "abs_errors = np.abs(errors)\n",
        "axes[1, 0].hist(abs_errors, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
        "axes[1, 0].axvline(x=np.mean(abs_errors), color='r', linestyle='--', linewidth=2, \n",
        "                  label=f'Mean MAE: {np.mean(abs_errors):.4f}')\n",
        "axes[1, 0].set_xlabel('Absolute Error', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[1, 0].set_title('Absolute Error Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# Plot 4: Residual Plot\n",
        "axes[1, 1].scatter(test_ratings, errors, alpha=0.3, s=1)\n",
        "axes[1, 1].axhline(y=0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
        "axes[1, 1].set_xlabel('Actual Rating', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Residual (Predicted - Actual)', fontsize=12)\n",
        "axes[1, 1].set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Per-Rating-Level Analysis\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Calculate metrics per actual rating\n",
        "rating_levels = [1, 2, 3, 4, 5]\n",
        "rmse_per_rating = []\n",
        "mae_per_rating = []\n",
        "count_per_rating = []\n",
        "\n",
        "for rating in rating_levels:\n",
        "    mask = test_ratings == rating\n",
        "    if np.sum(mask) > 0:\n",
        "        preds_subset = test_preds[mask]\n",
        "        ratings_subset = test_ratings[mask]\n",
        "        rmse_per_rating.append(np.sqrt(np.mean((preds_subset - ratings_subset) ** 2)))\n",
        "        mae_per_rating.append(np.mean(np.abs(preds_subset - ratings_subset)))\n",
        "        count_per_rating.append(np.sum(mask))\n",
        "    else:\n",
        "        rmse_per_rating.append(0)\n",
        "        mae_per_rating.append(0)\n",
        "        count_per_rating.append(0)\n",
        "\n",
        "# Plot 1: RMSE per Rating Level\n",
        "x_pos = np.arange(len(rating_levels))\n",
        "axes[0].bar(x_pos, rmse_per_rating, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "axes[0].set_xlabel('Actual Rating', fontsize=12)\n",
        "axes[0].set_ylabel('RMSE', fontsize=12)\n",
        "axes[0].set_title('RMSE by Actual Rating Level', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xticks(x_pos)\n",
        "axes[0].set_xticklabels(rating_levels)\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "for i, (rmse, count) in enumerate(zip(rmse_per_rating, count_per_rating)):\n",
        "    if count > 0:\n",
        "        axes[0].text(i, rmse + 0.01, f'{count:,}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Plot 2: MAE per Rating Level\n",
        "axes[1].bar(x_pos, mae_per_rating, alpha=0.7, color='coral', edgecolor='black')\n",
        "axes[1].set_xlabel('Actual Rating', fontsize=12)\n",
        "axes[1].set_ylabel('MAE', fontsize=12)\n",
        "axes[1].set_title('MAE by Actual Rating Level', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xticks(x_pos)\n",
        "axes[1].set_xticklabels(rating_levels)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "for i, (mae, count) in enumerate(zip(mae_per_rating, count_per_rating)):\n",
        "    if count > 0:\n",
        "        axes[1].text(i, mae + 0.01, f'{count:,}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5. Rating Distribution Comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Actual Rating Distribution\n",
        "unique_ratings, counts = np.unique(test_ratings, return_counts=True)\n",
        "axes[0].bar(unique_ratings, counts, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "axes[0].set_xlabel('Rating', fontsize=12)\n",
        "axes[0].set_ylabel('Count', fontsize=12)\n",
        "axes[0].set_title('Actual Rating Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "for rating, count in zip(unique_ratings, counts):\n",
        "    axes[0].text(rating, count + max(counts) * 0.01, f'{count:,}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# Plot 2: Predicted Rating Distribution (binned)\n",
        "pred_bins = np.arange(0.5, 6.5, 0.5)\n",
        "axes[1].hist(test_preds, bins=pred_bins, alpha=0.7, color='coral', edgecolor='black')\n",
        "axes[1].set_xlabel('Predicted Rating', fontsize=12)\n",
        "axes[1].set_ylabel('Count', fontsize=12)\n",
        "axes[1].set_title('Predicted Rating Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final metrics\n",
        "print(\"\\\\n\" + \"=\" * 70)\n",
        "print(\"Final Training Metrics\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Best RMSE: {best_test_rmse:.4f} (Epoch {best_epoch + 1})\")\n",
        "print(f\"Final RMSE: {training_history['test_rmse'][-1]:.4f}\")\n",
        "print(f\"Final MAE: {training_history['test_mae'][-1]:.4f}\")\n",
        "print(f\"\\\\nError Statistics:\")\n",
        "print(f\"  Mean Error: {mean_error:.4f}\")\n",
        "print(f\"  Std Error: {std_error:.4f}\")\n",
        "print(f\"  Mean Absolute Error: {np.mean(abs_errors):.4f}\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
